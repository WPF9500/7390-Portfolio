{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "G:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "G:\\Anaconda\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold,cross_val_score,train_test_split\n",
    "import tensorflow as tf\n",
    "from dateutil import parser\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = pd.read_csv(\"billboard_chart_data.csv\")  #read the billboard chart data which we use billboard API to get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = billboard[['title','artist','rank', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                               artist  rank        date\n",
       "0  God's Plan                                Drake     1  2018-02-17\n",
       "1     Perfect                           Ed Sheeran     2  2018-02-17\n",
       "2     Finesse                 Bruno Mars & Cardi B     3  2018-02-17\n",
       "3      Havana  Camila Cabello Featuring Young Thug     4  2018-02-17\n",
       "4    Rockstar      Post Malone Featuring 21 Savage     5  2018-02-17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100066 entries, 0 to 100065\n",
      "Data columns (total 4 columns):\n",
      "title     100066 non-null object\n",
      "artist    100066 non-null object\n",
      "rank      100066 non-null int64\n",
      "date      100066 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "billboard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = pd.read_csv(\"Full_data.csv\")  # read all the track's audio feature data(Use Spotify API to get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3Vo4wInECJQuz9BIBMOu8i</td>\n",
       "      <td>0du5cEVh5yTK9QJze8zA0C</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>11594549</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.926</td>\n",
       "      <td>105.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>1rfofaqEpACxVEHIZBJe6W</td>\n",
       "      <td>4nDoRrQiYLoBzwC5BhVJzF</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>3144195</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.394</td>\n",
       "      <td>104.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>0OAAAdiHJKa2wlCKqaYXV7</td>\n",
       "      <td>246dkjvS1zLTtiykXe5h60</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>3309267</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.127</td>\n",
       "      <td>159.764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       title                               artist  \\\n",
       "0   1  God's Plan                                Drake   \n",
       "1   2     Perfect                           Ed Sheeran   \n",
       "2   3     Finesse                 Bruno Mars & Cardi B   \n",
       "3   4      Havana  Camila Cabello Featuring Young Thug   \n",
       "4   5    Rockstar      Post Malone Featuring 21 Savage   \n",
       "\n",
       "                 track_id               artist_id  rank  hit  \\\n",
       "0  2XW4DbS6NddZxRPm5rMCeY  3TVXtAsR1Inumwj472S9r4     1  Yes   \n",
       "1  0tgVpDi06FyKpA1z0VMD4v  6eUKZXaKkcviH0Ku9w2n3V     1  Yes   \n",
       "2  3Vo4wInECJQuz9BIBMOu8i  0du5cEVh5yTK9QJze8zA0C     3  Yes   \n",
       "3  1rfofaqEpACxVEHIZBJe6W  4nDoRrQiYLoBzwC5BhVJzF     1  Yes   \n",
       "4  0OAAAdiHJKa2wlCKqaYXV7  246dkjvS1zLTtiykXe5h60     1  Yes   \n",
       "\n",
       "   artist_popularity  followers  popularity   ...     energy  key  loudness  \\\n",
       "0                100   18311322          99   ...      0.454    7    -9.488   \n",
       "1                 96   21431294          95   ...      0.448    8    -6.312   \n",
       "2                 91   11594549          95   ...      0.859    5    -4.877   \n",
       "3                 91    3144195          97   ...      0.523    2    -4.333   \n",
       "4                 96    3309267          90   ...      0.535    5    -6.637   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0     1       0.0963        0.0244          0.000056    0.4980    0.344   \n",
       "1     1       0.0232        0.1630          0.000000    0.1060    0.168   \n",
       "2     0       0.0996        0.0185          0.000000    0.0215    0.926   \n",
       "3     1       0.0300        0.1840          0.000036    0.1320    0.394   \n",
       "4     0       0.0776        0.1300          0.000130    0.1430    0.127   \n",
       "\n",
       "     tempo  \n",
       "0   77.170  \n",
       "1   95.050  \n",
       "2  105.115  \n",
       "3  104.988  \n",
       "4  159.764  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6237 entries, 0 to 6236\n",
      "Data columns (total 21 columns):\n",
      "id                   6237 non-null int64\n",
      "title                6237 non-null object\n",
      "artist               6237 non-null object\n",
      "track_id             6237 non-null object\n",
      "artist_id            6237 non-null object\n",
      "rank                 6237 non-null int64\n",
      "hit                  6237 non-null object\n",
      "artist_popularity    6237 non-null int64\n",
      "followers            6237 non-null int64\n",
      "popularity           6237 non-null int64\n",
      "danceability         6237 non-null float64\n",
      "energy               6237 non-null float64\n",
      "key                  6237 non-null int64\n",
      "loudness             6237 non-null float64\n",
      "mode                 6237 non-null int64\n",
      "speechiness          6237 non-null float64\n",
      "acousticness         6237 non-null float64\n",
      "instrumentalness     6237 non-null float64\n",
      "liveness             6237 non-null float64\n",
      "valence              6237 non-null float64\n",
      "tempo                6237 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 1023.3+ KB\n"
     ]
    }
   ],
   "source": [
    "track.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.merge(billboard,track,on=['title','artist']) #merge the two table by title and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merge_data['rank_y']\n",
    "del merge_data['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>2</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title      artist  rank_x        date  id                track_id  \\\n",
       "0  God's Plan       Drake       1  2018-02-17   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "1  God's Plan       Drake       1  2018-02-10   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "2  God's Plan       Drake       1  2018-02-03   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "3     Perfect  Ed Sheeran       2  2018-02-17   2  0tgVpDi06FyKpA1z0VMD4v   \n",
       "4     Perfect  Ed Sheeran       2  2018-02-10   2  0tgVpDi06FyKpA1z0VMD4v   \n",
       "\n",
       "                artist_id  artist_popularity  followers  popularity  ...    \\\n",
       "0  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "1  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "2  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "3  6eUKZXaKkcviH0Ku9w2n3V                 96   21431294          95  ...     \n",
       "4  6eUKZXaKkcviH0Ku9w2n3V                 96   21431294          95  ...     \n",
       "\n",
       "   energy  key  loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "0   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "1   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "2   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "3   0.448    8    -6.312     1       0.0232        0.1630          0.000000   \n",
       "4   0.448    8    -6.312     1       0.0232        0.1630          0.000000   \n",
       "\n",
       "   liveness  valence  tempo  \n",
       "0     0.498    0.344  77.17  \n",
       "1     0.498    0.344  77.17  \n",
       "2     0.498    0.344  77.17  \n",
       "3     0.106    0.168  95.05  \n",
       "4     0.106    0.168  95.05  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 89689 entries, 0 to 89688\n",
      "Data columns (total 21 columns):\n",
      "title                89689 non-null object\n",
      "artist               89689 non-null object\n",
      "rank_x               89689 non-null int64\n",
      "date                 89689 non-null object\n",
      "id                   89689 non-null int64\n",
      "track_id             89689 non-null object\n",
      "artist_id            89689 non-null object\n",
      "artist_popularity    89689 non-null int64\n",
      "followers            89689 non-null int64\n",
      "popularity           89689 non-null int64\n",
      "danceability         89689 non-null float64\n",
      "energy               89689 non-null float64\n",
      "key                  89689 non-null int64\n",
      "loudness             89689 non-null float64\n",
      "mode                 89689 non-null int64\n",
      "speechiness          89689 non-null float64\n",
      "acousticness         89689 non-null float64\n",
      "instrumentalness     89689 non-null float64\n",
      "liveness             89689 non-null float64\n",
      "valence              89689 non-null float64\n",
      "tempo                89689 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 15.1+ MB\n"
     ]
    }
   ],
   "source": [
    "merge_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = merge_data.sort_values(by=['date','rank_x'],ascending=True)  #sort the merger data according to date and rank_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.to_csv(\"merge_data.csv\", index = None)  #save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merge_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity   ...     energy  key  loudness  mode  speechiness  \\\n",
       "0    1465113          44   ...      0.504    0    -7.260     1       0.0301   \n",
       "1     160121          56   ...      0.531    8    -5.300     1       0.0361   \n",
       "2     816631          74   ...      0.505    2    -8.926     0       0.2450   \n",
       "3     865667          50   ...      0.514    7    -5.021     1       0.0271   \n",
       "4     372926          60   ...      0.551    0    -7.328     1       0.0616   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  \n",
       "0       0.66900          0.000000    0.1100    0.266  112.681  \n",
       "1       0.20200          0.000012    0.5950    0.399  119.957  \n",
       "2       0.03930          0.000000    0.0923    0.495   99.935  \n",
       "3       0.38200          0.000000    0.1860    0.148  135.819  \n",
       "4       0.00364          0.000000    0.0451    0.796  100.904  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label every songs “Yse” or \"No\" by their rank in given date, if rank lower than 30, label \"Yes\", otherwise, label \"No\" \n",
    "peak = []\n",
    "for i in range(0, len(df)):\n",
    "    tmp = []\n",
    "    c = df['rank_x'][i]\n",
    "    if c < 30:\n",
    "        tmp.append(\"Yes\")\n",
    "    else:\n",
    "        tmp.append(\"No\")\n",
    "    peak.append(tmp)\n",
    "df_peak = pd.DataFrame(peak)\n",
    "df_peak.columns = ['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89689 entries, 0 to 89688\n",
      "Data columns (total 1 columns):\n",
      "hit    89689 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 700.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_peak.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df, df_peak], axis=1)  #concat df with df_peak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity ...   key  loudness  mode  speechiness  acousticness  \\\n",
       "0    1465113          44 ...     0    -7.260     1       0.0301       0.66900   \n",
       "1     160121          56 ...     8    -5.300     1       0.0361       0.20200   \n",
       "2     816631          74 ...     2    -8.926     0       0.2450       0.03930   \n",
       "3     865667          50 ...     7    -5.021     1       0.0271       0.38200   \n",
       "4     372926          60 ...     0    -7.328     1       0.0616       0.00364   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  hit  \n",
       "0          0.000000    0.1100    0.266  112.681  Yes  \n",
       "1          0.000012    0.5950    0.399  119.957  Yes  \n",
       "2          0.000000    0.0923    0.495   99.935  Yes  \n",
       "3          0.000000    0.1860    0.148  135.819  Yes  \n",
       "4          0.000000    0.0451    0.796  100.904  Yes  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"billboard_charts.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days with the first day\n",
    "time = []\n",
    "for i in range(0, len(result)):\n",
    "    tmp = []\n",
    "    if i == 0:\n",
    "        tmp.append(\"0\")\n",
    "    else:\n",
    "        df1 = parser.parse(result['date'][0])\n",
    "        df2 = parser.parse(result['date'][i])\n",
    "        df = (df2-df1).days\n",
    "        #print(df)\n",
    "        tmp.append(df)\n",
    "    time.append(tmp)\n",
    "df_days = pd.DataFrame(time)\n",
    "df_days.columns= ['days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_data = pd.concat([result, df_days], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>hit</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity  ...   loudness  mode  speechiness  acousticness  \\\n",
       "0    1465113          44  ...     -7.260     1       0.0301       0.66900   \n",
       "1     160121          56  ...     -5.300     1       0.0361       0.20200   \n",
       "2     816631          74  ...     -8.926     0       0.2450       0.03930   \n",
       "3     865667          50  ...     -5.021     1       0.0271       0.38200   \n",
       "4     372926          60  ...     -7.328     1       0.0616       0.00364   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  hit  days  \n",
       "0          0.000000    0.1100    0.266  112.681  Yes     0  \n",
       "1          0.000012    0.5950    0.399  119.957  Yes     0  \n",
       "2          0.000000    0.0923    0.495   99.935  Yes     0  \n",
       "3          0.000000    0.1860    0.148  135.819  Yes     0  \n",
       "4          0.000000    0.0451    0.796  100.904  Yes     0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_data.to_csv('LSTM_data.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Full_data.csv\") # read data\n",
    "df = df[['rank','artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    "         ,'instrumentalness','liveness','valence','tempo']]\n",
    "data_zs = preprocessing.normalize(df, norm='l2') # normalization\n",
    "data_zs = pd.DataFrame(data_zs)\n",
    "data_zs.columns = ['rank','artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    "         ,'instrumentalness','liveness','valence','tempo'] # add columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>-0.796614</td>\n",
       "      <td>0.770435</td>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.947498</td>\n",
       "      <td>0.860511</td>\n",
       "      <td>-0.763978</td>\n",
       "      <td>0.744715</td>\n",
       "      <td>0.630041</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.799281</td>\n",
       "      <td>0.877603</td>\n",
       "      <td>0.890368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist_popularity</th>\n",
       "      <td>0.705908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.581229</td>\n",
       "      <td>0.941691</td>\n",
       "      <td>0.767426</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>-0.621750</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.619311</td>\n",
       "      <td>0.529364</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>0.792695</td>\n",
       "      <td>0.796074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>followers</th>\n",
       "      <td>-0.796614</td>\n",
       "      <td>-0.581229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.702855</td>\n",
       "      <td>-0.899792</td>\n",
       "      <td>-0.802854</td>\n",
       "      <td>-0.831456</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>-0.707081</td>\n",
       "      <td>-0.702654</td>\n",
       "      <td>-0.628552</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.772676</td>\n",
       "      <td>-0.805326</td>\n",
       "      <td>-0.909881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popularity</th>\n",
       "      <td>0.770435</td>\n",
       "      <td>0.941691</td>\n",
       "      <td>-0.702855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848601</td>\n",
       "      <td>0.835431</td>\n",
       "      <td>0.752888</td>\n",
       "      <td>-0.711136</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.618129</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.866959</td>\n",
       "      <td>0.856565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.767426</td>\n",
       "      <td>-0.899792</td>\n",
       "      <td>0.848601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953093</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>-0.907028</td>\n",
       "      <td>0.776749</td>\n",
       "      <td>0.803351</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.131064</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>0.943912</td>\n",
       "      <td>0.958921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.947498</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>-0.802854</td>\n",
       "      <td>0.835431</td>\n",
       "      <td>0.953093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857838</td>\n",
       "      <td>-0.800408</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.729046</td>\n",
       "      <td>0.594591</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>0.853236</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>0.923472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <td>0.860511</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>-0.831456</td>\n",
       "      <td>0.752888</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>0.857838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.867666</td>\n",
       "      <td>0.706227</td>\n",
       "      <td>0.657342</td>\n",
       "      <td>0.703256</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>0.872165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>-0.763978</td>\n",
       "      <td>-0.621750</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>-0.711136</td>\n",
       "      <td>-0.907028</td>\n",
       "      <td>-0.800408</td>\n",
       "      <td>-0.867666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.765798</td>\n",
       "      <td>-0.783093</td>\n",
       "      <td>-0.588246</td>\n",
       "      <td>-0.210410</td>\n",
       "      <td>-0.783172</td>\n",
       "      <td>-0.775187</td>\n",
       "      <td>-0.911791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>0.744715</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>-0.707081</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>0.776749</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.706227</td>\n",
       "      <td>-0.765798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.593416</td>\n",
       "      <td>0.377448</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.771454</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>0.765360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.630041</td>\n",
       "      <td>0.619311</td>\n",
       "      <td>-0.702654</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.803351</td>\n",
       "      <td>0.729046</td>\n",
       "      <td>0.657342</td>\n",
       "      <td>-0.783093</td>\n",
       "      <td>0.593416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503829</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.730178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.529364</td>\n",
       "      <td>-0.628552</td>\n",
       "      <td>0.618129</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.594591</td>\n",
       "      <td>0.703256</td>\n",
       "      <td>-0.588246</td>\n",
       "      <td>0.377448</td>\n",
       "      <td>0.503829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.538177</td>\n",
       "      <td>0.652968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.131064</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>-0.210410</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.162468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>0.799281</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>-0.772676</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>0.853236</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>-0.783172</td>\n",
       "      <td>0.771454</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785476</td>\n",
       "      <td>0.801417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.877603</td>\n",
       "      <td>0.792695</td>\n",
       "      <td>-0.805326</td>\n",
       "      <td>0.866959</td>\n",
       "      <td>0.943912</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>-0.775187</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.538177</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.785476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>0.890368</td>\n",
       "      <td>0.796074</td>\n",
       "      <td>-0.909881</td>\n",
       "      <td>0.856565</td>\n",
       "      <td>0.958921</td>\n",
       "      <td>0.923472</td>\n",
       "      <td>0.872165</td>\n",
       "      <td>-0.911791</td>\n",
       "      <td>0.765360</td>\n",
       "      <td>0.730178</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.801417</td>\n",
       "      <td>0.881241</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       rank  artist_popularity  followers  popularity  \\\n",
       "rank               1.000000           0.705908  -0.796614    0.770435   \n",
       "artist_popularity  0.705908           1.000000  -0.581229    0.941691   \n",
       "followers         -0.796614          -0.581229   1.000000   -0.702855   \n",
       "popularity         0.770435           0.941691  -0.702855    1.000000   \n",
       "danceability       0.907101           0.767426  -0.899792    0.848601   \n",
       "energy             0.947498           0.771203  -0.802854    0.835431   \n",
       "key                0.860511           0.676481  -0.831456    0.752888   \n",
       "loudness          -0.763978          -0.621750   0.907869   -0.711136   \n",
       "mode               0.744715           0.471224  -0.707081    0.548270   \n",
       "speechiness        0.630041           0.619311  -0.702654    0.707142   \n",
       "acousticness       0.694758           0.529364  -0.628552    0.618129   \n",
       "instrumentalness   0.138772           0.025657  -0.069452    0.012811   \n",
       "liveness           0.799281           0.541195  -0.772676    0.644572   \n",
       "valence            0.877603           0.792695  -0.805326    0.866959   \n",
       "tempo              0.890368           0.796074  -0.909881    0.856565   \n",
       "\n",
       "                   danceability    energy       key  loudness      mode  \\\n",
       "rank                   0.907101  0.947498  0.860511 -0.763978  0.744715   \n",
       "artist_popularity      0.767426  0.771203  0.676481 -0.621750  0.471224   \n",
       "followers             -0.899792 -0.802854 -0.831456  0.907869 -0.707081   \n",
       "popularity             0.848601  0.835431  0.752888 -0.711136  0.548270   \n",
       "danceability           1.000000  0.953093  0.886522 -0.907028  0.776749   \n",
       "energy                 0.953093  1.000000  0.857838 -0.800408  0.785333   \n",
       "key                    0.886522  0.857838  1.000000 -0.867666  0.706227   \n",
       "loudness              -0.907028 -0.800408 -0.867666  1.000000 -0.765798   \n",
       "mode                   0.776749  0.785333  0.706227 -0.765798  1.000000   \n",
       "speechiness            0.803351  0.729046  0.657342 -0.783093  0.593416   \n",
       "acousticness           0.592107  0.594591  0.703256 -0.588246  0.377448   \n",
       "instrumentalness       0.131064  0.141285  0.098196 -0.210410  0.006030   \n",
       "liveness               0.857685  0.853236  0.767902 -0.783172  0.771454   \n",
       "valence                0.943912  0.939267  0.828896 -0.775187  0.717171   \n",
       "tempo                  0.958921  0.923472  0.872165 -0.911791  0.765360   \n",
       "\n",
       "                   speechiness  acousticness  instrumentalness  liveness  \\\n",
       "rank                  0.630041      0.694758          0.138772  0.799281   \n",
       "artist_popularity     0.619311      0.529364          0.025657  0.541195   \n",
       "followers            -0.702654     -0.628552         -0.069452 -0.772676   \n",
       "popularity            0.707142      0.618129          0.012811  0.644572   \n",
       "danceability          0.803351      0.592107          0.131064  0.857685   \n",
       "energy                0.729046      0.594591          0.141285  0.853236   \n",
       "key                   0.657342      0.703256          0.098196  0.767902   \n",
       "loudness             -0.783093     -0.588246         -0.210410 -0.783172   \n",
       "mode                  0.593416      0.377448          0.006030  0.771454   \n",
       "speechiness           1.000000      0.503829          0.023720  0.763651   \n",
       "acousticness          0.503829      1.000000          0.010102  0.508264   \n",
       "instrumentalness      0.023720      0.010102          1.000000  0.049136   \n",
       "liveness              0.763651      0.508264          0.049136  1.000000   \n",
       "valence               0.790136      0.538177          0.033807  0.785476   \n",
       "tempo                 0.730178      0.652968          0.162468  0.801417   \n",
       "\n",
       "                    valence     tempo  \n",
       "rank               0.877603  0.890368  \n",
       "artist_popularity  0.792695  0.796074  \n",
       "followers         -0.805326 -0.909881  \n",
       "popularity         0.866959  0.856565  \n",
       "danceability       0.943912  0.958921  \n",
       "energy             0.939267  0.923472  \n",
       "key                0.828896  0.872165  \n",
       "loudness          -0.775187 -0.911791  \n",
       "mode               0.717171  0.765360  \n",
       "speechiness        0.790136  0.730178  \n",
       "acousticness       0.538177  0.652968  \n",
       "instrumentalness   0.033807  0.162468  \n",
       "liveness           0.785476  0.801417  \n",
       "valence            1.000000  0.881241  \n",
       "tempo              0.881241  1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_zs.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d517047400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d511e099b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = sns.diverging_palette(0, 255, sep=1, n=256, as_cmap=True)\n",
    "sns.heatmap(data_zs.corr(), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True  True False  True  True  True  True  True  True\n",
      "  True False]\n",
      "[2 4 1 1 1 3 1 1 1 1 1 1 1 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Use RFE to select feature\n",
    "logreg = LinearRegression()\n",
    "rfe = RFE(logreg, 10)\n",
    "X = ['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    ",'instrumentalness','liveness','valence','tempo']\n",
    "y = ['rank']\n",
    "rfe = rfe.fit(data_zs[X], data_zs[y] )\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>rank</td>       <th>  R-squared:         </th>  <td>   0.942</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.942</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.442e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 21 Apr 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:41:40</td>     <th>  Log-Likelihood:    </th>  <td>  23248.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6237</td>      <th>  AIC:               </th> <td>-4.648e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6229</td>      <th>  BIC:               </th> <td>-4.643e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-5.497e-05</td> <td> 7.41e-05</td> <td>   -0.741</td> <td> 0.458</td> <td>   -0.000</td> <td> 9.04e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>   17.7715</td> <td>    1.288</td> <td>   13.794</td> <td> 0.000</td> <td>   15.246</td> <td>   20.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>           <td>   65.7134</td> <td>    1.394</td> <td>   47.132</td> <td> 0.000</td> <td>   62.980</td> <td>   68.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>   11.8673</td> <td>    1.339</td> <td>    8.865</td> <td> 0.000</td> <td>    9.243</td> <td>   14.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>   21.1383</td> <td>    1.952</td> <td>   10.827</td> <td> 0.000</td> <td>   17.311</td> <td>   24.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>    9.6370</td> <td>    1.689</td> <td>    5.706</td> <td> 0.000</td> <td>    6.326</td> <td>   12.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td> -104.4557</td> <td>    2.516</td> <td>  -41.519</td> <td> 0.000</td> <td> -109.388</td> <td>  -99.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>   52.0083</td> <td>    0.906</td> <td>   57.384</td> <td> 0.000</td> <td>   50.232</td> <td>   53.785</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4646.598</td> <th>  Durbin-Watson:     </th>   <td>   1.993</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>32058791.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-1.813</td>  <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>354.211</td> <th>  Cond. No.          </th>   <td>4.08e+04</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   rank   R-squared:                       0.942\n",
       "Model:                            OLS   Adj. R-squared:                  0.942\n",
       "Method:                 Least Squares   F-statistic:                 1.442e+04\n",
       "Date:                Sat, 21 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        12:41:40   Log-Likelihood:                 23248.\n",
       "No. Observations:                6237   AIC:                        -4.648e+04\n",
       "Df Residuals:                    6229   BIC:                        -4.643e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -5.497e-05   7.41e-05     -0.741      0.458      -0.000    9.04e-05\n",
       "danceability        17.7715      1.288     13.794      0.000      15.246      20.297\n",
       "energy              65.7134      1.394     47.132      0.000      62.980      68.447\n",
       "valence             11.8673      1.339      8.865      0.000       9.243      14.491\n",
       "liveness            21.1383      1.952     10.827      0.000      17.311      24.966\n",
       "instrumentalness     9.6370      1.689      5.706      0.000       6.326      12.948\n",
       "speechiness       -104.4557      2.516    -41.519      0.000    -109.388     -99.524\n",
       "acousticness        52.0083      0.906     57.384      0.000      50.232      53.785\n",
       "==============================================================================\n",
       "Omnibus:                     4646.598   Durbin-Watson:                   1.993\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         32058791.271\n",
       "Skew:                          -1.813   Prob(JB):                         0.00\n",
       "Kurtosis:                     354.211   Cond. No.                     4.08e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.08e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_zs[[\"danceability\",\"energy\",'valence','liveness','instrumentalness','speechiness','acousticness']]\n",
    "X = sm.add_constant(x)\n",
    "y = data_zs[\"rank\"]\n",
    "model = sm.OLS(y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>rank</td>       <th>  R-squared:         </th>  <td>   0.953</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.952</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.041e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 21 Apr 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:41:40</td>     <th>  Log-Likelihood:    </th>  <td>  23880.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6237</td>      <th>  AIC:               </th> <td>-4.773e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6224</td>      <th>  BIC:               </th> <td>-4.765e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.0472</td> <td>    0.009</td> <td>   -5.532</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artist_popularity</th> <td>    0.3048</td> <td>    0.045</td> <td>    6.810</td> <td> 0.000</td> <td>    0.217</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>followers</th>         <td>    0.0472</td> <td>    0.009</td> <td>    5.534</td> <td> 0.000</td> <td>    0.030</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>        <td>   -0.7923</td> <td>    0.035</td> <td>  -22.894</td> <td> 0.000</td> <td>   -0.860</td> <td>   -0.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>      <td>   61.4343</td> <td>    2.138</td> <td>   28.735</td> <td> 0.000</td> <td>   57.243</td> <td>   65.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>            <td>   51.4285</td> <td>    1.520</td> <td>   33.842</td> <td> 0.000</td> <td>   48.449</td> <td>   54.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>               <td>   -0.1194</td> <td>    0.095</td> <td>   -1.260</td> <td> 0.208</td> <td>   -0.305</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>          <td>    1.6473</td> <td>    0.143</td> <td>   11.529</td> <td> 0.000</td> <td>    1.367</td> <td>    1.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>              <td>    7.2519</td> <td>    0.593</td> <td>   12.233</td> <td> 0.000</td> <td>    6.090</td> <td>    8.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>       <td>  -85.3607</td> <td>    2.923</td> <td>  -29.198</td> <td> 0.000</td> <td>  -91.092</td> <td>  -79.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>      <td>   70.5195</td> <td>    1.142</td> <td>   61.745</td> <td> 0.000</td> <td>   68.281</td> <td>   72.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th>  <td>   14.0853</td> <td>    1.931</td> <td>    7.293</td> <td> 0.000</td> <td>   10.299</td> <td>   17.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>           <td>   12.6765</td> <td>    1.435</td> <td>    8.835</td> <td> 0.000</td> <td>    9.864</td> <td>   15.489</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4696.602</td> <th>  Durbin-Watson:     </th>   <td>   1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>15831316.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.988</td>  <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>249.786</td> <th>  Cond. No.          </th>   <td>6.82e+04</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   rank   R-squared:                       0.953\n",
       "Model:                            OLS   Adj. R-squared:                  0.952\n",
       "Method:                 Least Squares   F-statistic:                 1.041e+04\n",
       "Date:                Sat, 21 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        12:41:40   Log-Likelihood:                 23880.\n",
       "No. Observations:                6237   AIC:                        -4.773e+04\n",
       "Df Residuals:                    6224   BIC:                        -4.765e+04\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.0472      0.009     -5.532      0.000      -0.064      -0.030\n",
       "artist_popularity     0.3048      0.045      6.810      0.000       0.217       0.392\n",
       "followers             0.0472      0.009      5.534      0.000       0.030       0.064\n",
       "popularity           -0.7923      0.035    -22.894      0.000      -0.860      -0.724\n",
       "danceability         61.4343      2.138     28.735      0.000      57.243      65.625\n",
       "energy               51.4285      1.520     33.842      0.000      48.449      54.408\n",
       "key                  -0.1194      0.095     -1.260      0.208      -0.305       0.066\n",
       "loudness              1.6473      0.143     11.529      0.000       1.367       1.927\n",
       "mode                  7.2519      0.593     12.233      0.000       6.090       8.414\n",
       "speechiness         -85.3607      2.923    -29.198      0.000     -91.092     -79.630\n",
       "acousticness         70.5195      1.142     61.745      0.000      68.281      72.758\n",
       "instrumentalness     14.0853      1.931      7.293      0.000      10.299      17.871\n",
       "valence              12.6765      1.435      8.835      0.000       9.864      15.489\n",
       "==============================================================================\n",
       "Omnibus:                     4696.602   Durbin-Watson:                   1.998\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15831316.449\n",
       "Skew:                           1.988   Prob(JB):                         0.00\n",
       "Kurtosis:                     249.786   Cond. No.                     6.82e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.82e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_zs[['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    ",'instrumentalness','valence']]\n",
    "X = sm.add_constant(x)\n",
    "y = data_zs[\"rank\"]\n",
    "model = sm.OLS(y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Full_data.csv\")\n",
    "data.hit.replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "data_zs = max_abs_scaler.fit_transform(data[['artist_popularity','followers','popularity','energy','loudness','mode','tempo']])\n",
    "data_zs = pd.DataFrame(data_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.854395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455823</td>\n",
       "      <td>-0.430979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.449799</td>\n",
       "      <td>-0.286714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.540994</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.862450</td>\n",
       "      <td>-0.221531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.146706</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>-0.196820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.491202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.154408</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.537149</td>\n",
       "      <td>-0.301476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4    5         6\n",
       "0  1.00  0.854395  1.000000  0.455823 -0.430979  1.0  0.361051\n",
       "1  0.96  0.999971  0.959596  0.449799 -0.286714  1.0  0.444705\n",
       "2  0.91  0.540994  0.959596  0.862450 -0.221531  0.0  0.491796\n",
       "3  0.91  0.146706  0.979798  0.525100 -0.196820  1.0  0.491202\n",
       "4  0.96  0.154408  0.909091  0.537149 -0.301476  0.0  0.747479"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_zs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582265\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>hit</td>       <th>  No. Observations:  </th>   <td>  6237</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  6230</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 21 Apr 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.09279</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>12:41:40</td>     <th>  Log-Likelihood:    </th>  <td> -3631.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -4003.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>3.328e-157</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th> <td>   -3.0881</td> <td>    0.286</td> <td>  -10.784</td> <td> 0.000</td> <td>   -3.649</td> <td>   -2.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th> <td>    1.6158</td> <td>    0.179</td> <td>    9.044</td> <td> 0.000</td> <td>    1.266</td> <td>    1.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th> <td>    6.0623</td> <td>    0.276</td> <td>   21.999</td> <td> 0.000</td> <td>    5.522</td> <td>    6.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th> <td>   -0.9523</td> <td>    0.167</td> <td>   -5.716</td> <td> 0.000</td> <td>   -1.279</td> <td>   -0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th> <td>    2.7656</td> <td>    0.309</td> <td>    8.946</td> <td> 0.000</td> <td>    2.160</td> <td>    3.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th> <td>   -0.1689</td> <td>    0.060</td> <td>   -2.824</td> <td> 0.005</td> <td>   -0.286</td> <td>   -0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th> <td>   -1.2393</td> <td>    0.205</td> <td>   -6.043</td> <td> 0.000</td> <td>   -1.641</td> <td>   -0.837</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    hit   No. Observations:                 6237\n",
       "Model:                          Logit   Df Residuals:                     6230\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Sat, 21 Apr 2018   Pseudo R-squ.:                 0.09279\n",
       "Time:                        12:41:40   Log-Likelihood:                -3631.6\n",
       "converged:                       True   LL-Null:                       -4003.0\n",
       "                                        LLR p-value:                3.328e-157\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0             -3.0881      0.286    -10.784      0.000      -3.649      -2.527\n",
       "1              1.6158      0.179      9.044      0.000       1.266       1.966\n",
       "2              6.0623      0.276     21.999      0.000       5.522       6.602\n",
       "3             -0.9523      0.167     -5.716      0.000      -1.279      -0.626\n",
       "4              2.7656      0.309      8.946      0.000       2.160       3.371\n",
       "5             -0.1689      0.060     -2.824      0.005      -0.286      -0.052\n",
       "6             -1.2393      0.205     -6.043      0.000      -1.641      -0.837\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_model =sm.Logit(data['hit'],data_zs).fit()\n",
    "L_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross-validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64       0.688      0.664      0.696      0.672      0.688\n",
      " 0.736      0.656      0.69354839 0.69354839]\n",
      "Accuracy: 0.68 (+/- 0.05)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['popularity', 'danceability', 'energy', 'key', 'loudness', 'valence', 'tempo']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_zs\n",
    "y = data[\"hit\"]\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=123)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "['popularity','danceability','energy','key','loudness','valence','tempo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[730  90]\n",
      " [319 109]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xm8zPX+wPHX2y5blhZZckjdKKlkKXW1SEmb9kVEirJ0pbRQFFelRX6UVC4tIrSohErS7VpPHUKbkA6K7DvHef/++HznGMecOXOOM/OdmfN+Ph7zODPf+X6/857vOWfe89lFVTHGGGNyUsTvAIwxxsQ3SxTGGGPCskRhjDEmLEsUxhhjwrJEYYwxJixLFMYYY8KyRGHyTERuE5EZfsfhNxGpKSI7RKRoDF+zloioiBSL1WtGk4gsFZEW+TjO/gZjSGwcRWITkVXAccABYAcwDeimqjv8jCsZedf6LlX9wscYagErgeKqmuFXHF4sCtRV1eVRfp1axMl7LqysRJEcrlTVskBD4EzgEZ/jyRc/vyUnyzf0vLDrbSJliSKJqOqfwHRcwgBAREqKyHMislpE/hKRkSJSOuj5q0UkTUS2ichvInKZt72CiLwhIutEZI2IDAxUsYhIBxH5r3d/pIg8FxyHiHwkIr28+yeIyGQR2SAiK0WkR9B+/UVkkoi8LSLbgA7Z35MXx5ve8b+LSF8RKRIUx7ci8n8islVEfhKRi7MdG+49fCsiL4rIJqC/iNQRkZkislFE/haRd0TkaG//t4CawMdeddND2auBRGSWiDzlnXe7iMwQkSpB8dzhvYeNItJPRFaJyCWhfpciUlpEnvf23yoi/w3+vQG3eb/Tv0XksaDjGovIHBHZ4r3v4SJSIuh5FZH7RORX4Fdv20si8of3N5AqIucH7V9URB71/ja2e8/XEJHZ3i6LvOtxk7d/G+/vaYuI/E9EGgSda5WI9BGRxcBOESkWfA282Bd6cfwlIi94hwZea4v3Ws2C/wa9Y+uLyOcissk79tFQ19Xkk6raLYFvwCrgEu9+deAH4KWg54cCU4BKQDngY2Cw91xjYCvQEveloRrwD++5D4FXgTLAscB84B7vuQ7Af737FwB/cLAasyKwGzjBO2cq8DhQAqgNrABaefv2B/YD13j7lg7x/t4EPvJirwX8AnQKiiMD+BdQHLjJez+VInwPGUB3oBhQGjjJuxYlgWNwH1BDQ11r73EtQIFi3uNZwG/Ayd75ZgFPe8/Vw1UNNveuxXPee78kh9/rCO/4akBR4FwvrsBrvua9xhnAXuBU77izgabee6oF/AjcH3ReBT7H/T2U9rbdDlT2jnkA+BMo5T33IO5v6hRAvNerHHSuk4LOfRawHmjixdzeu2Ylg65fGlAj6LWzrikwB2jn3S8LNA11nUP8DZYD1nmxl/IeN/H7fzOZbr4HYLcj/AW6f7QdwHbvn+lL4GjvOQF2AnWC9m8GrPTuvwq8GOKcx3kfPqWDtt0CfOXdD/4nFWA1cIH3uDMw07vfBFid7dyPAP/x7vcHZod5b0W9OOoFbbsHmBUUx1q8JOVtmw+0i/A9rM7ptb19rgG+z3atc0sUfYOevxeY5t1/HHg36LmjgH2ESBS4pLkbOCPEc4HXrJ7tPd+cw3u4H/gg6LECF+XyvjcHXhv4Gbg6h/2yJ4pXgKey7fMz8M+g69cxxN9vIFHMBgYAVXJ4zzkliluCf092K/ib1RMmh2tU9QsR+ScwDqgCbMF9Kz4KSBWRwL6C+wAG981uaojznYj7hr4u6LgiuJLDIVRVRWQ87p91NnAr8HbQeU4QkS1BhxQFvgl6fNg5g1TBffv+PWjb77hv2QFr1Pu0CHr+hAjfwyGvLSLHAsOA83HfSovgPjTz4s+g+7tw34zxYsp6PVXdJSIbczhHFdw349/y+joicjLwAtAI97svhivVBcv+vh8A7vJiVKC8FwO4v5FwcQQ7EWgvIt2DtpXwzhvytbPpBDwJ/CQiK4EBqvpJBK+blxhNPlgbRRJR1a+BMbhqDYC/cd9M66vq0d6tgrqGb3D/tHVCnOoP3LfxKkHHlVfV+jm89LvA9SJyIq4UMTnoPCuDznG0qpZT1dbBYYd5S3/jqmdODNpWE1gT9LiaBGUC7/m1Eb6H7K892NvWQFXL46pkJMz+ebEOVzUIuDYIXHVPKH8Dewj9u8nNK8BPuN5I5YFHOfQ9QND78Noj+gA3AhVV9Whc9V3gmJz+RkL5AxiU7fd9lKq+G+q1s1PVX1X1Flw14TPAJBEpE+6YfMRo8sESRfIZCrQUkYaqmomry37R+7aMiFQTkVbevm8Ad4rIxSJSxHvuH6q6DpgBPC8i5b3n6ngllsOo6vfABuB1YLqqBkoQ84FtXgNmaa9h9DQROSeSN6KqB4D3gEEiUs5LRL04WGIB96HSQ0SKi8gNwKnA1Ly+B085XDXeFhGphqufD/YXrp0lPyYBV4rIuV7j8gAO/wAHwPu9jQZeENcZoKjXgFsygtcpB2wDdojIP4CuEeyfgfv9FRORx3ElioDXgadEpK44DUQkkOCyX4/XgC4i0sTbt4yIXCEi5SKIGxG5XUSO8d5/4G/ogBdbJjlf+0+A40XkfnGdN8qJSJNIXtNExhJFklHVDbgG4H7epj7AcmCuuJ5FX+AaJlHV+cCdwIu4b5Ffc/Db+x24aoNluOqXSUDVMC/9LnAJruorEMsB4EpcL6yVuG/KrwMV8vCWuuPaWVYA//XOPzro+XlAXe/cg4DrVTVQpZPX9zAA1yC7FfgUeD/b84OBvl6Pnt55eA+o6lLvvYzHlS624xp+9+ZwSG9cI/ICYBPuG3Yk/6+9cdV/23Ef3BNy2X868Bmuk8DvuJJMcPXQC7hkPQOXgN7ANaKDa2Ma612PG1V1Ia6Najjuei8nRE+2MC4DlorIDuAlXLvLHlXdhfvdfuu9VtPgg1R1O64TwpW4KrlfgQvz8LomFzbgziQsEemAGwDX3O9Y8kpEyuK+NddV1ZV+x2NMOFaiMCZGRORKETnKq3d/DldiWOVvVMbkzhKFMbFzNa6hfS2uuuxmtSK9SQBW9WSMMSYsK1EYY4wJK+EG3FWpUkVr1arldxjGGJNQUlNT/1bVY/JzbMIlilq1arFw4UK/wzDGmIQiIr/nvldoVvVkjDEmLEsUxhhjwrJEYYwxJixLFMYYY8KyRGGMMSYsSxTGGGPCilqiEJHRIrJeRJbk8LyIyDARWS4ii0XkrGjFYowxJv+iWaIYg5s2OCeX4+a7qQvcjVtwxRhjTJyJ2oA7VZ0tIrXC7HI18KY3KdpcETlaRKp6C84YY4w5AuPmreaj79NpnPY156R9fUTn8nNkdjUOXSAl3dt2WKIQkbtxpQ5q1qwZk+CMMaagjZu3mo/S1uS+YwFYk/YjAz4fycW/LeD3aicd0bn8TBShloEMOZWtqo4CRgE0atTIprs1xsSdSJLAvJWbAGiSUim6wajy5tRnqbHhD3j+eU7s0QOKF8/36fxMFOlAjaDH1XHz9BtjTFwLlRQiSQJNUipxdcNq3NokSjUj//sfnH46lCsHTd6FKlWgRo3cj8uFn4liCtBNRMYDTYCt1j5hjIlH2RNDqKQQ9SQQzsaN8PDD8Prr8MQT0L8/nHlmgZ0+aolCRN4FWgBVRCQdeAIoDqCqI4GpQGvcAuy7gDujFYsxxuQmXNVR9sTga1IIpgpvvgm9e8PmzfDgg+5WwKLZ6+mWXJ5X4L5ovb4xpnDLa8NxuKqjuEkM2fXpA0OGwLnnwsiRrtopChJuPQpjjMkuv20GweI2GWS3ezfs3OnaHzp1grp13c8i0RsWZ4nCGJPwPkpbw7J126hXtXzWtoT54M+LadPgvvugYUOYPBlOOcXdoswShTEmoYQqPQSSxIR7mvkUVZStXQv33w8TJ7rE0K1bTF/eEoUxJi7l1MYQqkqpXtXyXN2wWsxii6kvv4Rrr4V9++Cpp1xjdcmSMQ3BEoUxJu6Mm7eaRz/4ATi8jSEpq5RC2b/fDZI74wxo3RoGDoSTjmyEdX5ZojDGxJXgJPHva09P/oSQ3bZt0K8fzJsH337rGq3Hj/c1JEsUxhjfhOutVOiShCpMmgQ9e8Kff8K998LevXDUUX5HZonCGBN7gQQRdyOc/bJhA7RvD5995kZUf/QRnHOO31FlsURhjImZUAmi0CWFUMqXh7//hqFDXffXYvH10Rxf0RhjklpgvIMlCGD2bBg0yI2HKFsW5s6N6qC5I2GJwhgTVcHtEEk/3iESf//turiOGQO1asGqVXDaaXGbJMAShTGmgIWbaTWpxzvkRhX+8x+XJLZtg0cegb5946KxOjeWKIwxRyw4OcTtTKvx4O23oV49N4Ff/fp+RxMxSxTGmHwL1ThtiSHIrl3w739Dly5Qvbprj6hQIa6rmUKxRGGMiUhuM7Racshm6lTXg2nVKqhWDbp2hYoV/Y4qXyxRGGNyFK5KKXDfEkQ26eluAr/Jk+HUU+Hrr+GCC/yO6ohYojDGhJR9viVLChEaNAg+/dRVOT3wAJQo4XdER8wShTHmMIV+vqW8mj8fSpd2K8wNHOh6NtWu7XdUBcYShTEmS/bGaUsSudi6FR59FF55Bdq0gSlToHJld0siliiMKSQiWUPaGqcjpAoTJsC//gXr10P37m6tiCRlicKYQiDc+g7BLEFE6O234Y47oFEj+OQTOPtsvyOKKksUxhQCgZKEVSUdgb17YcUK15PpxhshI8Mli6JF/Y4s6hJr1IcxJs/GzVvNvJWbaJJSyZJEfn31lVtprlUrlzBKloQ77ywUSQIsURiT1IKrnArtHEtHYv16V2q46CK3NOmoUTFfrzoeWNWTMUnKurgeoeXLoXFj2LEDHnvM3UqX9jsqX1iiMCaJhBpJbUkij7ZtcwsJ1akDnTpBx46uXaIQs6onY5JEoAQR3MXVkkQe7NwJffq4NSLS00EEhgwp9EkCrERhTMLKad0HSw758PHH0K0brF7tShEJsEZELFmiMCYB5DZza+CnjYHIo4wM19X1gw/c+hDffAPNm/sdVdyxRGFMHAu13kOAJYYjoOqqlooVg6pV4emn3SjrJJjALxosURgTR8ItI2pJoYDMnevWiXjtNTjrLBgxwu+I4p4lCmPiQE4lB0sQBWjzZjeB36uvwgknuMcmIlFNFCJyGfASUBR4XVWfzvZ8TWAscLS3z8OqOjWaMRnjN1spzgcTJkCPHvD3325RoQEDoFw5v6NKGFFLFCJSFBgBtATSgQUiMkVVlwXt1hd4T1VfEZF6wFSgVrRiMsYvtlKcz376yXV7nTYNzjzT72gSTjRLFI2B5aq6AkBExgNXA8GJQoHy3v0KwNooxmOML2ylOB/s2QPPPOPaIK680lU59e1baOZmKmjRTBTVgD+CHqcDTbLt0x+YISLdgTLAJaFOJCJ3A3cD1Kxp/1wmvtn4Bp998QXcey/8+qtbivTKK6F4cb+jSmjRTBQSYptme3wLMEZVnxeRZsBbInKaqmYecpDqKGAUQKNGjbKfw5i4YA3SPvvrL+jVC8aNg5NOghkzoGVLv6NKCtFMFOlAjaDH1Tm8aqkTcBmAqs4RkVJAFWB9FOMypsDk1PZgicEHn38OkybB44/DI49AqVJ+R5Q0opkoFgB1RSQFWAPcDNyabZ/VwMXAGBE5FSgFbIhiTMYUiFClB0sQPli0yFUxXX893HYbnHcepKT4HVXSiVqiUNUMEekGTMd1fR2tqktF5ElgoapOAR4AXhORf+GqpTqoqlUtmbiWvXHakoMPduyAJ56Al15yvZmuucaNsrYkERVRHUfhjYmYmm3b40H3lwHnRTMGYwqaLSvqsw8/hO7d3Qyvd98Ngwe7JGGixq6uMbnI3otp2bpttqyoX374Aa69Fk4/3Q2iO/dcvyMqFGw9CmPCyL7GA0C9quVtWdFY2r8fZs50908/HT79FFJTLUnEkJUojMnGVomLI//7H3TpAkuXws8/u26vrVv7HVWhY4nCGI/1ZIojmzbBww+7GV5r1ID333dJwvjCEoUp1GwcRBzaswcaNoS1a93I6v79oWxZv6Mq1CxRmEIl3HoPliB8lp4O1au7gXJPPeWSxRln+B2VwRKFKSRseo04tnu36+L6zDNuZPWVV0L79n5HZYJElChEpARQU1WXRzkeYwpUqARhiSGOzJjhJvD77Te4/XZo3NjviEwIuSYKEbkCeAEoAaSISEPgCVW9NtrBGXMkbAR1nOveHYYPh7p13YyvF1/sd0QmB5GUKJ7ETQ/+FYCqpomIdT8wcSt7KcK6tsaRAwfcz6JFoWlTqFIF+vSxCfziXCSJYr+qbhE5ZNZwm4/JxI1wDdRWiogj333nxkS0a+dKE7fd5ndEJkKRJIofReRGoIg3E2xPYG50wzImd9ZAnSC2b3dTfw8bBsccA1Wr+h2RyaNIEkU34HEgE3gfNxvsI9EMyphIfJS2JmveJUsMcWrGDOjY0Y2J6NIF/v1vOPpov6MyeRRJomilqn2APoENItIWlzSM8VW9quWZcE8zv8MwOSlRAo49FiZPhibZV0I2iSKSSQH7htj2WEEHYkykxs1bzU2vzmHZum1+h2Ky27/fjYd4zPuIaNECFi60JJHgcixRiEgr3DKl1UTkhaCnyuOqoYyJuVBdXk2c+O9/D07gd8MNkJkJRYq4m0lo4aqe1gNLgD3A0qDt24GHoxmUMaEEJwnr8hpHNm50XVzfeANq1oSPP4Y2bfyOyhSgHBOFqn4PfC8i76jqnhjGZMxhLEnEsY0bYfx4eOgh17upTBm/IzIFLJLG7GoiMgioB2SNilHVk6MWlTEeGzwXp378Ed57z61bffLJsHo1VKrkd1QmSiKpPBwD/AcQ4HLgPWB8FGMyBjh0dbkmKZUsScSDXbtcQ/UZZ8BLL7kZX8GSRJKLpERxlKpOF5HnVPU3oK+IfBPtwEzhZlVNcWjaNDeB38qVbnbXIUPcADqT9CJJFHvFzd/xm4h0AdYAx0Y3LFPYBabksCQRJ3bscFNvVK4MX33lur2aQiOSqqd/AWWBHsB5QGegYzSDMoXbuHmrs6qbLEn46MABePtt97NsWTfD66JFliQKoVxLFKo6z7u7HWgHICLVoxmUKbyCq5xsjISPUlPhnnvcz9Kl4brrbLW5QixsiUJEzhGRa0Skive4voi8iU0KaKLA2iXiwNat0KOHW0BozRrX7bVtW7+jMj4LNzJ7MHAdsAjXgP0BbubYZ4AusQnPFAbWBTaOXHcdzJwJ990HAwdChQp+R2TiQLiqp6uBM1R1t4hUAtZ6j3+OTWgmWdn6EXFmxQrXe6lcORg0yE25cc45fkdl4ki4RLFHVXcDqOomEfnJkoQpCIHpwetVLQ9YgvDNvn3w3HPw1FOuuumZZ2zyPhNSuERRW0QCU4kLUCvoMapqFZcmYsGliECSsOnBfTR7tpvA78cf4frrXaIwJgfhEsV12R4Pj2YgJrkFlyLqVS1vPZr89OKL0KsX1KoFn34KrVv7HZGJc+EmBfwyloGY5GelCB9lZsLOna4d4oorYMMG6NsXjjrK78hMAohkZLYx+RKqusn4YOlSV80UWGnu5JPdkqTGRCiqK4qIyGUi8rOILBeRkGtYiMiNIrJMRJaKyLhoxmNiI7ACXWBCP8Cqm/ywaxc88gg0bOjaItq0AVW/ozIJKOIShYiUVNW9edi/KDACaAmkAwtEZIqqLgvapy7wCHCeqm4WEZtDKgkE2iOsN5OPvv/eDZRbtQruvBOefRaqVPE7KpOgck0UItIYeAOoANQUkTOAu1S1ey6HNgaWq+oK7zzjcWMzlgXt0xkYoaqbAVR1fd7fgokXgaom69XkI1UQcSvN1awJY8fCBRf4HZVJcJGUKIYBbYAPAVR1kYhcGMFx1YA/gh6nA9k7aZ8MICLfAkWB/qo6LYJzmzgR3A6RfeCciaGMDBg+HKZMgc8/d7O8fv2131GZJBFJoiiiqr+7mcazHIjgOAmxLXsFaTGgLtACqA58IyKnqeqWQ04kcjdwN0DNmlaNES+C52ZqklLJqpr8Mn++a6z+/nu4/HLYtg0qVvQ7KpNEIkkUf3jVT+q1O3QHfonguHSgRtDj6rhpQLLvM1dV9wMrReRnXOJYELyTqo4CRgE0atTIWuPigE3gFwd27IA+feCVV6BqVZg40c3VJKG+oxmTf5H0euoK9AJqAn8BTb1tuVkA1BWRFBEpAdwMTMm2z4fAhQDeDLUnAysiC934yRYWigPFi8OsWdC9+8ER1pYkTBREUqLIUNWb83piVc0QkW7AdFz7w2hVXSoiTwILVXWK99ylIrIMV531oKpuzOtrmdiyhYV8tHw5PPkkjBjhBs+lpkKpUn5HZZJcJIligVclNAF4X1W3R3pyVZ0KTM227fGg+4orrfSK9JzGX7awkE/27nVdXAcNghIloHNnOP98SxImJnKtelLVOsBA4GzgBxH5UETyXMIwycGqnHzw1VdudbnHH4drroGffnJJwpgYiWhktqr+T1V7AGcB24B3ohqViWtW5RRDqq4UsX8/TJvmVpw74QS/ozKFTK6JQkTKishtIvIxMB/YAJwb9chM3Am0TZgoy8yE116DP/5wjdNvvQVLlkCrVn5HZgqpSEoUS3A9nZ5V1ZNU9QFVnRfluEwcClQ7WdtEFC1eDM2bw913w+uvu21Vq0Lp0v7GZQq1SBqza6tqZtQjMXEp+wywVu0UJTt2wIABbq2IihVhzBi44w6/ozIGCJMoROR5VX0AmCwihw1ysxXukl/2kdc2A2wU9e8Pzz8Pd90FTz/tpuAwJk6EK1FM8H7aynaFkI28joE//nCLCf3jH/Dww65HU/PmfkdlzGHCrXA337t7qqoekiy8gXS2Al4SCa5igoMT/FmSiIKMDBg2zHV3PftsN3lflSqWJEzciqQxu2OIbZ0KOhDjr8D04AFNUipZkoiGuXOhUSN44AFo0cJNA25MnAvXRnETbn6mFBF5P+ipcsCW0EeZRGZrSETZp5/ClVe6cRDvv++qmmxuJpMAwrVRzAc24mZ9HRG0fTvwfTSDMiZpqMLatVCtGlxyiZunqWdPN0+TMQkiXBvFSmAl8EXswjEmifzyC9x7r/u5bBmULQt9+/odlTF5lmMbhYh87f3cLCKbgm6bRcSG5xqTkz17XHfX00+HhQvhkUdswJxJaOGqngLLndqK7EkueNpwc4T+/NOtUf3rr3DLLfDCC3D88X5HZcwRybFEETQauwZQVFUPAM2Ae4AyMYjNxIBNG15A9u93P487ziWKGTNg3DhLEiYpRNI99kPcMqh1gDeBU4FxUY3KxIxNG36EMjNh5EioUwfS010vptdfh5Yt/Y7MmAITyVxPmaq6X0TaAkNVdZiIWK+nBBcYYGfzNx2BRYvgnntg3jy46KKDpQpjkkwkJYoMEbkBaAd84m0rHr2QTCwEkoTN35QPqtC7txtVvWKFmwb8iy8gJcXvyIyJikhKFB2Be3HTjK8QkRTg3eiGZWLBBtjlkwhs3gydOrkJ/CpW9DsiY6IqkqVQlwA9gIUi8g/gD1UdFPXITNTYAkT58PvvbiT1d9+5x6+9Bq++aknCFAqRrHB3PrAceAMYDfwiIudFOzATHdbLKY/274dnn4V69eDzz+Hnn932IhGtImxMUoik6ulFoLWqLgMQkVOBt4BG0QzMRIf1csqD//3PNVYvWQJXX+1mfK1p18wUPpEkihKBJAGgqj+KSIkoxmSizHo5ReiLL2DrVvjwQ5cojCmkIik/fycir4pIc+/2CjYpYEKytolcqMKbb8Jnn7nHffq4OZosSZhCLpISRRdcY/ZDgACzgf+LZlCmYAXGTASShLVNhPDTT9C1K8yaBTfcAJdfDiVLupsxhVzYRCEipwN1gA9U9dnYhGQKUvZ1r69uWM2qnYLt3g3//jc88wyUKeN6Mt11l99RGRNXwi1c9ChuJbvvgHNE5ElVHR2zyMwRs3WvI/DxxzBwINx+Ozz3nJuryRhziHAlituABqq6U0SOAabiuseaBGBJIow//4S0NLjsMlfNVKsWNG7sd1TGxK1wjdl7VXUngKpuyGVfE0csSeTgwAF4+WU45RRo185VO4lYkjAmF+FKFLWD1soWoE7w2tmq2jaqkZl8s7ESIXz3HXTpAgsWuCVJX37ZFhMyJkLhEsV12R4Pj2Yg5sjZjLA5WLnSlRqqVHFrRNx8sytJGGMiEm7N7C9jGYg5MqF6NxVqqvDDD9CggZvV9T//gSuvhKOP9jsyYxJOJOMoTJyzNolsVq6Ebt1g2jT4/nuXLNq18zsqYxJWVBuoReQyEflZRJaLyMNh9rteRFREbP6oPLIkEWTfPjftd/368PXXrrtrvXp+R2VMwou4RCEiJVV1bx72LwqMAFoC6cACEZkSPG+Ut1853MjveZGe2ziWJIIcOADnngupqdC2LQwdCjVq+B2VMUkhkmnGG4vID8Cv3uMzRCSSKTwaA8tVdYWq7gPGA6EmzXkKeBbYE3nYhdu4eau56dU5liQAtm1zP4sWhY4d3QC6yZMtSRhTgCKpehoGtAE2AqjqIuDCCI6rBvwR9Djd25ZFRM4EaqjqJ4QhIneLyEIRWbhhw4YIXjp5BUoR81ZuoklKpcKbJFRhzBioXRs++shtu/deaNPG17CMSUaRVD0VUdXf5dDuhAciOC5U/0PNelKkCG6tiw65nUhVRwGjABo1aqS57J7UbIwEbkbXrl1h9mw47zyoU8fviIxJapGUKP4QkcaAikhREbkf+CWC49KB4PJ/dWBt0ONywGnALBFZBTQFpliDds4C04QX6jESzz4LZ5zhFhN6/XWXLE47ze+ojElqkSSKrkAvoCbwF+4DvWsExy0A6opIirfQ0c3AlMCTqrpVVauoai1VrQXMBa5S1YV5fA+FRqA0USjHSKhXkDz+eLjtNjcteKdOtiSpMTGQa9WTqq7HfcjniapmiEg3YDpQFBitqktF5ElgoapOCX8GE6zQlibWroWePeH886FHD7jjDnczxsRMrolCRF4jqG0hQFXvzu1YVZ2Km3U2eNvjOezbIrfzFWaFrjQRmMDvscdg/37X9dXz3VwrAAAdI0lEQVQY44tIGrO/CLpfCriWQ3szmSgrdKWJtDS3eFBqKlx6qUsY1mBtjG8iqXqaEPxYRN4CPo9aROYwha40sXWrq3KaMMGtF2ET+Bnjq/zM9ZQCnFjQgZjwkro0oQoTJ8Kvv7qqpn/+E1asgFKl/I7MGENkI7M3i8gm77YFV5p4NPqhGThY7ZS0fvsNWreGm25yA+f273fbLUkYEzfClijEjbI7A1jjbcpU1UI94C2WgudySrpqp7173aR9AwdC8eLw0ktuZHUxm9DYmHgT9r9SVVVEPlDVs2MVkDm4AFGgJJGUo7D/+AOeesqtETF0KFRLskRoTBKJ5OvbfBE5S1W/i3o0hVQgMQQEEkRgAaKkSRIbNrgG6m7d4KST3FQctWv7HZUxJhc5JgoRKaaqGUBzoLOI/AbsxM3hpKp6VoxiTGrZV6YL/EyqBJGZ6VaYe+gh2L4dWraEU06xJGFMgghXopgPnAVcE6NYCp1CsZ7EkiVuAr///teNrh450iUJY0zCCJcoBEBVf4tRLIVKoUgS+/a5AXP79sHo0dChg42JMCYBhUsUx4hIr5yeVNUXohBP0isUDdUzZ7qxECVKwHvvwT/+AVWq+B2VMSafwiWKokBZQq8rYfIhe3tEUrVDAKSnuwn83n/flSDuvBOaN/c7KmPMEQqXKNap6pMxi6QQSNpFhzIyYPhw6NfPTeY3eLCbCtwYkxRybaMwBSOpJ/Zr1w7Gj4fLL4cRIyAlxe+IjDEFKFyiuDhmURQCSTex35YtbhR12bJw331w3XXuZo3VxiSdHOd6UtUknmAodsbNW81Nr85h2bptyVGaUHWlh1NPdVVN4Nohrr/ekoQxScrWkYyyj9LWsGzdNupVLZ/4pYnly6FVK7jlFqheHW6/3e+IjDExYDOwxUC9quWZcE8zv8M4MuPGQceOULKka7ju0gWKFvU7KmNMDFiiiJLAeIlAaSJh7d/vZndt1MhVLz37LJxwgt9RGWNiyBJFAcltYr+Es349PPAA7NzpxkWcfDK8/bbfURljfGCJogAk1cR+mZnw+uvQp49LEn36uLERVs1kTKFliaIAJM1AuhUrXAP1nDnQogW88oqbfsMYU6hZoiggSdH1tUIFNz5i7Fg3iM66uxpjsESRL9nbIxK6wXrKFBgzBiZOhMqV3bTgRazXtDHmIPtEyKNAe0SgsRpIzDESq1fDNdfA1VfDL7/AunVuuyUJY0w2VqKIUNJMD56R4daofuIJN8r6mWfgX/9yXWCNMSYESxQRSKrpwQ8ccL2aLroI/u//oFYtvyMyxsQ5SxRhJE0pYvNmePpp6NsXypWDb7+FSpWssdoYExGrkA4jMLK6SUqlxEwSqvDOO66L6/PPw1dfue2VK1uSMMZEzEoUOQhePyIh52n65Re491748kto3BimT4eGDf2OyhiTgCxRhBDcJpFwvZkC7r8fFi6El1+Gu++2kdXGmHyzRJFNcJJIuOqmzz931Uw1arhR1SVLwvHH+x2VMSbBRbWNQkQuE5GfRWS5iDwc4vleIrJMRBaLyJcicmI044lEQk7H8eefcOutcOmlrrsrwIknWpIwxhSIqCUKESkKjAAuB+oBt4hIvWy7fQ80UtUGwCTg2WjFE4mEW9c6MxNGjnSliMmT3diI557zOypjTJKJZomiMbBcVVeo6j5gPHB18A6q+pWq7vIezgWqRzGesBKyXWLwYOjaFc4+GxYvhv79oVQpv6MyxiSZaLZRVAP+CHqcDjQJs38n4LNQT4jI3cDdADVrRuebfsJUOW3fDn//DSkpbpW5lBS3NKl1dzXGREk0SxShPrk05I4itwONgCGhnlfVUaraSFUbHXPMMQUY4qHiuspJFT74AOrVg5tuco8rV3ZtE5YkjDFRFM1EkQ7UCHpcHVibfScRuQR4DLhKVfdGMZ6Qxs1bzU2vzmHZum2xfunI/f47XHUVtG3rRlQPG2bJwRgTM9GseloA1BWRFGANcDNwa/AOInIm8Cpwmaquj2IshwieJjzulyydMwcuucTdf+456NkTilmvZmNM7ETtE0dVM0SkGzAdKAqMVtWlIvIksFBVp+CqmsoCE8V9Q16tqldFK6bsczc1SakUv5P8bdsG5cvDWWdBx47w4IMQpfYZY4wJR1RDNhvErUaNGunChQvzdEyoBBGXyQFg40Z4+GGYMQOWLoWyZf2OyBiTBEQkVVUb5efYQlGHETy5X9wmCFV46y144AE322uvXtYOYYyJC4UiUYBbhS5uJ/fbutWtNjdrFjRr5gbRNWjgd1TGGAMUokQRl1RdqaF8eahSBUaNgk6dbDlSY0xcSepPpLju+jp9umuoTk93yWLiROjc2ZKEMSbuJO2nUmBKjnkrN1Gvavn46fq6bh3cfDNcdhns2gXrY9Yr2Bhj8iXpqp7ievnSESPg0Udh714YMAD69HFTgRtjTBxLukQR1z2cUlOhSROXMOrW9TsaY4yJSFIlirhbvnTbNnj8cWjXzs3w+vLLrgRh3V6NMQkkqdooAtNy+N4eoQqTJsGpp7p5mb7+2m0vVcqShDEm4SRNooibRYdWroQ2beCGG+DYY91cTb16+RePMcYcoaRJFHFTmnjnHZg9G158ERYscG0SxhiTwJKqjcK30sQ337ieTJdc4ibv69ABqvu2WJ8xxhSopClR+OLvv93MrhdcAE8+6baVLGlJwhiTVJKqRBEzqjBmjCs9bN3qxkP06+d3VElv//79pKens2fPHr9DMSZulSpViurVq1O8ePECO6clivyYOtWVJM47z03gd9ppfkdUKKSnp1OuXDlq1aqFWO8xYw6jqmzcuJH09HRSUlIK7LxW9RSpXbvg22/d/dat4aOPXKO1JYmY2bNnD5UrV7YkYUwORITKlSsXeKnbEkUkPvvMJYTLL4ctW9xYiKuusgn8fGBJwpjwovE/Yp904axZ48ZDtG7tGqk//hiOPtrvqIwxJqYsUeRk/XqoVw8++QQGDoRFi+Cf//Q7KuOzsgWwNO3atWu5/vrrc3x+y5YtvPzyyxHvn12HDh1ISUmhYcOGnHHGGXz55ZdHFG9BGzlyJG+++WaBnGvdunW0adOmQM4VLWPHjqVu3brUrVuXsWPHhtynf//+VKtWjYYNG9KwYUOmTp0KwDvvvJO1rWHDhhQpUoS0tDQALrnkEjZv3hybN6GqCXU7++yzNbt35v6uJ/b5RG8c+b/Dnsuz9PSD9196SXX58iM/pykQy5Yt8zsELVOmTNRfY+XKlVq/fv18H9++fXudOHGiqqrOnDlTTzrppAKJa//+/QVynoLUu3dv/fDDDyPePyMjI4rRHG7jxo2akpKiGzdu1E2bNmlKSopu2rTpsP2eeOIJHTJkSNhzLV68WFNSUrIejxkzRgcOHBhy31D/K8BCzefnblL0eiqQUdlbt0LfvvDqqzB3rltUqEePAorQFLQBHy9l2dqCXZCq3gnleeLK+nk+7vfff6djx45s2LCBY445hv/85z/UrFmT3377jdtuu40DBw5w+eWX88ILL7Bjxw5WrVpFmzZtWLJkCUuXLuXOO+9k3759ZGZmMnnyZPr168dvv/1Gw4YNadmyJffdd1/W/gcOHKBPnz5Mnz4dEaFz58507949x9iaNWvGmjVrsh6npqbSq1cvduzYQZUqVRgzZgxVq1ZlwYIFdOrUiTJlytC8eXM+++wzlixZwpgxY/j000/Zs2cPO3fuZObMmQwZMoT33nuPvXv3cu211zJgwAB27tzJjTfeSHp6OgcOHKBfv37cdNNNPPzww0yZMoVixYpx6aWX8txzz9G/f3/Kli1L7969SUtLo0uXLuzatYs6deowevRoKlasSIsWLWjSpAlfffUVW7Zs4Y033uD8888/7P1NnjyZgQMHArBq1SratWvHzp07ARg+fDjnnnsus2bNYsCAAVStWpW0tDSWLVvG22+/zbBhw9i3bx9NmjTh5ZdfpmjRonTt2pUFCxawe/durr/+egYMGJDnv4dg06dPp2XLllSqVAmAli1bMm3aNG655ZY8n+vdd9895LirrrqK888/n8cee+yIYoxE0lQ95XtUtiq8956bwG/ECOjSBerUKfgATdLq1q0bd9xxB4sXL+a2226jh/cFo2fPnvTs2ZMFCxZwwgknhDx25MiR9OzZk7S0NBYuXEj16tV5+umnqVOnDmlpaQwZMuSQ/UeNGsXKlSv5/vvvs14vnGnTpnHNNdcAbhxK9+7dmTRpEqmpqXTs2DHrQ+bOO+9k5MiRzJkzh6JFix5yjjlz5jB27FhmzpzJjBkz+PXXX5k/fz5paWmkpqYye/Zspk2bxgknnMCiRYtYsmQJl112GZs2beKDDz5g6dKlLF68mL59+x4W3x133MEzzzzD4sWLOf300w/5YM7IyGD+/PkMHTo05Af2ypUrqVixIiW9NV2OPfZYPv/8c7777jsmTJiQ9XsAmD9/PoMGDWLZsmX8+OOPTJgwgW+//Za0tDSKFi3KO++8A8CgQYNYuHAhixcv5uuvv2bx4sWHve6QIUMOqQ4K3HqE+GK5Zs0aatSokfW4evXqhyTuYMOHD6dBgwZ07NgxZJXShAkTDkkUFStWZO/evWzcuDHk+QpSQpcoAosULVu3jXpVy+f9BKrQti18+KErQUyZAo0aFXygpsDl55t/tMyZM4f3338fgHbt2vHQQw9lbf/www8BuPXWW+ndu/dhxzZr1oxBgwaRnp5O27ZtqZvLOiVffPEFXbp0oVgx968b+Kaa3YMPPshDDz3E+vXrmTt3LgA///wzS5YsoWXLlgAcOHCAqlWrsmXLFrZv3865556bFesnn3ySda7gb8QzZsxgxowZnHnmmQDs2LGDX3/9lfPPP5/evXvTp08f2rRpw/nnn09GRgalSpXirrvu4oorrjisLWHr1q1s2bKFf3ptf+3bt+eGG27Ier5t27YAnH322axateqw97hu3TqOOeaYrMf79++nW7duWR/+v/zyS9ZzjRs3zhpX8OWXX5Kamso555wDwO7duzn22GMBeO+99xg1ahQZGRmsW7eOZcuW0aBBg8Ou7YMPPhjyumfnanwOFapXUteuXenXrx8iQr9+/XjggQcYPXp01vPz5s3jqKOO4rRs3fGPPfZY1q5dS+XKlSOKJ78SOlEEJ4k8VTvt3w/Fi7turs2bw0UXwb33QrZvUsbkR166J9566600adKETz/9lFatWvH6669Tu3btHPdX1YjOP2TIENq2bcuwYcNo3749qampqCr169dnzpw5h+ybW4NomTJlDnn9Rx55hHvuueew/VJTU5k6dSqPPPIIl156KY8//jjz58/nyy+/ZPz48QwfPpyZM2fmGntAoKRQtGhRMjIyDnu+dOnSh4wXePHFFznuuONYtGgRmZmZlCpVKsf30L59ewYPHnzI+VauXMlzzz3HggULqFixIh06dAg5HmHIkCFZJZBgF1xwAcOGDTtkW/Xq1Zk1a1bW4/T0dFq0aHHYsccdd1zW/c6dOx+WVMePHx+yumrPnj2ULl36sO0FLeGrnupVLc+Ee5pFXu00axY0aOAGzAE88AB0725JwuTbueeey/jx4wHXS6V58+YANG3alMmTJwNkPZ/dihUrqF27Nj169OCqq65i8eLFlCtXju3bt4fc/9JLL2XkyJFZH5ybNm3KMa4iRYrQs2dPMjMzmT59OqeccgobNmzIShT79+9n6dKlVKxYkXLlymWVPHKKFaBVq1aMHj2aHTt2AK5qZf369axdu5ajjjqK22+/nd69e/Pdd9+xY8cOtm7dSuvWrRk6dGhWb52AChUqULFiRb755hsA3nrrrazSRSROPvnkQ0oaW7dupWrVqhQpUoS33nqLAwcOhDzu4osvZtKkSaz31qvftGkTv//+O9u2baNMmTJUqFCBv/76i88++yzk8Q8++CBpaWmH3bInicD1mjFjBps3b2bz5s3MmDGDVq1aHbbfunXrsu5/8MEHh5QcMjMzmThxIjfffPMhx6gqf/75J7Vq1crxGhWUhC5R5MmGDdC7N7z5JqSkQLlyfkdkEtCuXbuoHjTpY69evRg2bBgdO3ZkyJAhWY3ZAEOHDuX222/n+eef54orrqBChQqHnW/ChAm8/fbbFC9enOOPP57HH3+cSpUqcd5553Haaadx+eWXc99992Xtf9ddd/HLL7/QoEEDihcvTufOnenWrVuO8YoIffv25dlnn6VVq1ZMmjSJHj16sHXrVjIyMrj//vupX78+b7zxBp07d6ZMmTK0aNEiZKzgEtWPP/5Is2ZuBcmyZcvy9ttvs3z5ch588EGKFClC8eLFeeWVV9i+fTtXX301e/bsQVV58cUXDzvf2LFjsxqza9eunXXtIlGmTBnq1KnD8uXLOemkk7j33nu57rrrmDhxIhdeeOEhpYhg9erVY+DAgVx66aVkZmZSvHhxRowYQdOmTTnzzDOpX78+tWvX5rzzzos4lpxUqlSJfv36ZVVzBX6/4H6XXbp0oVGjRjz00EOkpaUhItSqVYtXX3016xyzZ8+mevXqh5U0U1NTadq0aVY1ZFTlt7uUX7dA99g8dYkdN061YkXV4sVVH31UdefO3I8xcSceusfmxc6dOzUzM1NVVd9991296qqrfI4oZ9u3b8+6P3jwYO3Ro4eP0UTu/fff18cee8zvMHzRo0cP/eKLL0I+Z91jPXnqEpuR4abgGDnSDaIzJgZSU1Pp1q0bqsrRRx99SONkvPn0008ZPHgwGRkZnHjiiYwZM8bvkCJy7bXXxqTXTzw67bTTuPjii2PyWqIhWuXjWaNGjbTXiPd59IMfaJJSiQn3NDt8p5074amnoGZN10gdeI82T1BC+/HHHzn11FP9DsOYuBfqf0VEUlU1X906E64xe9POfTz6wQ9ADqWJTz6B+vXhmWcg0D1OxJJEkki0LzbGxFo0/kcSLlFs2bUfgH9fe/qhPZ3S092YiCuvhDJl3BTgQ4f6FKWJhlKlSrFx40ZLFsbkQNWtRxHcNbggJGQbRchR2CtWwPTpMHgw9OoFJUr4E5yJmurVq5Oens6GDRv8DsWYuBVY4a4gJVyi2LkvaODN/PkwZw707OnWrV69GqI8QtH4p3jx4gW6apcxJjJRrXoSkctE5GcRWS4iD4d4vqSITPCenycitSI57/V1yrpG6qZN4YUXXOM1WJIwxpgoiFqiEJGiwAjgcqAecIuIZO+b2gnYrKonAS8Cz+R23uMydnLD7S3dLK89esAPP7g2CWOMMVERzRJFY2C5qq5Q1X3AeODqbPtcDQRW8pgEXCy5TGRzzMY/oUYNWLDANVaXz8dkgMYYYyIWzTaKasAfQY/TgSY57aOqGSKyFagM/B28k4jcDdztPdwrCxcu4eyzoxJ0gqlCtmtViNm1OMiuxUF2LQ46Jb8HRjNRhCoZZO/XGMk+qOooYBSAiCzM76CRZGPX4iC7FgfZtTjIrsVBIrIwv8dGs+opHagR9Lg6sDanfUSkGFAByHk6TGOMMTEXzUSxAKgrIikiUgK4GZiSbZ8pQHvv/vXATLXRVMYYE1eiVvXktTl0A6YDRYHRqrpURJ7EzWI4BXgDeEtEluNKEjfnfMYso6IVcwKya3GQXYuD7FocZNfioHxfi4SbFNAYY0xsJdxcT8YYY2LLEoUxxpiw4jZRRGv6j0QUwbXoJSLLRGSxiHwpIif6EWcs5HYtgva7XkRURJK2a2Qk10JEbvT+NpaKyLhYxxgrEfyP1BSRr0Tke+//pLUfcUabiIwWkfUisiSH50VEhnnXabGInBXRifO7NF40b7jG79+A2kAJYBFQL9s+9wIjvfs3AxP8jtvHa3EhcJR3v2thvhbefuWA2cBcoJHfcfv4d1EX+B6o6D0+1u+4fbwWo4Cu3v16wCq/447StbgAOAtYksPzrYHPcGPYmgLzIjlvvJYoojL9R4LK9Vqo6lequst7OBc3ZiUZRfJ3AfAU8CywJ5bBxVgk16IzMEJVNwOo6voYxxgrkVwLBQLz/VTg8DFdSUFVZxN+LNrVwJvqzAWOFpGquZ03XhNFqOk/si9nd8j0H0Bg+o9kE8m1CNYJ940hGeV6LUTkTKCGqn4Sy8B8EMnfxcnAySLyrYjMFZHLYhZdbEVyLfoDt4tIOjAV6B6b0OJOXj9PgPhdj6LApv9IAhG/TxG5HWgE/DOqEfkn7LUQkSK4WYg7xCogH0Xyd1EMV/3UAlfK/EZETlPVLVGOLdYiuRa3AGNU9XkRaYYbv3WaqmZGP7y4kq/PzXgtUdj0HwdFci0QkUuAx4CrVHVvjGKLtdyuRTngNGCWiKzC1cFOSdIG7Uj/Rz5S1f2quhL4GZc4kk0k16IT8B6Aqs4BSuEmDCxsIvo8yS5eE4VN/3FQrtfCq255FZckkrUeGnK5Fqq6VVWrqGotVa2Fa6+5SlXzPRlaHIvkf+RDXEcHRKQKripqRUyjjI1IrsVq4GIAETkVlygK45q6U4A7vN5PTYGtqrout4PisupJozf9R8KJ8FoMAcoCE732/NWqepVvQUdJhNeiUIjwWkwHLhWRZcAB4EFV3ehf1NER4bV4AHhNRP6Fq2rpkIxfLEXkXVxVYxWvPeYJoDiAqo7Etc+0BpYDu4A7IzpvEl4rY4wxBSheq56MMcbECUsUxhhjwrJEYYwxJixLFMYYY8KyRGGMMSYsSxQm7ojIARFJC7rVCrNvrZxmyszja87yZh9d5E15cUo+ztFFRO7w7ncQkROCnntdROoVcJwLRKRhBMfcLyJHHelrm8LLEoWJR7tVtWHQbVWMXvc2VT0DN9nkkLwerKojVfVN72EH4ISg5+5S1WUFEuXBOF8msjjvByxRmHyzRGESgldy+EZEvvNu54bYp76IzPdKIYtFpK63/fag7a+KSNFcXm42cJJ37MXeGgY/eHP9l/S2Py0H1wB5ztvWX0R6i8j1uDm33vFes7RXEmgkIl1F5NmgmDuIyP/lM845BE3oJiKviMhCcWtPDPC29cAlrK9E5Ctv26UiMse7jhNFpGwur2MKOUsUJh6VDqp2+sDbth5oqapnATcBw0Ic1wV4SVUb4j6o073pGm4CzvO2HwBuy+X1rwR+EJFSwBjgJlU9HTeTQVcRqQRcC9RX1QbAwOCDVXUSsBD3zb+hqu4OenoS0Dbo8U3AhHzGeRlumo6Ax1S1EdAA+KeINFDVYbi5fC5U1Qu9qTz6Apd413Ih0CuX1zGFXFxO4WEKvd3eh2Ww4sBwr07+AG7eouzmAI+JSHXgfVX9VUQuBs4GFnjTm5TGJZ1Q3hGR3cAq3DTUpwArVfUX7/mxwH3AcNxaF6+LyKdAxFOaq+oGEVnhzbPzq/ca33rnzUucZXDTVQSvUHajiNyN+7+uilugZ3G2Y5t627/1XqcE7roZkyNLFCZR/Av4CzgDVxI+bFEiVR0nIvOAK4DpInIXblrlsar6SASvcVvwBIIiEnJ9E29uoca4SeZuBroBF+XhvUwAbgR+Aj5QVRX3qR1xnLhV3J4GRgBtRSQF6A2co6qbRWQMbuK77AT4XFVvyUO8ppCzqieTKCoA67z1A9rhvk0fQkRqAyu86pYpuCqYL4HrReRYb59KEvma4j8BtUTkJO9xO+Brr06/gqpOxTUUh+p5tB037Xko7wPX4NZImOBty1OcqrofV4XU1Ku2Kg/sBLaKyHHA5TnEMhc4L/CeROQoEQlVOjMmiyUKkyheBtqLyFxctdPOEPvcBCwRkTTgH7glH5fhPlBniMhi4HNctUyuVHUPbnbNiSLyA5AJjMR96H7ine9rXGknuzHAyEBjdrbzbgaWASeq6nxvW57j9No+ngd6q+oi3PrYS4HRuOqsgFHAZyLylapuwPXIetd7nbm4a2VMjmz2WGOMMWFZicIYY0xYliiMMcaEZYnCGGNMWJYojDHGhGWJwhhjTFiWKIwxxoRlicIYY0xY/w9hpTOxue9i7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d51756bf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning model（CNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use CNN to predict whether a given track can reach top 30 by using their artist's popularity, followers, and audion feature, audio analysis information which we get on spotify api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3Vo4wInECJQuz9BIBMOu8i</td>\n",
       "      <td>0du5cEVh5yTK9QJze8zA0C</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>11594549</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.926</td>\n",
       "      <td>105.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>1rfofaqEpACxVEHIZBJe6W</td>\n",
       "      <td>4nDoRrQiYLoBzwC5BhVJzF</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>3144195</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.394</td>\n",
       "      <td>104.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>0OAAAdiHJKa2wlCKqaYXV7</td>\n",
       "      <td>246dkjvS1zLTtiykXe5h60</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>3309267</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.127</td>\n",
       "      <td>159.764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       title                               artist  \\\n",
       "0   1  God's Plan                                Drake   \n",
       "1   2     Perfect                           Ed Sheeran   \n",
       "2   3     Finesse                 Bruno Mars & Cardi B   \n",
       "3   4      Havana  Camila Cabello Featuring Young Thug   \n",
       "4   5    Rockstar      Post Malone Featuring 21 Savage   \n",
       "\n",
       "                 track_id               artist_id  rank  hit  \\\n",
       "0  2XW4DbS6NddZxRPm5rMCeY  3TVXtAsR1Inumwj472S9r4     1  Yes   \n",
       "1  0tgVpDi06FyKpA1z0VMD4v  6eUKZXaKkcviH0Ku9w2n3V     1  Yes   \n",
       "2  3Vo4wInECJQuz9BIBMOu8i  0du5cEVh5yTK9QJze8zA0C     3  Yes   \n",
       "3  1rfofaqEpACxVEHIZBJe6W  4nDoRrQiYLoBzwC5BhVJzF     1  Yes   \n",
       "4  0OAAAdiHJKa2wlCKqaYXV7  246dkjvS1zLTtiykXe5h60     1  Yes   \n",
       "\n",
       "   artist_popularity  followers  popularity   ...     energy  key  loudness  \\\n",
       "0                100   18311322          99   ...      0.454    7    -9.488   \n",
       "1                 96   21431294          95   ...      0.448    8    -6.312   \n",
       "2                 91   11594549          95   ...      0.859    5    -4.877   \n",
       "3                 91    3144195          97   ...      0.523    2    -4.333   \n",
       "4                 96    3309267          90   ...      0.535    5    -6.637   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0     1       0.0963        0.0244          0.000056    0.4980    0.344   \n",
       "1     1       0.0232        0.1630          0.000000    0.1060    0.168   \n",
       "2     0       0.0996        0.0185          0.000000    0.0215    0.926   \n",
       "3     1       0.0300        0.1840          0.000036    0.1320    0.394   \n",
       "4     0       0.0776        0.1300          0.000130    0.1430    0.127   \n",
       "\n",
       "     tempo  \n",
       "0   77.170  \n",
       "1   95.050  \n",
       "2  105.115  \n",
       "3  104.988  \n",
       "4  159.764  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6237 entries, 0 to 6236\n",
      "Data columns (total 21 columns):\n",
      "id                   6237 non-null int64\n",
      "title                6237 non-null object\n",
      "artist               6237 non-null object\n",
      "track_id             6237 non-null object\n",
      "artist_id            6237 non-null object\n",
      "rank                 6237 non-null int64\n",
      "hit                  6237 non-null object\n",
      "artist_popularity    6237 non-null int64\n",
      "followers            6237 non-null int64\n",
      "popularity           6237 non-null int64\n",
      "danceability         6237 non-null float64\n",
      "energy               6237 non-null float64\n",
      "key                  6237 non-null int64\n",
      "loudness             6237 non-null float64\n",
      "mode                 6237 non-null int64\n",
      "speechiness          6237 non-null float64\n",
      "acousticness         6237 non-null float64\n",
      "instrumentalness     6237 non-null float64\n",
      "liveness             6237 non-null float64\n",
      "valence              6237 non-null float64\n",
      "tempo                6237 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 1023.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rank</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6.237000e+03</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3515.977233</td>\n",
       "      <td>46.833093</td>\n",
       "      <td>72.287959</td>\n",
       "      <td>2.818100e+06</td>\n",
       "      <td>57.089145</td>\n",
       "      <td>0.625627</td>\n",
       "      <td>0.698398</td>\n",
       "      <td>5.255892</td>\n",
       "      <td>-5.833668</td>\n",
       "      <td>0.678211</td>\n",
       "      <td>0.099974</td>\n",
       "      <td>0.161373</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.185531</td>\n",
       "      <td>0.518813</td>\n",
       "      <td>121.944461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2095.961730</td>\n",
       "      <td>29.967952</td>\n",
       "      <td>14.870323</td>\n",
       "      <td>4.272673e+06</td>\n",
       "      <td>14.185772</td>\n",
       "      <td>0.144532</td>\n",
       "      <td>0.170741</td>\n",
       "      <td>3.563445</td>\n",
       "      <td>2.146704</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.101689</td>\n",
       "      <td>0.203845</td>\n",
       "      <td>0.066063</td>\n",
       "      <td>0.141792</td>\n",
       "      <td>0.223985</td>\n",
       "      <td>28.966124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22.015000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>51.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1683.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>3.556930e+05</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-6.965000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>98.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3454.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.083347e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.531000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>120.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5329.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.019306e+06</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.341000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>140.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7234.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.143193e+07</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-0.463000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         rank  artist_popularity     followers   popularity  \\\n",
       "count  6237.000000  6237.000000        6237.000000  6.237000e+03  6237.000000   \n",
       "mean   3515.977233    46.833093          72.287959  2.818100e+06    57.089145   \n",
       "std    2095.961730    29.967952          14.870323  4.272673e+06    14.185772   \n",
       "min       1.000000     1.000000           0.000000  2.000000e+00     0.000000   \n",
       "25%    1683.000000    20.000000          64.000000  3.556930e+05    48.000000   \n",
       "50%    3454.000000    46.000000          75.000000  1.083347e+06    58.000000   \n",
       "75%    5329.000000    73.000000          82.000000  3.019306e+06    67.000000   \n",
       "max    7234.000000   100.000000         100.000000  2.143193e+07    99.000000   \n",
       "\n",
       "       danceability       energy          key     loudness         mode  \\\n",
       "count   6237.000000  6237.000000  6237.000000  6237.000000  6237.000000   \n",
       "mean       0.625627     0.698398     5.255892    -5.833668     0.678211   \n",
       "std        0.144532     0.170741     3.563445     2.146704     0.467200   \n",
       "min        0.113000     0.056500     0.000000   -22.015000     0.000000   \n",
       "25%        0.529000     0.587000     2.000000    -6.965000     0.000000   \n",
       "50%        0.628000     0.723000     5.000000    -5.531000     1.000000   \n",
       "75%        0.725000     0.832000     8.000000    -4.341000     1.000000   \n",
       "max        0.994000     0.996000    11.000000    -0.463000     1.000000   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness     liveness      valence  \\\n",
       "count  6237.000000   6237.000000       6237.000000  6237.000000  6237.000000   \n",
       "mean      0.099974      0.161373          0.008434     0.185531     0.518813   \n",
       "std       0.101689      0.203845          0.066063     0.141792     0.223985   \n",
       "min       0.022400      0.000003          0.000000     0.016400     0.034900   \n",
       "25%       0.035100      0.017300          0.000000     0.094400     0.338000   \n",
       "50%       0.052900      0.072600          0.000000     0.129000     0.520000   \n",
       "75%       0.120000      0.231000          0.000017     0.243000     0.694000   \n",
       "max       0.765000      0.986000          0.982000     0.987000     0.976000   \n",
       "\n",
       "             tempo  \n",
       "count  6237.000000  \n",
       "mean    121.944461  \n",
       "std      28.966124  \n",
       "min      51.316000  \n",
       "25%      98.011000  \n",
       "50%     120.637000  \n",
       "75%     140.171000  \n",
       "max     213.737000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train ,x_test = train_test_split(data,test_size=0.25)  #split data into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>1119</td>\n",
       "      <td>Worth It</td>\n",
       "      <td>Fifth Harmony Featuring Kid Ink</td>\n",
       "      <td>41Fflg7qHiVOD6dEPvsCzO</td>\n",
       "      <td>1l8Fu6IkuTP0U5QetQJ5Xt</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>80</td>\n",
       "      <td>4786833</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.865</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.594</td>\n",
       "      <td>99.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>1662</td>\n",
       "      <td>Headlights</td>\n",
       "      <td>Eminem Featuring Nate Ruess</td>\n",
       "      <td>222dTwr5XeEgAzEtsrQA0R</td>\n",
       "      <td>7dGJo4pcD2V6oG8kP0tJRR</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>93</td>\n",
       "      <td>15210343</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793</td>\n",
       "      <td>11</td>\n",
       "      <td>-3.659</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.464</td>\n",
       "      <td>146.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>2406</td>\n",
       "      <td>I Believe I Can Fly</td>\n",
       "      <td>Jermaine Paul</td>\n",
       "      <td>6YRrsapstM9gOkObdJQMOG</td>\n",
       "      <td>302jOsgU7aZW71XhLem57k</td>\n",
       "      <td>83</td>\n",
       "      <td>No</td>\n",
       "      <td>43</td>\n",
       "      <td>1187</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.867</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.194</td>\n",
       "      <td>120.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3864</td>\n",
       "      <td>Womanizer</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>4fixebDZAVToLbUCuEloa2</td>\n",
       "      <td>26dSoYclwsYLMAKD3tpOr4</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>80</td>\n",
       "      <td>3940044</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.226</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.235</td>\n",
       "      <td>139.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1121</td>\n",
       "      <td>This Could Be Us</td>\n",
       "      <td>Rae Sremmurd</td>\n",
       "      <td>4jTiyLlOJVJj3mCr7yfPQD</td>\n",
       "      <td>7iZtZyCzp3LItcw1wtPI3D</td>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>84</td>\n",
       "      <td>2967527</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.060</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.721</td>\n",
       "      <td>142.992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                title                           artist  \\\n",
       "1039  1119             Worth It  Fifth Harmony Featuring Kid Ink   \n",
       "1541  1662           Headlights      Eminem Featuring Nate Ruess   \n",
       "2210  2406  I Believe I Can Fly                    Jermaine Paul   \n",
       "3463  3864            Womanizer                   Britney Spears   \n",
       "1041  1121     This Could Be Us                     Rae Sremmurd   \n",
       "\n",
       "                    track_id               artist_id  rank  hit  \\\n",
       "1039  41Fflg7qHiVOD6dEPvsCzO  1l8Fu6IkuTP0U5QetQJ5Xt    12  Yes   \n",
       "1541  222dTwr5XeEgAzEtsrQA0R  7dGJo4pcD2V6oG8kP0tJRR    45   No   \n",
       "2210  6YRrsapstM9gOkObdJQMOG  302jOsgU7aZW71XhLem57k    83   No   \n",
       "3463  4fixebDZAVToLbUCuEloa2  26dSoYclwsYLMAKD3tpOr4     1  Yes   \n",
       "1041  4jTiyLlOJVJj3mCr7yfPQD  7iZtZyCzp3LItcw1wtPI3D    49   No   \n",
       "\n",
       "      artist_popularity  followers  popularity   ...     energy  key  \\\n",
       "1039                 80    4786833          78   ...      0.765    8   \n",
       "1541                 93   15210343          61   ...      0.793   11   \n",
       "2210                 43       1187          17   ...      0.686    4   \n",
       "3463                 80    3940044          75   ...      0.695   11   \n",
       "1041                 84    2967527          72   ...      0.692    5   \n",
       "\n",
       "      loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "1039    -3.865     1       0.0882         0.063          0.000007    0.1180   \n",
       "1541    -3.659     1       0.1410         0.160          0.000000    0.8330   \n",
       "2210    -5.867     0       0.0529         0.208          0.000000    0.1190   \n",
       "3463    -5.226     1       0.0622         0.073          0.000000    0.0889   \n",
       "1041    -5.060     0       0.1480         0.275          0.000000    0.1180   \n",
       "\n",
       "      valence    tempo  \n",
       "1039    0.594   99.987  \n",
       "1541    0.464  146.688  \n",
       "2210    0.194  120.637  \n",
       "3463    0.235  139.000  \n",
       "1041    0.721  142.992  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>5059</td>\n",
       "      <td>Pretty Vegas</td>\n",
       "      <td>INXS</td>\n",
       "      <td>37oEm1l0NL6Ud2ccgsdgND</td>\n",
       "      <td>1eClJfHLoDI4rZe5HxzBFv</td>\n",
       "      <td>37</td>\n",
       "      <td>No</td>\n",
       "      <td>68</td>\n",
       "      <td>510759</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.536</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.676</td>\n",
       "      <td>141.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>6427</td>\n",
       "      <td>Never Had A Dream Come True</td>\n",
       "      <td>S Club 7</td>\n",
       "      <td>1rzssdgfZiGs7VAMsYN9SF</td>\n",
       "      <td>1kM5rgJvkiDMOoKX56H6pX</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>60</td>\n",
       "      <td>153310</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.481</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.48700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.456</td>\n",
       "      <td>122.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>4659</td>\n",
       "      <td>Maneater</td>\n",
       "      <td>Nelly Furtado</td>\n",
       "      <td>1M9uq2yCiOhsO6BFozo2OE</td>\n",
       "      <td>2jw70GZXlAI8QzWeY2bgRc</td>\n",
       "      <td>16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>73</td>\n",
       "      <td>930598</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777</td>\n",
       "      <td>6</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.03350</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.729</td>\n",
       "      <td>66.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>2792</td>\n",
       "      <td>Losing My Religion</td>\n",
       "      <td>Dia Frampton</td>\n",
       "      <td>0Q5U5alAt2pjop9331E5cQ</td>\n",
       "      <td>3ppkM4QtM781APpaX7H9t7</td>\n",
       "      <td>54</td>\n",
       "      <td>No</td>\n",
       "      <td>54</td>\n",
       "      <td>32920</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.16100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.484</td>\n",
       "      <td>129.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>541</td>\n",
       "      <td>Make Love</td>\n",
       "      <td>Gucci Mane &amp; Nicki Minaj</td>\n",
       "      <td>2KlDzyTJVmOlH1sVTuPcSF</td>\n",
       "      <td>13y7CgLHjMVRMDqxdx0Xdo</td>\n",
       "      <td>78</td>\n",
       "      <td>No</td>\n",
       "      <td>89</td>\n",
       "      <td>1820366</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.820</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.00529</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.183</td>\n",
       "      <td>129.994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        title                    artist  \\\n",
       "4453  5059                 Pretty Vegas                      INXS   \n",
       "5578  6427  Never Had A Dream Come True                  S Club 7   \n",
       "4120  4659                     Maneater             Nelly Furtado   \n",
       "2545  2792           Losing My Religion              Dia Frampton   \n",
       "505    541                    Make Love  Gucci Mane & Nicki Minaj   \n",
       "\n",
       "                    track_id               artist_id  rank  hit  \\\n",
       "4453  37oEm1l0NL6Ud2ccgsdgND  1eClJfHLoDI4rZe5HxzBFv    37   No   \n",
       "5578  1rzssdgfZiGs7VAMsYN9SF  1kM5rgJvkiDMOoKX56H6pX    10  Yes   \n",
       "4120  1M9uq2yCiOhsO6BFozo2OE  2jw70GZXlAI8QzWeY2bgRc    16  Yes   \n",
       "2545  0Q5U5alAt2pjop9331E5cQ  3ppkM4QtM781APpaX7H9t7    54   No   \n",
       "505   2KlDzyTJVmOlH1sVTuPcSF  13y7CgLHjMVRMDqxdx0Xdo    78   No   \n",
       "\n",
       "      artist_popularity  followers  popularity   ...     energy  key  \\\n",
       "4453                 68     510759          44   ...      0.922    5   \n",
       "5578                 60     153310          52   ...      0.547    5   \n",
       "4120                 73     930598          58   ...      0.777    6   \n",
       "2545                 54      32920          32   ...      0.694    3   \n",
       "505                  89    1820366          57   ...      0.548   11   \n",
       "\n",
       "      loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "4453    -4.536     0       0.0604       0.00121          0.000025    0.0785   \n",
       "5578    -5.481     1       0.0283       0.48700          0.000000    0.0923   \n",
       "4120    -4.739     1       0.0636       0.03350          0.000179    0.1070   \n",
       "2545    -5.421     0       0.0311       0.16100          0.000000    0.1590   \n",
       "505     -6.820     1       0.1540       0.00529          0.001550    0.1100   \n",
       "\n",
       "      valence    tempo  \n",
       "4453    0.676  141.968  \n",
       "5578    0.456  122.951  \n",
       "4120    0.729   66.404  \n",
       "2545    0.484  129.844  \n",
       "505     0.183  129.994  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('Train_data.csv', index = False)\n",
    "x_test.to_csv(\"Test_data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise weights and bias for the filter\n",
    "def weight_variable(shape, name = \"ConV\"):    \n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.1) )  \n",
    "def bias_variable(shape,name = \"Fc\"):    \n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the filter input shape for tf.nn.conv_2d\n",
    "def conv2d(x, W):    \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs the max pooling on the input \n",
    "def max_pool_2x2(x):    \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    path = file_name + \".csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.hit.replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "    \n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 14\n",
    "    n_channels = 14\n",
    "    \n",
    "    \"labels\"\n",
    "    labels = df[\"hit\"]\n",
    "    \n",
    "    # Initiate array\n",
    "    X = np.zeros((len(labels), n_steps, n_channels))\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        for j in range(0, n_steps):\n",
    "            a = df.loc[i][7:21].tolist()\n",
    "            tmp = []\n",
    "            tmp = tmp + a\n",
    "            tmp = np.asarray(tmp)\n",
    "            X[i,j,:] = tmp\n",
    "        if i%100 == 0:\n",
    "            print(i)  \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "    # Standardize train and test\n",
    "    X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "    X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 2):\n",
    "    expansion = np.eye(n_class) #Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
    "    y = expansion[:, labels-1].T\n",
    "    assert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(train_data, train_target, batch_size):  \n",
    "    idx = [ i for i in range(0,len(train_target)) ]  \n",
    "    np.random.shuffle(idx);  \n",
    "    batch_data = []; batch_target = [];  \n",
    "    for i in range(0,batch_size):  \n",
    "        batch_data.append(train_data[idx[i]]);  \n",
    "        batch_target.append(train_target[idx[i]])  \n",
    "    return batch_data, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "x_train, labels_train = read_data(\"Train_data\") # train\n",
    "x_test, labels_test = read_data(\"Test_data\") # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: N = 4677, steps = 14, channels = 14\n",
      "Test data shape: N = 1560, steps = 14, channels = 14\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data shape: N = {:d}, steps = {:d}, channels = {:d}\".format(x_train.shape[0],\n",
    "                                                                             x_train.shape[1],\n",
    "                                                                             x_train.shape[2]))\n",
    "print (\"Test data shape: N = {:d}, steps = {:d}, channels = {:d}\".format(x_test.shape[0],\n",
    "                                                                   \n",
    "                                                                         x_test.shape[1],\n",
    "                                                                         x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_test = standardize(x_train, x_test)  # standardize the training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot label\n",
    "y_tr = one_hot(labels_train)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train  \n",
    "train_target = y_tr  \n",
    "test_data = Y_test     \n",
    "test_target = y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 14  # seq_len = steps\n",
    "n_channels = 14 # feature\n",
    "n_classes = 2 # Yes or No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the training data placeholders\n",
    "x = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep') # keep_probability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_num = 5000   \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Convolutional Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([3, 3, 1, 20],name = \"W\")    # define the filter size(2*2), feature map to 20\n",
    "b_conv1 = bias_variable([20], name = \"b\")  # define the bias size, which is the number of convolution kernel\n",
    "x_image = tf.reshape(x, [-1, 14, 14, 1])  # reshape  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # relu activation function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool1 = max_pool_2x2(h_conv1) # output is [7,7,20,40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([3, 3, 20, 40])    \n",
    "b_conv2 = bias_variable([40])    \n",
    "h_conv2 = tf.nn.relu( conv2d(h_pool1, W_conv2) + b_conv2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2 = max_pool_2x2(h_conv2) # 输出[4，4,40,100]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([4 * 4 * 40, 80], name =\"W\")    \n",
    "b_fc1 = bias_variable([80], name = \"b\")    \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*40])    \n",
    "h_fc1 = tf.nn.relu( tf.matmul(h_pool2_flat, W_fc1) + b_fc1 )     \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  #some hidden node weight is not kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([80, 2], name = \"W\")    \n",
    "b_fc2 = bias_variable([2], name = \"b\")    \n",
    "y_conv=tf.nn.softmax( tf.matmul(h_fc1_drop, W_fc2) + b_fc2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, training accuracy 0.640\n",
      "step 100, training accuracy 0.760\n",
      "step 200, training accuracy 0.720\n",
      "step 300, training accuracy 0.680\n",
      "step 400, training accuracy 0.540\n",
      "step 500, training accuracy 0.720\n",
      "step 600, training accuracy 0.700\n",
      "step 700, training accuracy 0.780\n",
      "step 800, training accuracy 0.880\n",
      "step 900, training accuracy 0.700\n",
      "step 1000, training accuracy 0.640\n",
      "step 1100, training accuracy 0.820\n",
      "step 1200, training accuracy 0.780\n",
      "step 1300, training accuracy 0.700\n",
      "step 1400, training accuracy 0.720\n",
      "step 1500, training accuracy 0.780\n",
      "step 1600, training accuracy 0.760\n",
      "step 1700, training accuracy 0.760\n",
      "step 1800, training accuracy 0.780\n",
      "step 1900, training accuracy 0.780\n",
      "step 2000, training accuracy 0.660\n",
      "step 2100, training accuracy 0.760\n",
      "step 2200, training accuracy 0.640\n",
      "step 2300, training accuracy 0.700\n",
      "step 2400, training accuracy 0.620\n",
      "step 2500, training accuracy 0.700\n",
      "step 2600, training accuracy 0.720\n",
      "step 2700, training accuracy 0.640\n",
      "step 2800, training accuracy 0.700\n",
      "step 2900, training accuracy 0.800\n",
      "step 3000, training accuracy 0.600\n",
      "step 3100, training accuracy 0.760\n",
      "step 3200, training accuracy 0.820\n",
      "step 3300, training accuracy 0.740\n",
      "step 3400, training accuracy 0.780\n",
      "step 3500, training accuracy 0.700\n",
      "step 3600, training accuracy 0.740\n",
      "step 3700, training accuracy 0.700\n",
      "step 3800, training accuracy 0.620\n",
      "step 3900, training accuracy 0.700\n",
      "step 4000, training accuracy 0.800\n",
      "step 4100, training accuracy 0.620\n",
      "step 4200, training accuracy 0.760\n",
      "step 4300, training accuracy 0.860\n",
      "step 4400, training accuracy 0.740\n",
      "step 4500, training accuracy 0.780\n",
      "step 4600, training accuracy 0.660\n",
      "step 4700, training accuracy 0.760\n",
      "step 4800, training accuracy 0.720\n",
      "step 4900, training accuracy 0.840\n",
      "Training finished\n",
      "test accuracy 0.677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession() \n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv)) #cost function\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # Gradient estimation  \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))  \n",
    "\n",
    "probabilities=y_conv\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))   \n",
    "\n",
    "sess.run(tf.initialize_all_variables()) \n",
    "\n",
    "train_acc = []\n",
    "for i in range(epochs_num): \n",
    "\n",
    "    batch_data, batch_target = next_batch(train_data,train_target,batch_size) \n",
    "    \n",
    "    if i%100 == 0:    \n",
    "        train_accuracy = accuracy.eval(feed_dict={ x:batch_data, y: batch_target, keep_prob: 1.0} )    \n",
    "        print (\"step %d, training accuracy %.3f\"%(i, train_accuracy))   \n",
    "    if i%10 ==0:    \n",
    "        train_accuracy = accuracy.eval(feed_dict={ x:batch_data, y: batch_target, keep_prob: 1.0} )\n",
    "        \n",
    "    train_step.run(feed_dict={x: batch_data, y:batch_target, keep_prob: 0.5})   # keep_prob is 0.5  \n",
    "  \n",
    "print(\"Training finished\")    \n",
    "print(\"test accuracy %.3f\" % accuracy.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0}) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = correct_prediction.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False ...  True False  True]\n"
     ]
    }
   ],
   "source": [
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pd.DataFrame(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1560 entries, 0 to 1559\n",
      "Data columns (total 1 columns):\n",
      "0    1560 non-null bool\n",
      "dtypes: bool(1)\n",
      "memory usage: 1.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.DataFrame(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target .to_csv(\"test_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform one hot to True or False\n",
    "target = []\n",
    "for i in range(0,len(test_target)):\n",
    "    tmp = []\n",
    "    if test_target.loc[i][0] == 1:\n",
    "        tmp.append(False)\n",
    "    else:\n",
    "        tmp.append(True)\n",
    "    target.append(tmp)\n",
    "test_1 = pd.DataFrame(target)\n",
    "test_1.columns= ['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1560 entries, 0 to 1559\n",
      "Data columns (total 1 columns):\n",
      "hit    1560 non-null bool\n",
      "dtypes: bool(1)\n",
      "memory usage: 1.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_rf = confusion_matrix(test_1,test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[384 205]\n",
      " [120 851]]\n"
     ]
    }
   ],
   "source": [
    "print(cnf_rf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities=y_conv\n",
    "pro = probabilities.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d522bed2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(test_1,pro[:,1] )\n",
    "fpr, tpr, thresholds = roc_curve(test_1, pro[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Some part of code are copyed from (https://github.com/healthDataScience/deep-learning-HAR) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"LSTM_data.csv\")\n",
    "data = data[['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode',\n",
    "        'speechiness','acousticness','instrumentalness','liveness','valence','tempo','days','hit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:80000].to_csv(\"Train_data_lstm.csv\",index = None)\n",
    "data[80000:len(data)].to_csv(\"Test_data_lstm.csv\",index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    path = file_name + \".csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode',\n",
    "        'speechiness','acousticness','instrumentalness','liveness','valence','tempo','days','hit']]\n",
    "    df.hit.replace((\"Yes\", \"No\"), (1, 0), inplace=True)\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 1\n",
    "    n_channels = 15\n",
    "    \n",
    "    \"labels\"\n",
    "    labels = df[\"hit\"]\n",
    "    \n",
    "    # Initiate array\n",
    "    X = np.zeros((len(labels), n_steps, n_channels))\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        for j in range(0, n_steps):\n",
    "            #seg = segments[j]\n",
    "            a = df.loc[i][0:15].tolist() # 15 channels\n",
    "            tmp = []\n",
    "            tmp = tmp + a\n",
    "            tmp = np.asarray(tmp)\n",
    "            X[i,j,:] = tmp\n",
    "        if i%20000 == 0:\n",
    "            print(i)  \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "    # Standardize train and test\n",
    "    X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "    X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 2):\n",
    "    expansion = np.eye(n_class)  # generate diagonal matrix\n",
    "    y = expansion[:, labels-1].T  # Transpose the matrix\n",
    "    assert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x_train, labels_train = read_data(\"Train_data_lstm\") # train\n",
    "x_test, labels_test = read_data(\"Test_data_lstm\") # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train,  # We want to keep the order of the data, so we trun of the shuffle \n",
    "                                                shuffle=False,\n",
    "                                                stratify = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 45         # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 200       # Batch size\n",
    "seq_len = 1          # Number of steps\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 200\n",
    "\n",
    "# Fixed\n",
    "n_classes = 2\n",
    "n_channels = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # （N, seq_len, channels ）reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=tf.nn.relu) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)  # Add basic LTSM Cell\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers) # Add multipul RNN Cell\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-97-8c3c177c07e1>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    #Creates a recurrent neural network specified by RNNCell cell\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits',activation=tf.nn.relu)\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping: to avoid gradient explosion\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "    \n",
    "    #1.Compute the gradients with compute_gradients()\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    \n",
    "    #2.Process the gradients as you wish\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    #Given a tensor t, this operation returns a tensor of the same type and shape as t with its values clipped \n",
    "    #toclip_value_min and clip_value_max. Any values less than clip_value_min are set to clip_value_min. \n",
    "    #Any values greater than clip_value_max are set to clip_value_max.\n",
    "    \n",
    "    #3.Apply the processed gradients with apply_gradients().\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 Iteration: 50 Train loss: 0.617617 Train acc: 0.725000\n",
      "Epoch: 0/200 Iteration: 100 Train loss: 0.693046 Train acc: 0.600000\n",
      "Epoch: 0/200 Iteration: 150 Train loss: 0.599799 Train acc: 0.725000\n",
      "Epoch: 0/200 Iteration: 200 Train loss: 0.581368 Train acc: 0.730000\n",
      "Epoch: 0/200 Iteration: 250 Train loss: 0.623055 Train acc: 0.705000\n",
      "Epoch: 0/200 Iteration: 250 Validation loss: 0.683612 Validation acc: 0.706900\n",
      "Epoch: 0/200 Iteration: 300 Train loss: 0.612897 Train acc: 0.720000\n",
      "Epoch: 1/200 Iteration: 350 Train loss: 0.593036 Train acc: 0.725000\n",
      "Epoch: 1/200 Iteration: 400 Train loss: 0.696407 Train acc: 0.600000\n",
      "Epoch: 1/200 Iteration: 450 Train loss: 0.601250 Train acc: 0.725000\n",
      "Epoch: 1/200 Iteration: 500 Train loss: 0.605033 Train acc: 0.730000\n",
      "Epoch: 1/200 Iteration: 500 Validation loss: 0.680356 Validation acc: 0.706200\n",
      "Epoch: 1/200 Iteration: 550 Train loss: 0.609042 Train acc: 0.705000\n",
      "Epoch: 1/200 Iteration: 600 Train loss: 0.602133 Train acc: 0.715000\n",
      "Epoch: 2/200 Iteration: 650 Train loss: 0.592595 Train acc: 0.725000\n",
      "Epoch: 2/200 Iteration: 700 Train loss: 0.703373 Train acc: 0.600000\n",
      "Epoch: 2/200 Iteration: 750 Train loss: 0.583408 Train acc: 0.725000\n",
      "Epoch: 2/200 Iteration: 750 Validation loss: 0.672483 Validation acc: 0.679800\n",
      "Epoch: 2/200 Iteration: 800 Train loss: 0.586726 Train acc: 0.730000\n",
      "Epoch: 2/200 Iteration: 850 Train loss: 0.588764 Train acc: 0.705000\n",
      "Epoch: 2/200 Iteration: 900 Train loss: 0.579916 Train acc: 0.725000\n",
      "Epoch: 3/200 Iteration: 950 Train loss: 0.579280 Train acc: 0.720000\n",
      "Epoch: 3/200 Iteration: 1000 Train loss: 0.683872 Train acc: 0.615000\n",
      "Epoch: 3/200 Iteration: 1000 Validation loss: 0.652794 Validation acc: 0.657600\n",
      "Epoch: 3/200 Iteration: 1050 Train loss: 0.535515 Train acc: 0.735000\n",
      "Epoch: 3/200 Iteration: 1100 Train loss: 0.549698 Train acc: 0.715000\n",
      "Epoch: 3/200 Iteration: 1150 Train loss: 0.554453 Train acc: 0.715000\n",
      "Epoch: 3/200 Iteration: 1200 Train loss: 0.512978 Train acc: 0.715000\n",
      "Epoch: 4/200 Iteration: 1250 Train loss: 0.544380 Train acc: 0.705000\n",
      "Epoch: 4/200 Iteration: 1250 Validation loss: 0.624794 Validation acc: 0.647600\n",
      "Epoch: 4/200 Iteration: 1300 Train loss: 0.658977 Train acc: 0.670000\n",
      "Epoch: 4/200 Iteration: 1350 Train loss: 0.523817 Train acc: 0.725000\n",
      "Epoch: 4/200 Iteration: 1400 Train loss: 0.541640 Train acc: 0.735000\n",
      "Epoch: 4/200 Iteration: 1450 Train loss: 0.538311 Train acc: 0.690000\n",
      "Epoch: 4/200 Iteration: 1500 Train loss: 0.497958 Train acc: 0.740000\n",
      "Epoch: 4/200 Iteration: 1500 Validation loss: 0.603060 Validation acc: 0.655000\n",
      "Epoch: 5/200 Iteration: 1550 Train loss: 0.575776 Train acc: 0.725000\n",
      "Epoch: 5/200 Iteration: 1600 Train loss: 0.667603 Train acc: 0.670000\n",
      "Epoch: 5/200 Iteration: 1650 Train loss: 0.518872 Train acc: 0.740000\n",
      "Epoch: 5/200 Iteration: 1700 Train loss: 0.519418 Train acc: 0.705000\n",
      "Epoch: 5/200 Iteration: 1750 Train loss: 0.545903 Train acc: 0.720000\n",
      "Epoch: 5/200 Iteration: 1750 Validation loss: 0.601546 Validation acc: 0.654500\n",
      "Epoch: 5/200 Iteration: 1800 Train loss: 0.490177 Train acc: 0.750000\n",
      "Epoch: 6/200 Iteration: 1850 Train loss: 0.555904 Train acc: 0.705000\n",
      "Epoch: 6/200 Iteration: 1900 Train loss: 0.660523 Train acc: 0.680000\n",
      "Epoch: 6/200 Iteration: 1950 Train loss: 0.539198 Train acc: 0.730000\n",
      "Epoch: 6/200 Iteration: 2000 Train loss: 0.540876 Train acc: 0.735000\n",
      "Epoch: 6/200 Iteration: 2000 Validation loss: 0.593976 Validation acc: 0.666800\n",
      "Epoch: 6/200 Iteration: 2050 Train loss: 0.511112 Train acc: 0.735000\n",
      "Epoch: 6/200 Iteration: 2100 Train loss: 0.467458 Train acc: 0.750000\n",
      "Epoch: 7/200 Iteration: 2150 Train loss: 0.583015 Train acc: 0.690000\n",
      "Epoch: 7/200 Iteration: 2200 Train loss: 0.659623 Train acc: 0.675000\n",
      "Epoch: 7/200 Iteration: 2250 Train loss: 0.510126 Train acc: 0.720000\n",
      "Epoch: 7/200 Iteration: 2250 Validation loss: 0.587098 Validation acc: 0.678050\n",
      "Epoch: 7/200 Iteration: 2300 Train loss: 0.536621 Train acc: 0.705000\n",
      "Epoch: 7/200 Iteration: 2350 Train loss: 0.510458 Train acc: 0.735000\n",
      "Epoch: 7/200 Iteration: 2400 Train loss: 0.481375 Train acc: 0.760000\n",
      "Epoch: 8/200 Iteration: 2450 Train loss: 0.593143 Train acc: 0.670000\n",
      "Epoch: 8/200 Iteration: 2500 Train loss: 0.649310 Train acc: 0.670000\n",
      "Epoch: 8/200 Iteration: 2500 Validation loss: 0.580716 Validation acc: 0.685950\n",
      "Epoch: 8/200 Iteration: 2550 Train loss: 0.526918 Train acc: 0.750000\n",
      "Epoch: 8/200 Iteration: 2600 Train loss: 0.527857 Train acc: 0.695000\n",
      "Epoch: 8/200 Iteration: 2650 Train loss: 0.499288 Train acc: 0.755000\n",
      "Epoch: 8/200 Iteration: 2700 Train loss: 0.472162 Train acc: 0.755000\n",
      "Epoch: 9/200 Iteration: 2750 Train loss: 0.588994 Train acc: 0.675000\n",
      "Epoch: 9/200 Iteration: 2750 Validation loss: 0.575635 Validation acc: 0.682700\n",
      "Epoch: 9/200 Iteration: 2800 Train loss: 0.650193 Train acc: 0.680000\n",
      "Epoch: 9/200 Iteration: 2850 Train loss: 0.508687 Train acc: 0.775000\n",
      "Epoch: 9/200 Iteration: 2900 Train loss: 0.521453 Train acc: 0.730000\n",
      "Epoch: 9/200 Iteration: 2950 Train loss: 0.510027 Train acc: 0.730000\n",
      "Epoch: 9/200 Iteration: 3000 Train loss: 0.482966 Train acc: 0.790000\n",
      "Epoch: 9/200 Iteration: 3000 Validation loss: 0.569353 Validation acc: 0.692250\n",
      "Epoch: 10/200 Iteration: 3050 Train loss: 0.583434 Train acc: 0.710000\n",
      "Epoch: 10/200 Iteration: 3100 Train loss: 0.650233 Train acc: 0.675000\n",
      "Epoch: 10/200 Iteration: 3150 Train loss: 0.516623 Train acc: 0.755000\n",
      "Epoch: 10/200 Iteration: 3200 Train loss: 0.545520 Train acc: 0.730000\n",
      "Epoch: 10/200 Iteration: 3250 Train loss: 0.501733 Train acc: 0.750000\n",
      "Epoch: 10/200 Iteration: 3250 Validation loss: 0.570689 Validation acc: 0.689100\n",
      "Epoch: 10/200 Iteration: 3300 Train loss: 0.484699 Train acc: 0.760000\n",
      "Epoch: 11/200 Iteration: 3350 Train loss: 0.588988 Train acc: 0.705000\n",
      "Epoch: 11/200 Iteration: 3400 Train loss: 0.634268 Train acc: 0.695000\n",
      "Epoch: 11/200 Iteration: 3450 Train loss: 0.515221 Train acc: 0.750000\n",
      "Epoch: 11/200 Iteration: 3500 Train loss: 0.530160 Train acc: 0.715000\n",
      "Epoch: 11/200 Iteration: 3500 Validation loss: 0.567035 Validation acc: 0.699200\n",
      "Epoch: 11/200 Iteration: 3550 Train loss: 0.500974 Train acc: 0.735000\n",
      "Epoch: 11/200 Iteration: 3600 Train loss: 0.461098 Train acc: 0.775000\n",
      "Epoch: 12/200 Iteration: 3650 Train loss: 0.581354 Train acc: 0.670000\n",
      "Epoch: 12/200 Iteration: 3700 Train loss: 0.664753 Train acc: 0.675000\n",
      "Epoch: 12/200 Iteration: 3750 Train loss: 0.528849 Train acc: 0.750000\n",
      "Epoch: 12/200 Iteration: 3750 Validation loss: 0.565097 Validation acc: 0.701500\n",
      "Epoch: 12/200 Iteration: 3800 Train loss: 0.517599 Train acc: 0.725000\n",
      "Epoch: 12/200 Iteration: 3850 Train loss: 0.498231 Train acc: 0.760000\n",
      "Epoch: 12/200 Iteration: 3900 Train loss: 0.464402 Train acc: 0.780000\n",
      "Epoch: 13/200 Iteration: 3950 Train loss: 0.606317 Train acc: 0.665000\n",
      "Epoch: 13/200 Iteration: 4000 Train loss: 0.646950 Train acc: 0.675000\n",
      "Epoch: 13/200 Iteration: 4000 Validation loss: 0.562787 Validation acc: 0.704150\n",
      "Epoch: 13/200 Iteration: 4050 Train loss: 0.519248 Train acc: 0.750000\n",
      "Epoch: 13/200 Iteration: 4100 Train loss: 0.539077 Train acc: 0.720000\n",
      "Epoch: 13/200 Iteration: 4150 Train loss: 0.488941 Train acc: 0.760000\n",
      "Epoch: 13/200 Iteration: 4200 Train loss: 0.453048 Train acc: 0.760000\n",
      "Epoch: 14/200 Iteration: 4250 Train loss: 0.587644 Train acc: 0.680000\n",
      "Epoch: 14/200 Iteration: 4250 Validation loss: 0.561715 Validation acc: 0.699500\n",
      "Epoch: 14/200 Iteration: 4300 Train loss: 0.643716 Train acc: 0.690000\n",
      "Epoch: 14/200 Iteration: 4350 Train loss: 0.524668 Train acc: 0.735000\n",
      "Epoch: 14/200 Iteration: 4400 Train loss: 0.543266 Train acc: 0.730000\n",
      "Epoch: 14/200 Iteration: 4450 Train loss: 0.484094 Train acc: 0.780000\n",
      "Epoch: 14/200 Iteration: 4500 Train loss: 0.463896 Train acc: 0.750000\n",
      "Epoch: 14/200 Iteration: 4500 Validation loss: 0.559333 Validation acc: 0.699750\n",
      "Epoch: 15/200 Iteration: 4550 Train loss: 0.586998 Train acc: 0.695000\n",
      "Epoch: 15/200 Iteration: 4600 Train loss: 0.644207 Train acc: 0.700000\n",
      "Epoch: 15/200 Iteration: 4650 Train loss: 0.521021 Train acc: 0.765000\n",
      "Epoch: 15/200 Iteration: 4700 Train loss: 0.518038 Train acc: 0.730000\n",
      "Epoch: 15/200 Iteration: 4750 Train loss: 0.483036 Train acc: 0.765000\n",
      "Epoch: 15/200 Iteration: 4750 Validation loss: 0.560788 Validation acc: 0.695500\n",
      "Epoch: 15/200 Iteration: 4800 Train loss: 0.455829 Train acc: 0.785000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/200 Iteration: 4850 Train loss: 0.568839 Train acc: 0.710000\n",
      "Epoch: 16/200 Iteration: 4900 Train loss: 0.624816 Train acc: 0.710000\n",
      "Epoch: 16/200 Iteration: 4950 Train loss: 0.508082 Train acc: 0.760000\n",
      "Epoch: 16/200 Iteration: 5000 Train loss: 0.542312 Train acc: 0.730000\n",
      "Epoch: 16/200 Iteration: 5000 Validation loss: 0.558434 Validation acc: 0.699100\n",
      "Epoch: 16/200 Iteration: 5050 Train loss: 0.476580 Train acc: 0.785000\n",
      "Epoch: 16/200 Iteration: 5100 Train loss: 0.440158 Train acc: 0.775000\n",
      "Epoch: 17/200 Iteration: 5150 Train loss: 0.577342 Train acc: 0.710000\n",
      "Epoch: 17/200 Iteration: 5200 Train loss: 0.642038 Train acc: 0.715000\n",
      "Epoch: 17/200 Iteration: 5250 Train loss: 0.521654 Train acc: 0.740000\n",
      "Epoch: 17/200 Iteration: 5250 Validation loss: 0.556295 Validation acc: 0.704900\n",
      "Epoch: 17/200 Iteration: 5300 Train loss: 0.524765 Train acc: 0.740000\n",
      "Epoch: 17/200 Iteration: 5350 Train loss: 0.473271 Train acc: 0.785000\n",
      "Epoch: 17/200 Iteration: 5400 Train loss: 0.456972 Train acc: 0.765000\n",
      "Epoch: 18/200 Iteration: 5450 Train loss: 0.567559 Train acc: 0.720000\n",
      "Epoch: 18/200 Iteration: 5500 Train loss: 0.660295 Train acc: 0.725000\n",
      "Epoch: 18/200 Iteration: 5500 Validation loss: 0.554222 Validation acc: 0.703150\n",
      "Epoch: 18/200 Iteration: 5550 Train loss: 0.513000 Train acc: 0.740000\n",
      "Epoch: 18/200 Iteration: 5600 Train loss: 0.528791 Train acc: 0.730000\n",
      "Epoch: 18/200 Iteration: 5650 Train loss: 0.491372 Train acc: 0.785000\n",
      "Epoch: 18/200 Iteration: 5700 Train loss: 0.438127 Train acc: 0.785000\n",
      "Epoch: 19/200 Iteration: 5750 Train loss: 0.584838 Train acc: 0.705000\n",
      "Epoch: 19/200 Iteration: 5750 Validation loss: 0.553076 Validation acc: 0.702250\n",
      "Epoch: 19/200 Iteration: 5800 Train loss: 0.631330 Train acc: 0.690000\n",
      "Epoch: 19/200 Iteration: 5850 Train loss: 0.521737 Train acc: 0.740000\n",
      "Epoch: 19/200 Iteration: 5900 Train loss: 0.538600 Train acc: 0.730000\n",
      "Epoch: 19/200 Iteration: 5950 Train loss: 0.498544 Train acc: 0.745000\n",
      "Epoch: 19/200 Iteration: 6000 Train loss: 0.453505 Train acc: 0.775000\n",
      "Epoch: 19/200 Iteration: 6000 Validation loss: 0.551388 Validation acc: 0.701400\n",
      "Epoch: 20/200 Iteration: 6050 Train loss: 0.588790 Train acc: 0.650000\n",
      "Epoch: 20/200 Iteration: 6100 Train loss: 0.629424 Train acc: 0.705000\n",
      "Epoch: 20/200 Iteration: 6150 Train loss: 0.510862 Train acc: 0.750000\n",
      "Epoch: 20/200 Iteration: 6200 Train loss: 0.529031 Train acc: 0.735000\n",
      "Epoch: 20/200 Iteration: 6250 Train loss: 0.467054 Train acc: 0.785000\n",
      "Epoch: 20/200 Iteration: 6250 Validation loss: 0.552937 Validation acc: 0.700300\n",
      "Epoch: 20/200 Iteration: 6300 Train loss: 0.456909 Train acc: 0.765000\n",
      "Epoch: 21/200 Iteration: 6350 Train loss: 0.572353 Train acc: 0.715000\n",
      "Epoch: 21/200 Iteration: 6400 Train loss: 0.611893 Train acc: 0.715000\n",
      "Epoch: 21/200 Iteration: 6450 Train loss: 0.523169 Train acc: 0.755000\n",
      "Epoch: 21/200 Iteration: 6500 Train loss: 0.524699 Train acc: 0.740000\n",
      "Epoch: 21/200 Iteration: 6500 Validation loss: 0.551309 Validation acc: 0.699550\n",
      "Epoch: 21/200 Iteration: 6550 Train loss: 0.485111 Train acc: 0.770000\n",
      "Epoch: 21/200 Iteration: 6600 Train loss: 0.455638 Train acc: 0.785000\n",
      "Epoch: 22/200 Iteration: 6650 Train loss: 0.564805 Train acc: 0.690000\n",
      "Epoch: 22/200 Iteration: 6700 Train loss: 0.630913 Train acc: 0.695000\n",
      "Epoch: 22/200 Iteration: 6750 Train loss: 0.523850 Train acc: 0.740000\n",
      "Epoch: 22/200 Iteration: 6750 Validation loss: 0.548125 Validation acc: 0.708300\n",
      "Epoch: 22/200 Iteration: 6800 Train loss: 0.525333 Train acc: 0.740000\n",
      "Epoch: 22/200 Iteration: 6850 Train loss: 0.471997 Train acc: 0.785000\n",
      "Epoch: 22/200 Iteration: 6900 Train loss: 0.454602 Train acc: 0.760000\n",
      "Epoch: 23/200 Iteration: 6950 Train loss: 0.584388 Train acc: 0.695000\n",
      "Epoch: 23/200 Iteration: 7000 Train loss: 0.616303 Train acc: 0.715000\n",
      "Epoch: 23/200 Iteration: 7000 Validation loss: 0.546630 Validation acc: 0.706200\n",
      "Epoch: 23/200 Iteration: 7050 Train loss: 0.510747 Train acc: 0.735000\n",
      "Epoch: 23/200 Iteration: 7100 Train loss: 0.515645 Train acc: 0.745000\n",
      "Epoch: 23/200 Iteration: 7150 Train loss: 0.485171 Train acc: 0.755000\n",
      "Epoch: 23/200 Iteration: 7200 Train loss: 0.435844 Train acc: 0.760000\n",
      "Epoch: 24/200 Iteration: 7250 Train loss: 0.569701 Train acc: 0.715000\n",
      "Epoch: 24/200 Iteration: 7250 Validation loss: 0.544290 Validation acc: 0.708250\n",
      "Epoch: 24/200 Iteration: 7300 Train loss: 0.616373 Train acc: 0.720000\n",
      "Epoch: 24/200 Iteration: 7350 Train loss: 0.508841 Train acc: 0.750000\n",
      "Epoch: 24/200 Iteration: 7400 Train loss: 0.528337 Train acc: 0.735000\n",
      "Epoch: 24/200 Iteration: 7450 Train loss: 0.480911 Train acc: 0.770000\n",
      "Epoch: 24/200 Iteration: 7500 Train loss: 0.444067 Train acc: 0.765000\n",
      "Epoch: 24/200 Iteration: 7500 Validation loss: 0.544278 Validation acc: 0.704450\n",
      "Epoch: 25/200 Iteration: 7550 Train loss: 0.569500 Train acc: 0.715000\n",
      "Epoch: 25/200 Iteration: 7600 Train loss: 0.641426 Train acc: 0.740000\n",
      "Epoch: 25/200 Iteration: 7650 Train loss: 0.505491 Train acc: 0.755000\n",
      "Epoch: 25/200 Iteration: 7700 Train loss: 0.527640 Train acc: 0.735000\n",
      "Epoch: 25/200 Iteration: 7750 Train loss: 0.457743 Train acc: 0.790000\n",
      "Epoch: 25/200 Iteration: 7750 Validation loss: 0.546665 Validation acc: 0.699750\n",
      "Epoch: 25/200 Iteration: 7800 Train loss: 0.441014 Train acc: 0.785000\n",
      "Epoch: 26/200 Iteration: 7850 Train loss: 0.575957 Train acc: 0.695000\n",
      "Epoch: 26/200 Iteration: 7900 Train loss: 0.630700 Train acc: 0.700000\n",
      "Epoch: 26/200 Iteration: 7950 Train loss: 0.532061 Train acc: 0.725000\n",
      "Epoch: 26/200 Iteration: 8000 Train loss: 0.532047 Train acc: 0.715000\n",
      "Epoch: 26/200 Iteration: 8000 Validation loss: 0.544625 Validation acc: 0.706800\n",
      "Epoch: 26/200 Iteration: 8050 Train loss: 0.470700 Train acc: 0.795000\n",
      "Epoch: 26/200 Iteration: 8100 Train loss: 0.421314 Train acc: 0.790000\n",
      "Epoch: 27/200 Iteration: 8150 Train loss: 0.566875 Train acc: 0.725000\n",
      "Epoch: 27/200 Iteration: 8200 Train loss: 0.662199 Train acc: 0.695000\n",
      "Epoch: 27/200 Iteration: 8250 Train loss: 0.516909 Train acc: 0.760000\n",
      "Epoch: 27/200 Iteration: 8250 Validation loss: 0.541439 Validation acc: 0.704200\n",
      "Epoch: 27/200 Iteration: 8300 Train loss: 0.520007 Train acc: 0.725000\n",
      "Epoch: 27/200 Iteration: 8350 Train loss: 0.451353 Train acc: 0.785000\n",
      "Epoch: 27/200 Iteration: 8400 Train loss: 0.438262 Train acc: 0.775000\n",
      "Epoch: 28/200 Iteration: 8450 Train loss: 0.578912 Train acc: 0.685000\n",
      "Epoch: 28/200 Iteration: 8500 Train loss: 0.633738 Train acc: 0.685000\n",
      "Epoch: 28/200 Iteration: 8500 Validation loss: 0.540591 Validation acc: 0.707050\n",
      "Epoch: 28/200 Iteration: 8550 Train loss: 0.507390 Train acc: 0.770000\n",
      "Epoch: 28/200 Iteration: 8600 Train loss: 0.523122 Train acc: 0.730000\n",
      "Epoch: 28/200 Iteration: 8650 Train loss: 0.467703 Train acc: 0.785000\n",
      "Epoch: 28/200 Iteration: 8700 Train loss: 0.430427 Train acc: 0.780000\n",
      "Epoch: 29/200 Iteration: 8750 Train loss: 0.573646 Train acc: 0.690000\n",
      "Epoch: 29/200 Iteration: 8750 Validation loss: 0.539482 Validation acc: 0.703800\n",
      "Epoch: 29/200 Iteration: 8800 Train loss: 0.619175 Train acc: 0.715000\n",
      "Epoch: 29/200 Iteration: 8850 Train loss: 0.516307 Train acc: 0.770000\n",
      "Epoch: 29/200 Iteration: 8900 Train loss: 0.524650 Train acc: 0.740000\n",
      "Epoch: 29/200 Iteration: 8950 Train loss: 0.466118 Train acc: 0.785000\n",
      "Epoch: 29/200 Iteration: 9000 Train loss: 0.423043 Train acc: 0.775000\n",
      "Epoch: 29/200 Iteration: 9000 Validation loss: 0.541064 Validation acc: 0.703450\n",
      "Epoch: 30/200 Iteration: 9050 Train loss: 0.580362 Train acc: 0.715000\n",
      "Epoch: 30/200 Iteration: 9100 Train loss: 0.644076 Train acc: 0.700000\n",
      "Epoch: 30/200 Iteration: 9150 Train loss: 0.521236 Train acc: 0.780000\n",
      "Epoch: 30/200 Iteration: 9200 Train loss: 0.523863 Train acc: 0.705000\n",
      "Epoch: 30/200 Iteration: 9250 Train loss: 0.468666 Train acc: 0.785000\n",
      "Epoch: 30/200 Iteration: 9250 Validation loss: 0.543485 Validation acc: 0.703050\n",
      "Epoch: 30/200 Iteration: 9300 Train loss: 0.411193 Train acc: 0.815000\n",
      "Epoch: 31/200 Iteration: 9350 Train loss: 0.576988 Train acc: 0.695000\n",
      "Epoch: 31/200 Iteration: 9400 Train loss: 0.619532 Train acc: 0.710000\n",
      "Epoch: 31/200 Iteration: 9450 Train loss: 0.509508 Train acc: 0.780000\n",
      "Epoch: 31/200 Iteration: 9500 Train loss: 0.523321 Train acc: 0.730000\n",
      "Epoch: 31/200 Iteration: 9500 Validation loss: 0.541061 Validation acc: 0.703500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/200 Iteration: 9550 Train loss: 0.488494 Train acc: 0.760000\n",
      "Epoch: 31/200 Iteration: 9600 Train loss: 0.442841 Train acc: 0.795000\n",
      "Epoch: 32/200 Iteration: 9650 Train loss: 0.570862 Train acc: 0.735000\n",
      "Epoch: 32/200 Iteration: 9700 Train loss: 0.605438 Train acc: 0.710000\n",
      "Epoch: 32/200 Iteration: 9750 Train loss: 0.509313 Train acc: 0.765000\n",
      "Epoch: 32/200 Iteration: 9750 Validation loss: 0.538954 Validation acc: 0.706500\n",
      "Epoch: 32/200 Iteration: 9800 Train loss: 0.541200 Train acc: 0.730000\n",
      "Epoch: 32/200 Iteration: 9850 Train loss: 0.466359 Train acc: 0.805000\n",
      "Epoch: 32/200 Iteration: 9900 Train loss: 0.420825 Train acc: 0.785000\n",
      "Epoch: 33/200 Iteration: 9950 Train loss: 0.569996 Train acc: 0.705000\n",
      "Epoch: 33/200 Iteration: 10000 Train loss: 0.601725 Train acc: 0.700000\n",
      "Epoch: 33/200 Iteration: 10000 Validation loss: 0.537427 Validation acc: 0.707150\n",
      "Epoch: 33/200 Iteration: 10050 Train loss: 0.509586 Train acc: 0.760000\n",
      "Epoch: 33/200 Iteration: 10100 Train loss: 0.531714 Train acc: 0.730000\n",
      "Epoch: 33/200 Iteration: 10150 Train loss: 0.479045 Train acc: 0.775000\n",
      "Epoch: 33/200 Iteration: 10200 Train loss: 0.418939 Train acc: 0.800000\n",
      "Epoch: 34/200 Iteration: 10250 Train loss: 0.570607 Train acc: 0.700000\n",
      "Epoch: 34/200 Iteration: 10250 Validation loss: 0.537003 Validation acc: 0.708500\n",
      "Epoch: 34/200 Iteration: 10300 Train loss: 0.627908 Train acc: 0.730000\n",
      "Epoch: 34/200 Iteration: 10350 Train loss: 0.493720 Train acc: 0.760000\n",
      "Epoch: 34/200 Iteration: 10400 Train loss: 0.514740 Train acc: 0.745000\n",
      "Epoch: 34/200 Iteration: 10450 Train loss: 0.449226 Train acc: 0.790000\n",
      "Epoch: 34/200 Iteration: 10500 Train loss: 0.412356 Train acc: 0.790000\n",
      "Epoch: 34/200 Iteration: 10500 Validation loss: 0.538833 Validation acc: 0.703850\n",
      "Epoch: 35/200 Iteration: 10550 Train loss: 0.566877 Train acc: 0.695000\n",
      "Epoch: 35/200 Iteration: 10600 Train loss: 0.631441 Train acc: 0.700000\n",
      "Epoch: 35/200 Iteration: 10650 Train loss: 0.505698 Train acc: 0.745000\n",
      "Epoch: 35/200 Iteration: 10700 Train loss: 0.525187 Train acc: 0.750000\n",
      "Epoch: 35/200 Iteration: 10750 Train loss: 0.460961 Train acc: 0.805000\n",
      "Epoch: 35/200 Iteration: 10750 Validation loss: 0.540133 Validation acc: 0.703150\n",
      "Epoch: 35/200 Iteration: 10800 Train loss: 0.425930 Train acc: 0.795000\n",
      "Epoch: 36/200 Iteration: 10850 Train loss: 0.568945 Train acc: 0.720000\n",
      "Epoch: 36/200 Iteration: 10900 Train loss: 0.609745 Train acc: 0.740000\n",
      "Epoch: 36/200 Iteration: 10950 Train loss: 0.524245 Train acc: 0.760000\n",
      "Epoch: 36/200 Iteration: 11000 Train loss: 0.523386 Train acc: 0.735000\n",
      "Epoch: 36/200 Iteration: 11000 Validation loss: 0.538559 Validation acc: 0.706150\n",
      "Epoch: 36/200 Iteration: 11050 Train loss: 0.447376 Train acc: 0.790000\n",
      "Epoch: 36/200 Iteration: 11100 Train loss: 0.423096 Train acc: 0.770000\n",
      "Epoch: 37/200 Iteration: 11150 Train loss: 0.566461 Train acc: 0.695000\n",
      "Epoch: 37/200 Iteration: 11200 Train loss: 0.626045 Train acc: 0.710000\n",
      "Epoch: 37/200 Iteration: 11250 Train loss: 0.501001 Train acc: 0.755000\n",
      "Epoch: 37/200 Iteration: 11250 Validation loss: 0.536566 Validation acc: 0.704450\n",
      "Epoch: 37/200 Iteration: 11300 Train loss: 0.516267 Train acc: 0.745000\n",
      "Epoch: 37/200 Iteration: 11350 Train loss: 0.451952 Train acc: 0.775000\n",
      "Epoch: 37/200 Iteration: 11400 Train loss: 0.421978 Train acc: 0.790000\n",
      "Epoch: 38/200 Iteration: 11450 Train loss: 0.578245 Train acc: 0.705000\n",
      "Epoch: 38/200 Iteration: 11500 Train loss: 0.636389 Train acc: 0.700000\n",
      "Epoch: 38/200 Iteration: 11500 Validation loss: 0.535404 Validation acc: 0.705250\n",
      "Epoch: 38/200 Iteration: 11550 Train loss: 0.511188 Train acc: 0.790000\n",
      "Epoch: 38/200 Iteration: 11600 Train loss: 0.515496 Train acc: 0.765000\n",
      "Epoch: 38/200 Iteration: 11650 Train loss: 0.445541 Train acc: 0.785000\n",
      "Epoch: 38/200 Iteration: 11700 Train loss: 0.421258 Train acc: 0.785000\n",
      "Epoch: 39/200 Iteration: 11750 Train loss: 0.581078 Train acc: 0.725000\n",
      "Epoch: 39/200 Iteration: 11750 Validation loss: 0.534430 Validation acc: 0.703900\n",
      "Epoch: 39/200 Iteration: 11800 Train loss: 0.624810 Train acc: 0.675000\n",
      "Epoch: 39/200 Iteration: 11850 Train loss: 0.502084 Train acc: 0.755000\n",
      "Epoch: 39/200 Iteration: 11900 Train loss: 0.544526 Train acc: 0.735000\n",
      "Epoch: 39/200 Iteration: 11950 Train loss: 0.447787 Train acc: 0.775000\n",
      "Epoch: 39/200 Iteration: 12000 Train loss: 0.436762 Train acc: 0.765000\n",
      "Epoch: 39/200 Iteration: 12000 Validation loss: 0.536097 Validation acc: 0.703750\n",
      "Epoch: 40/200 Iteration: 12050 Train loss: 0.568217 Train acc: 0.705000\n",
      "Epoch: 40/200 Iteration: 12100 Train loss: 0.625788 Train acc: 0.720000\n",
      "Epoch: 40/200 Iteration: 12150 Train loss: 0.492464 Train acc: 0.755000\n",
      "Epoch: 40/200 Iteration: 12200 Train loss: 0.514452 Train acc: 0.730000\n",
      "Epoch: 40/200 Iteration: 12250 Train loss: 0.460943 Train acc: 0.785000\n",
      "Epoch: 40/200 Iteration: 12250 Validation loss: 0.538589 Validation acc: 0.705450\n",
      "Epoch: 40/200 Iteration: 12300 Train loss: 0.419991 Train acc: 0.770000\n",
      "Epoch: 41/200 Iteration: 12350 Train loss: 0.552792 Train acc: 0.715000\n",
      "Epoch: 41/200 Iteration: 12400 Train loss: 0.627384 Train acc: 0.715000\n",
      "Epoch: 41/200 Iteration: 12450 Train loss: 0.504051 Train acc: 0.780000\n",
      "Epoch: 41/200 Iteration: 12500 Train loss: 0.528171 Train acc: 0.745000\n",
      "Epoch: 41/200 Iteration: 12500 Validation loss: 0.537366 Validation acc: 0.703700\n",
      "Epoch: 41/200 Iteration: 12550 Train loss: 0.447484 Train acc: 0.795000\n",
      "Epoch: 41/200 Iteration: 12600 Train loss: 0.429885 Train acc: 0.780000\n",
      "Epoch: 42/200 Iteration: 12650 Train loss: 0.557265 Train acc: 0.725000\n",
      "Epoch: 42/200 Iteration: 12700 Train loss: 0.594677 Train acc: 0.735000\n",
      "Epoch: 42/200 Iteration: 12750 Train loss: 0.505078 Train acc: 0.770000\n",
      "Epoch: 42/200 Iteration: 12750 Validation loss: 0.535888 Validation acc: 0.704500\n",
      "Epoch: 42/200 Iteration: 12800 Train loss: 0.522141 Train acc: 0.750000\n",
      "Epoch: 42/200 Iteration: 12850 Train loss: 0.458554 Train acc: 0.800000\n",
      "Epoch: 42/200 Iteration: 12900 Train loss: 0.450975 Train acc: 0.770000\n",
      "Epoch: 43/200 Iteration: 12950 Train loss: 0.556725 Train acc: 0.705000\n",
      "Epoch: 43/200 Iteration: 13000 Train loss: 0.597130 Train acc: 0.730000\n",
      "Epoch: 43/200 Iteration: 13000 Validation loss: 0.535030 Validation acc: 0.704550\n",
      "Epoch: 43/200 Iteration: 13050 Train loss: 0.500177 Train acc: 0.755000\n",
      "Epoch: 43/200 Iteration: 13100 Train loss: 0.529808 Train acc: 0.730000\n",
      "Epoch: 43/200 Iteration: 13150 Train loss: 0.445012 Train acc: 0.805000\n",
      "Epoch: 43/200 Iteration: 13200 Train loss: 0.409144 Train acc: 0.795000\n",
      "Epoch: 44/200 Iteration: 13250 Train loss: 0.555867 Train acc: 0.705000\n",
      "Epoch: 44/200 Iteration: 13250 Validation loss: 0.534123 Validation acc: 0.703550\n",
      "Epoch: 44/200 Iteration: 13300 Train loss: 0.624230 Train acc: 0.730000\n",
      "Epoch: 44/200 Iteration: 13350 Train loss: 0.510942 Train acc: 0.740000\n",
      "Epoch: 44/200 Iteration: 13400 Train loss: 0.512229 Train acc: 0.750000\n",
      "Epoch: 44/200 Iteration: 13450 Train loss: 0.453677 Train acc: 0.775000\n",
      "Epoch: 44/200 Iteration: 13500 Train loss: 0.421519 Train acc: 0.780000\n",
      "Epoch: 44/200 Iteration: 13500 Validation loss: 0.536400 Validation acc: 0.701400\n",
      "Epoch: 45/200 Iteration: 13550 Train loss: 0.554598 Train acc: 0.720000\n",
      "Epoch: 45/200 Iteration: 13600 Train loss: 0.622131 Train acc: 0.730000\n",
      "Epoch: 45/200 Iteration: 13650 Train loss: 0.479017 Train acc: 0.770000\n",
      "Epoch: 45/200 Iteration: 13700 Train loss: 0.522613 Train acc: 0.730000\n",
      "Epoch: 45/200 Iteration: 13750 Train loss: 0.467469 Train acc: 0.785000\n",
      "Epoch: 45/200 Iteration: 13750 Validation loss: 0.538950 Validation acc: 0.701350\n",
      "Epoch: 45/200 Iteration: 13800 Train loss: 0.421345 Train acc: 0.790000\n",
      "Epoch: 46/200 Iteration: 13850 Train loss: 0.569215 Train acc: 0.720000\n",
      "Epoch: 46/200 Iteration: 13900 Train loss: 0.591492 Train acc: 0.715000\n",
      "Epoch: 46/200 Iteration: 13950 Train loss: 0.505984 Train acc: 0.750000\n",
      "Epoch: 46/200 Iteration: 14000 Train loss: 0.526656 Train acc: 0.755000\n",
      "Epoch: 46/200 Iteration: 14000 Validation loss: 0.537847 Validation acc: 0.701350\n",
      "Epoch: 46/200 Iteration: 14050 Train loss: 0.458508 Train acc: 0.790000\n",
      "Epoch: 46/200 Iteration: 14100 Train loss: 0.425370 Train acc: 0.775000\n",
      "Epoch: 47/200 Iteration: 14150 Train loss: 0.566062 Train acc: 0.715000\n",
      "Epoch: 47/200 Iteration: 14200 Train loss: 0.604610 Train acc: 0.730000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/200 Iteration: 14250 Train loss: 0.518155 Train acc: 0.745000\n",
      "Epoch: 47/200 Iteration: 14250 Validation loss: 0.535858 Validation acc: 0.703800\n",
      "Epoch: 47/200 Iteration: 14300 Train loss: 0.518534 Train acc: 0.750000\n",
      "Epoch: 47/200 Iteration: 14350 Train loss: 0.447679 Train acc: 0.765000\n",
      "Epoch: 47/200 Iteration: 14400 Train loss: 0.404281 Train acc: 0.785000\n",
      "Epoch: 48/200 Iteration: 14450 Train loss: 0.580775 Train acc: 0.725000\n",
      "Epoch: 48/200 Iteration: 14500 Train loss: 0.619644 Train acc: 0.735000\n",
      "Epoch: 48/200 Iteration: 14500 Validation loss: 0.534251 Validation acc: 0.703950\n",
      "Epoch: 48/200 Iteration: 14550 Train loss: 0.501935 Train acc: 0.745000\n",
      "Epoch: 48/200 Iteration: 14600 Train loss: 0.510015 Train acc: 0.755000\n",
      "Epoch: 48/200 Iteration: 14650 Train loss: 0.448681 Train acc: 0.805000\n",
      "Epoch: 48/200 Iteration: 14700 Train loss: 0.420973 Train acc: 0.790000\n",
      "Epoch: 49/200 Iteration: 14750 Train loss: 0.575470 Train acc: 0.695000\n",
      "Epoch: 49/200 Iteration: 14750 Validation loss: 0.533881 Validation acc: 0.702450\n",
      "Epoch: 49/200 Iteration: 14800 Train loss: 0.600625 Train acc: 0.715000\n",
      "Epoch: 49/200 Iteration: 14850 Train loss: 0.494323 Train acc: 0.755000\n",
      "Epoch: 49/200 Iteration: 14900 Train loss: 0.525995 Train acc: 0.745000\n",
      "Epoch: 49/200 Iteration: 14950 Train loss: 0.439269 Train acc: 0.805000\n",
      "Epoch: 49/200 Iteration: 15000 Train loss: 0.408710 Train acc: 0.780000\n",
      "Epoch: 49/200 Iteration: 15000 Validation loss: 0.535291 Validation acc: 0.701500\n",
      "Epoch: 50/200 Iteration: 15050 Train loss: 0.570966 Train acc: 0.690000\n",
      "Epoch: 50/200 Iteration: 15100 Train loss: 0.606140 Train acc: 0.755000\n",
      "Epoch: 50/200 Iteration: 15150 Train loss: 0.509194 Train acc: 0.760000\n",
      "Epoch: 50/200 Iteration: 15200 Train loss: 0.527784 Train acc: 0.740000\n",
      "Epoch: 50/200 Iteration: 15250 Train loss: 0.442022 Train acc: 0.795000\n",
      "Epoch: 50/200 Iteration: 15250 Validation loss: 0.537593 Validation acc: 0.700200\n",
      "Epoch: 50/200 Iteration: 15300 Train loss: 0.414311 Train acc: 0.775000\n",
      "Epoch: 51/200 Iteration: 15350 Train loss: 0.559714 Train acc: 0.715000\n",
      "Epoch: 51/200 Iteration: 15400 Train loss: 0.594753 Train acc: 0.715000\n",
      "Epoch: 51/200 Iteration: 15450 Train loss: 0.490911 Train acc: 0.750000\n",
      "Epoch: 51/200 Iteration: 15500 Train loss: 0.494934 Train acc: 0.750000\n",
      "Epoch: 51/200 Iteration: 15500 Validation loss: 0.536655 Validation acc: 0.701450\n",
      "Epoch: 51/200 Iteration: 15550 Train loss: 0.447284 Train acc: 0.805000\n",
      "Epoch: 51/200 Iteration: 15600 Train loss: 0.419205 Train acc: 0.795000\n",
      "Epoch: 52/200 Iteration: 15650 Train loss: 0.569224 Train acc: 0.705000\n",
      "Epoch: 52/200 Iteration: 15700 Train loss: 0.632105 Train acc: 0.725000\n",
      "Epoch: 52/200 Iteration: 15750 Train loss: 0.503063 Train acc: 0.780000\n",
      "Epoch: 52/200 Iteration: 15750 Validation loss: 0.535290 Validation acc: 0.700600\n",
      "Epoch: 52/200 Iteration: 15800 Train loss: 0.521974 Train acc: 0.735000\n",
      "Epoch: 52/200 Iteration: 15850 Train loss: 0.439689 Train acc: 0.800000\n",
      "Epoch: 52/200 Iteration: 15900 Train loss: 0.412896 Train acc: 0.795000\n",
      "Epoch: 53/200 Iteration: 15950 Train loss: 0.565291 Train acc: 0.705000\n",
      "Epoch: 53/200 Iteration: 16000 Train loss: 0.582924 Train acc: 0.745000\n",
      "Epoch: 53/200 Iteration: 16000 Validation loss: 0.533876 Validation acc: 0.701800\n",
      "Epoch: 53/200 Iteration: 16050 Train loss: 0.503497 Train acc: 0.740000\n",
      "Epoch: 53/200 Iteration: 16100 Train loss: 0.520656 Train acc: 0.745000\n",
      "Epoch: 53/200 Iteration: 16150 Train loss: 0.439850 Train acc: 0.805000\n",
      "Epoch: 53/200 Iteration: 16200 Train loss: 0.420113 Train acc: 0.780000\n",
      "Epoch: 54/200 Iteration: 16250 Train loss: 0.562674 Train acc: 0.715000\n",
      "Epoch: 54/200 Iteration: 16250 Validation loss: 0.533616 Validation acc: 0.702300\n",
      "Epoch: 54/200 Iteration: 16300 Train loss: 0.593101 Train acc: 0.765000\n",
      "Epoch: 54/200 Iteration: 16350 Train loss: 0.517465 Train acc: 0.785000\n",
      "Epoch: 54/200 Iteration: 16400 Train loss: 0.514059 Train acc: 0.755000\n",
      "Epoch: 54/200 Iteration: 16450 Train loss: 0.446532 Train acc: 0.800000\n",
      "Epoch: 54/200 Iteration: 16500 Train loss: 0.418617 Train acc: 0.785000\n",
      "Epoch: 54/200 Iteration: 16500 Validation loss: 0.535303 Validation acc: 0.700400\n",
      "Epoch: 55/200 Iteration: 16550 Train loss: 0.582256 Train acc: 0.720000\n",
      "Epoch: 55/200 Iteration: 16600 Train loss: 0.606812 Train acc: 0.710000\n",
      "Epoch: 55/200 Iteration: 16650 Train loss: 0.500943 Train acc: 0.745000\n",
      "Epoch: 55/200 Iteration: 16700 Train loss: 0.509943 Train acc: 0.755000\n",
      "Epoch: 55/200 Iteration: 16750 Train loss: 0.426399 Train acc: 0.810000\n",
      "Epoch: 55/200 Iteration: 16750 Validation loss: 0.537613 Validation acc: 0.698200\n",
      "Epoch: 55/200 Iteration: 16800 Train loss: 0.408005 Train acc: 0.780000\n",
      "Epoch: 56/200 Iteration: 16850 Train loss: 0.567428 Train acc: 0.725000\n",
      "Epoch: 56/200 Iteration: 16900 Train loss: 0.587957 Train acc: 0.745000\n",
      "Epoch: 56/200 Iteration: 16950 Train loss: 0.511162 Train acc: 0.750000\n",
      "Epoch: 56/200 Iteration: 17000 Train loss: 0.521945 Train acc: 0.740000\n",
      "Epoch: 56/200 Iteration: 17000 Validation loss: 0.537160 Validation acc: 0.701050\n",
      "Epoch: 56/200 Iteration: 17050 Train loss: 0.434967 Train acc: 0.800000\n",
      "Epoch: 56/200 Iteration: 17100 Train loss: 0.415325 Train acc: 0.820000\n",
      "Epoch: 57/200 Iteration: 17150 Train loss: 0.553985 Train acc: 0.710000\n",
      "Epoch: 57/200 Iteration: 17200 Train loss: 0.590858 Train acc: 0.730000\n",
      "Epoch: 57/200 Iteration: 17250 Train loss: 0.496097 Train acc: 0.755000\n",
      "Epoch: 57/200 Iteration: 17250 Validation loss: 0.535372 Validation acc: 0.699700\n",
      "Epoch: 57/200 Iteration: 17300 Train loss: 0.520302 Train acc: 0.755000\n",
      "Epoch: 57/200 Iteration: 17350 Train loss: 0.441836 Train acc: 0.775000\n",
      "Epoch: 57/200 Iteration: 17400 Train loss: 0.414710 Train acc: 0.775000\n",
      "Epoch: 58/200 Iteration: 17450 Train loss: 0.550624 Train acc: 0.735000\n",
      "Epoch: 58/200 Iteration: 17500 Train loss: 0.596681 Train acc: 0.725000\n",
      "Epoch: 58/200 Iteration: 17500 Validation loss: 0.534142 Validation acc: 0.700350\n",
      "Epoch: 58/200 Iteration: 17550 Train loss: 0.491624 Train acc: 0.765000\n",
      "Epoch: 58/200 Iteration: 17600 Train loss: 0.503019 Train acc: 0.740000\n",
      "Epoch: 58/200 Iteration: 17650 Train loss: 0.436375 Train acc: 0.785000\n",
      "Epoch: 58/200 Iteration: 17700 Train loss: 0.410495 Train acc: 0.785000\n",
      "Epoch: 59/200 Iteration: 17750 Train loss: 0.569042 Train acc: 0.735000\n",
      "Epoch: 59/200 Iteration: 17750 Validation loss: 0.533879 Validation acc: 0.701000\n",
      "Epoch: 59/200 Iteration: 17800 Train loss: 0.559238 Train acc: 0.745000\n",
      "Epoch: 59/200 Iteration: 17850 Train loss: 0.499000 Train acc: 0.755000\n",
      "Epoch: 59/200 Iteration: 17900 Train loss: 0.519135 Train acc: 0.745000\n",
      "Epoch: 59/200 Iteration: 17950 Train loss: 0.443711 Train acc: 0.805000\n",
      "Epoch: 59/200 Iteration: 18000 Train loss: 0.425108 Train acc: 0.770000\n",
      "Epoch: 59/200 Iteration: 18000 Validation loss: 0.535248 Validation acc: 0.699300\n",
      "Epoch: 60/200 Iteration: 18050 Train loss: 0.575456 Train acc: 0.700000\n",
      "Epoch: 60/200 Iteration: 18100 Train loss: 0.602685 Train acc: 0.735000\n",
      "Epoch: 60/200 Iteration: 18150 Train loss: 0.526600 Train acc: 0.760000\n",
      "Epoch: 60/200 Iteration: 18200 Train loss: 0.500227 Train acc: 0.770000\n",
      "Epoch: 60/200 Iteration: 18250 Train loss: 0.437881 Train acc: 0.790000\n",
      "Epoch: 60/200 Iteration: 18250 Validation loss: 0.536901 Validation acc: 0.699050\n",
      "Epoch: 60/200 Iteration: 18300 Train loss: 0.396900 Train acc: 0.800000\n",
      "Epoch: 61/200 Iteration: 18350 Train loss: 0.555349 Train acc: 0.695000\n",
      "Epoch: 61/200 Iteration: 18400 Train loss: 0.600246 Train acc: 0.725000\n",
      "Epoch: 61/200 Iteration: 18450 Train loss: 0.512697 Train acc: 0.755000\n",
      "Epoch: 61/200 Iteration: 18500 Train loss: 0.515524 Train acc: 0.750000\n",
      "Epoch: 61/200 Iteration: 18500 Validation loss: 0.536075 Validation acc: 0.701600\n",
      "Epoch: 61/200 Iteration: 18550 Train loss: 0.447310 Train acc: 0.790000\n",
      "Epoch: 61/200 Iteration: 18600 Train loss: 0.415743 Train acc: 0.785000\n",
      "Epoch: 62/200 Iteration: 18650 Train loss: 0.571592 Train acc: 0.725000\n",
      "Epoch: 62/200 Iteration: 18700 Train loss: 0.608622 Train acc: 0.720000\n",
      "Epoch: 62/200 Iteration: 18750 Train loss: 0.480728 Train acc: 0.765000\n",
      "Epoch: 62/200 Iteration: 18750 Validation loss: 0.534601 Validation acc: 0.701750\n",
      "Epoch: 62/200 Iteration: 18800 Train loss: 0.523708 Train acc: 0.740000\n",
      "Epoch: 62/200 Iteration: 18850 Train loss: 0.422049 Train acc: 0.795000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/200 Iteration: 18900 Train loss: 0.411486 Train acc: 0.805000\n",
      "Epoch: 63/200 Iteration: 18950 Train loss: 0.568051 Train acc: 0.710000\n",
      "Epoch: 63/200 Iteration: 19000 Train loss: 0.576472 Train acc: 0.725000\n",
      "Epoch: 63/200 Iteration: 19000 Validation loss: 0.534049 Validation acc: 0.700500\n",
      "Epoch: 63/200 Iteration: 19050 Train loss: 0.479473 Train acc: 0.765000\n",
      "Epoch: 63/200 Iteration: 19100 Train loss: 0.526371 Train acc: 0.740000\n",
      "Epoch: 63/200 Iteration: 19150 Train loss: 0.432452 Train acc: 0.800000\n",
      "Epoch: 63/200 Iteration: 19200 Train loss: 0.413878 Train acc: 0.800000\n",
      "Epoch: 64/200 Iteration: 19250 Train loss: 0.579757 Train acc: 0.720000\n",
      "Epoch: 64/200 Iteration: 19250 Validation loss: 0.533466 Validation acc: 0.704550\n",
      "Epoch: 64/200 Iteration: 19300 Train loss: 0.590001 Train acc: 0.730000\n",
      "Epoch: 64/200 Iteration: 19350 Train loss: 0.512427 Train acc: 0.735000\n",
      "Epoch: 64/200 Iteration: 19400 Train loss: 0.517218 Train acc: 0.745000\n",
      "Epoch: 64/200 Iteration: 19450 Train loss: 0.431708 Train acc: 0.805000\n",
      "Epoch: 64/200 Iteration: 19500 Train loss: 0.406432 Train acc: 0.770000\n",
      "Epoch: 64/200 Iteration: 19500 Validation loss: 0.535083 Validation acc: 0.700200\n",
      "Epoch: 65/200 Iteration: 19550 Train loss: 0.566780 Train acc: 0.700000\n",
      "Epoch: 65/200 Iteration: 19600 Train loss: 0.593090 Train acc: 0.755000\n",
      "Epoch: 65/200 Iteration: 19650 Train loss: 0.497948 Train acc: 0.770000\n",
      "Epoch: 65/200 Iteration: 19700 Train loss: 0.511829 Train acc: 0.740000\n",
      "Epoch: 65/200 Iteration: 19750 Train loss: 0.431187 Train acc: 0.800000\n",
      "Epoch: 65/200 Iteration: 19750 Validation loss: 0.537341 Validation acc: 0.698800\n",
      "Epoch: 65/200 Iteration: 19800 Train loss: 0.430176 Train acc: 0.800000\n",
      "Epoch: 66/200 Iteration: 19850 Train loss: 0.564718 Train acc: 0.705000\n",
      "Epoch: 66/200 Iteration: 19900 Train loss: 0.579133 Train acc: 0.745000\n",
      "Epoch: 66/200 Iteration: 19950 Train loss: 0.487790 Train acc: 0.775000\n",
      "Epoch: 66/200 Iteration: 20000 Train loss: 0.520447 Train acc: 0.755000\n",
      "Epoch: 66/200 Iteration: 20000 Validation loss: 0.536878 Validation acc: 0.703000\n",
      "Epoch: 66/200 Iteration: 20050 Train loss: 0.439941 Train acc: 0.800000\n",
      "Epoch: 66/200 Iteration: 20100 Train loss: 0.413150 Train acc: 0.790000\n",
      "Epoch: 67/200 Iteration: 20150 Train loss: 0.561074 Train acc: 0.720000\n",
      "Epoch: 67/200 Iteration: 20200 Train loss: 0.590243 Train acc: 0.735000\n",
      "Epoch: 67/200 Iteration: 20250 Train loss: 0.501023 Train acc: 0.770000\n",
      "Epoch: 67/200 Iteration: 20250 Validation loss: 0.535464 Validation acc: 0.701900\n",
      "Epoch: 67/200 Iteration: 20300 Train loss: 0.508781 Train acc: 0.740000\n",
      "Epoch: 67/200 Iteration: 20350 Train loss: 0.438591 Train acc: 0.785000\n",
      "Epoch: 67/200 Iteration: 20400 Train loss: 0.410201 Train acc: 0.815000\n",
      "Epoch: 68/200 Iteration: 20450 Train loss: 0.554601 Train acc: 0.740000\n",
      "Epoch: 68/200 Iteration: 20500 Train loss: 0.592044 Train acc: 0.745000\n",
      "Epoch: 68/200 Iteration: 20500 Validation loss: 0.534344 Validation acc: 0.700350\n",
      "Epoch: 68/200 Iteration: 20550 Train loss: 0.487400 Train acc: 0.755000\n",
      "Epoch: 68/200 Iteration: 20600 Train loss: 0.495713 Train acc: 0.760000\n",
      "Epoch: 68/200 Iteration: 20650 Train loss: 0.419755 Train acc: 0.795000\n",
      "Epoch: 68/200 Iteration: 20700 Train loss: 0.401684 Train acc: 0.805000\n",
      "Epoch: 69/200 Iteration: 20750 Train loss: 0.568005 Train acc: 0.710000\n",
      "Epoch: 69/200 Iteration: 20750 Validation loss: 0.533656 Validation acc: 0.704650\n",
      "Epoch: 69/200 Iteration: 20800 Train loss: 0.586414 Train acc: 0.745000\n",
      "Epoch: 69/200 Iteration: 20850 Train loss: 0.487942 Train acc: 0.750000\n",
      "Epoch: 69/200 Iteration: 20900 Train loss: 0.536676 Train acc: 0.740000\n",
      "Epoch: 69/200 Iteration: 20950 Train loss: 0.437665 Train acc: 0.815000\n",
      "Epoch: 69/200 Iteration: 21000 Train loss: 0.429666 Train acc: 0.795000\n",
      "Epoch: 69/200 Iteration: 21000 Validation loss: 0.535426 Validation acc: 0.698950\n",
      "Epoch: 70/200 Iteration: 21050 Train loss: 0.574519 Train acc: 0.725000\n",
      "Epoch: 70/200 Iteration: 21100 Train loss: 0.599663 Train acc: 0.725000\n",
      "Epoch: 70/200 Iteration: 21150 Train loss: 0.505865 Train acc: 0.760000\n",
      "Epoch: 70/200 Iteration: 21200 Train loss: 0.489429 Train acc: 0.735000\n",
      "Epoch: 70/200 Iteration: 21250 Train loss: 0.427376 Train acc: 0.800000\n",
      "Epoch: 70/200 Iteration: 21250 Validation loss: 0.537595 Validation acc: 0.698800\n",
      "Epoch: 70/200 Iteration: 21300 Train loss: 0.419510 Train acc: 0.810000\n",
      "Epoch: 71/200 Iteration: 21350 Train loss: 0.587822 Train acc: 0.695000\n",
      "Epoch: 71/200 Iteration: 21400 Train loss: 0.574421 Train acc: 0.740000\n",
      "Epoch: 71/200 Iteration: 21450 Train loss: 0.488802 Train acc: 0.775000\n",
      "Epoch: 71/200 Iteration: 21500 Train loss: 0.502905 Train acc: 0.730000\n",
      "Epoch: 71/200 Iteration: 21500 Validation loss: 0.537232 Validation acc: 0.699350\n",
      "Epoch: 71/200 Iteration: 21550 Train loss: 0.425528 Train acc: 0.820000\n",
      "Epoch: 71/200 Iteration: 21600 Train loss: 0.421605 Train acc: 0.790000\n",
      "Epoch: 72/200 Iteration: 21650 Train loss: 0.549696 Train acc: 0.720000\n",
      "Epoch: 72/200 Iteration: 21700 Train loss: 0.575084 Train acc: 0.755000\n",
      "Epoch: 72/200 Iteration: 21750 Train loss: 0.499238 Train acc: 0.750000\n",
      "Epoch: 72/200 Iteration: 21750 Validation loss: 0.536034 Validation acc: 0.698100\n",
      "Epoch: 72/200 Iteration: 21800 Train loss: 0.522921 Train acc: 0.740000\n",
      "Epoch: 72/200 Iteration: 21850 Train loss: 0.429810 Train acc: 0.800000\n",
      "Epoch: 72/200 Iteration: 21900 Train loss: 0.401542 Train acc: 0.795000\n",
      "Epoch: 73/200 Iteration: 21950 Train loss: 0.569447 Train acc: 0.725000\n",
      "Epoch: 73/200 Iteration: 22000 Train loss: 0.564903 Train acc: 0.760000\n",
      "Epoch: 73/200 Iteration: 22000 Validation loss: 0.534776 Validation acc: 0.700500\n",
      "Epoch: 73/200 Iteration: 22050 Train loss: 0.480277 Train acc: 0.780000\n",
      "Epoch: 73/200 Iteration: 22100 Train loss: 0.506524 Train acc: 0.745000\n",
      "Epoch: 73/200 Iteration: 22150 Train loss: 0.409242 Train acc: 0.825000\n",
      "Epoch: 73/200 Iteration: 22200 Train loss: 0.417413 Train acc: 0.800000\n",
      "Epoch: 74/200 Iteration: 22250 Train loss: 0.576907 Train acc: 0.750000\n",
      "Epoch: 74/200 Iteration: 22250 Validation loss: 0.533940 Validation acc: 0.701650\n",
      "Epoch: 74/200 Iteration: 22300 Train loss: 0.587944 Train acc: 0.740000\n",
      "Epoch: 74/200 Iteration: 22350 Train loss: 0.505155 Train acc: 0.750000\n",
      "Epoch: 74/200 Iteration: 22400 Train loss: 0.509443 Train acc: 0.755000\n",
      "Epoch: 74/200 Iteration: 22450 Train loss: 0.436832 Train acc: 0.795000\n",
      "Epoch: 74/200 Iteration: 22500 Train loss: 0.405860 Train acc: 0.815000\n",
      "Epoch: 74/200 Iteration: 22500 Validation loss: 0.535333 Validation acc: 0.699400\n",
      "Epoch: 75/200 Iteration: 22550 Train loss: 0.562591 Train acc: 0.735000\n",
      "Epoch: 75/200 Iteration: 22600 Train loss: 0.583089 Train acc: 0.740000\n",
      "Epoch: 75/200 Iteration: 22650 Train loss: 0.491947 Train acc: 0.775000\n",
      "Epoch: 75/200 Iteration: 22700 Train loss: 0.500653 Train acc: 0.745000\n",
      "Epoch: 75/200 Iteration: 22750 Train loss: 0.414114 Train acc: 0.790000\n",
      "Epoch: 75/200 Iteration: 22750 Validation loss: 0.537254 Validation acc: 0.700050\n",
      "Epoch: 75/200 Iteration: 22800 Train loss: 0.413331 Train acc: 0.800000\n",
      "Epoch: 76/200 Iteration: 22850 Train loss: 0.540570 Train acc: 0.715000\n",
      "Epoch: 76/200 Iteration: 22900 Train loss: 0.596668 Train acc: 0.730000\n",
      "Epoch: 76/200 Iteration: 22950 Train loss: 0.503073 Train acc: 0.755000\n",
      "Epoch: 76/200 Iteration: 23000 Train loss: 0.495282 Train acc: 0.750000\n",
      "Epoch: 76/200 Iteration: 23000 Validation loss: 0.536696 Validation acc: 0.700350\n",
      "Epoch: 76/200 Iteration: 23050 Train loss: 0.426962 Train acc: 0.810000\n",
      "Epoch: 76/200 Iteration: 23100 Train loss: 0.417881 Train acc: 0.770000\n",
      "Epoch: 77/200 Iteration: 23150 Train loss: 0.587515 Train acc: 0.705000\n",
      "Epoch: 77/200 Iteration: 23200 Train loss: 0.572585 Train acc: 0.760000\n",
      "Epoch: 77/200 Iteration: 23250 Train loss: 0.493196 Train acc: 0.755000\n",
      "Epoch: 77/200 Iteration: 23250 Validation loss: 0.535152 Validation acc: 0.699150\n",
      "Epoch: 77/200 Iteration: 23300 Train loss: 0.517704 Train acc: 0.725000\n",
      "Epoch: 77/200 Iteration: 23350 Train loss: 0.413310 Train acc: 0.800000\n",
      "Epoch: 77/200 Iteration: 23400 Train loss: 0.396341 Train acc: 0.830000\n",
      "Epoch: 78/200 Iteration: 23450 Train loss: 0.554415 Train acc: 0.710000\n",
      "Epoch: 78/200 Iteration: 23500 Train loss: 0.584810 Train acc: 0.750000\n",
      "Epoch: 78/200 Iteration: 23500 Validation loss: 0.534027 Validation acc: 0.700600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/200 Iteration: 23550 Train loss: 0.475394 Train acc: 0.770000\n",
      "Epoch: 78/200 Iteration: 23600 Train loss: 0.500937 Train acc: 0.730000\n",
      "Epoch: 78/200 Iteration: 23650 Train loss: 0.412265 Train acc: 0.810000\n",
      "Epoch: 78/200 Iteration: 23700 Train loss: 0.400711 Train acc: 0.815000\n",
      "Epoch: 79/200 Iteration: 23750 Train loss: 0.561868 Train acc: 0.725000\n",
      "Epoch: 79/200 Iteration: 23750 Validation loss: 0.533669 Validation acc: 0.703300\n",
      "Epoch: 79/200 Iteration: 23800 Train loss: 0.601586 Train acc: 0.730000\n",
      "Epoch: 79/200 Iteration: 23850 Train loss: 0.490011 Train acc: 0.755000\n",
      "Epoch: 79/200 Iteration: 23900 Train loss: 0.497647 Train acc: 0.755000\n",
      "Epoch: 79/200 Iteration: 23950 Train loss: 0.416770 Train acc: 0.830000\n",
      "Epoch: 79/200 Iteration: 24000 Train loss: 0.425855 Train acc: 0.785000\n",
      "Epoch: 79/200 Iteration: 24000 Validation loss: 0.534552 Validation acc: 0.701100\n",
      "Epoch: 80/200 Iteration: 24050 Train loss: 0.573830 Train acc: 0.730000\n",
      "Epoch: 80/200 Iteration: 24100 Train loss: 0.581192 Train acc: 0.755000\n",
      "Epoch: 80/200 Iteration: 24150 Train loss: 0.487633 Train acc: 0.750000\n",
      "Epoch: 80/200 Iteration: 24200 Train loss: 0.521353 Train acc: 0.740000\n",
      "Epoch: 80/200 Iteration: 24250 Train loss: 0.408450 Train acc: 0.805000\n",
      "Epoch: 80/200 Iteration: 24250 Validation loss: 0.536000 Validation acc: 0.701750\n",
      "Epoch: 80/200 Iteration: 24300 Train loss: 0.410218 Train acc: 0.800000\n",
      "Epoch: 81/200 Iteration: 24350 Train loss: 0.581245 Train acc: 0.730000\n",
      "Epoch: 81/200 Iteration: 24400 Train loss: 0.572421 Train acc: 0.760000\n",
      "Epoch: 81/200 Iteration: 24450 Train loss: 0.490560 Train acc: 0.755000\n",
      "Epoch: 81/200 Iteration: 24500 Train loss: 0.506499 Train acc: 0.745000\n",
      "Epoch: 81/200 Iteration: 24500 Validation loss: 0.535966 Validation acc: 0.700200\n",
      "Epoch: 81/200 Iteration: 24550 Train loss: 0.411713 Train acc: 0.800000\n",
      "Epoch: 81/200 Iteration: 24600 Train loss: 0.410461 Train acc: 0.775000\n",
      "Epoch: 82/200 Iteration: 24650 Train loss: 0.567195 Train acc: 0.720000\n",
      "Epoch: 82/200 Iteration: 24700 Train loss: 0.567060 Train acc: 0.785000\n",
      "Epoch: 82/200 Iteration: 24750 Train loss: 0.481584 Train acc: 0.770000\n",
      "Epoch: 82/200 Iteration: 24750 Validation loss: 0.534750 Validation acc: 0.701850\n",
      "Epoch: 82/200 Iteration: 24800 Train loss: 0.516881 Train acc: 0.745000\n",
      "Epoch: 82/200 Iteration: 24850 Train loss: 0.402962 Train acc: 0.790000\n",
      "Epoch: 82/200 Iteration: 24900 Train loss: 0.410853 Train acc: 0.780000\n",
      "Epoch: 83/200 Iteration: 24950 Train loss: 0.570894 Train acc: 0.715000\n",
      "Epoch: 83/200 Iteration: 25000 Train loss: 0.589560 Train acc: 0.735000\n",
      "Epoch: 83/200 Iteration: 25000 Validation loss: 0.534032 Validation acc: 0.704300\n",
      "Epoch: 83/200 Iteration: 25050 Train loss: 0.495129 Train acc: 0.745000\n",
      "Epoch: 83/200 Iteration: 25100 Train loss: 0.497482 Train acc: 0.755000\n",
      "Epoch: 83/200 Iteration: 25150 Train loss: 0.411882 Train acc: 0.805000\n",
      "Epoch: 83/200 Iteration: 25200 Train loss: 0.390114 Train acc: 0.815000\n",
      "Epoch: 84/200 Iteration: 25250 Train loss: 0.558508 Train acc: 0.735000\n",
      "Epoch: 84/200 Iteration: 25250 Validation loss: 0.533439 Validation acc: 0.707000\n",
      "Epoch: 84/200 Iteration: 25300 Train loss: 0.609730 Train acc: 0.735000\n",
      "Epoch: 84/200 Iteration: 25350 Train loss: 0.496845 Train acc: 0.765000\n",
      "Epoch: 84/200 Iteration: 25400 Train loss: 0.507050 Train acc: 0.750000\n",
      "Epoch: 84/200 Iteration: 25450 Train loss: 0.416340 Train acc: 0.820000\n",
      "Epoch: 84/200 Iteration: 25500 Train loss: 0.412777 Train acc: 0.810000\n",
      "Epoch: 84/200 Iteration: 25500 Validation loss: 0.534224 Validation acc: 0.703100\n",
      "Epoch: 85/200 Iteration: 25550 Train loss: 0.558083 Train acc: 0.690000\n",
      "Epoch: 85/200 Iteration: 25600 Train loss: 0.556483 Train acc: 0.775000\n",
      "Epoch: 85/200 Iteration: 25650 Train loss: 0.499885 Train acc: 0.765000\n",
      "Epoch: 85/200 Iteration: 25700 Train loss: 0.507614 Train acc: 0.740000\n",
      "Epoch: 85/200 Iteration: 25750 Train loss: 0.419969 Train acc: 0.810000\n",
      "Epoch: 85/200 Iteration: 25750 Validation loss: 0.535827 Validation acc: 0.701000\n",
      "Epoch: 85/200 Iteration: 25800 Train loss: 0.402417 Train acc: 0.810000\n",
      "Epoch: 86/200 Iteration: 25850 Train loss: 0.557229 Train acc: 0.740000\n",
      "Epoch: 86/200 Iteration: 25900 Train loss: 0.598379 Train acc: 0.740000\n",
      "Epoch: 86/200 Iteration: 25950 Train loss: 0.472729 Train acc: 0.770000\n",
      "Epoch: 86/200 Iteration: 26000 Train loss: 0.497062 Train acc: 0.755000\n",
      "Epoch: 86/200 Iteration: 26000 Validation loss: 0.536413 Validation acc: 0.703300\n",
      "Epoch: 86/200 Iteration: 26050 Train loss: 0.406438 Train acc: 0.805000\n",
      "Epoch: 86/200 Iteration: 26100 Train loss: 0.412904 Train acc: 0.815000\n",
      "Epoch: 87/200 Iteration: 26150 Train loss: 0.554619 Train acc: 0.735000\n",
      "Epoch: 87/200 Iteration: 26200 Train loss: 0.551699 Train acc: 0.755000\n",
      "Epoch: 87/200 Iteration: 26250 Train loss: 0.471957 Train acc: 0.755000\n",
      "Epoch: 87/200 Iteration: 26250 Validation loss: 0.535464 Validation acc: 0.702950\n",
      "Epoch: 87/200 Iteration: 26300 Train loss: 0.505878 Train acc: 0.725000\n",
      "Epoch: 87/200 Iteration: 26350 Train loss: 0.390044 Train acc: 0.810000\n",
      "Epoch: 87/200 Iteration: 26400 Train loss: 0.400193 Train acc: 0.795000\n",
      "Epoch: 88/200 Iteration: 26450 Train loss: 0.542779 Train acc: 0.735000\n",
      "Epoch: 88/200 Iteration: 26500 Train loss: 0.590501 Train acc: 0.745000\n",
      "Epoch: 88/200 Iteration: 26500 Validation loss: 0.534145 Validation acc: 0.703400\n",
      "Epoch: 88/200 Iteration: 26550 Train loss: 0.483604 Train acc: 0.755000\n",
      "Epoch: 88/200 Iteration: 26600 Train loss: 0.507766 Train acc: 0.745000\n",
      "Epoch: 88/200 Iteration: 26650 Train loss: 0.411182 Train acc: 0.800000\n",
      "Epoch: 88/200 Iteration: 26700 Train loss: 0.396997 Train acc: 0.805000\n",
      "Epoch: 89/200 Iteration: 26750 Train loss: 0.560915 Train acc: 0.715000\n",
      "Epoch: 89/200 Iteration: 26750 Validation loss: 0.533628 Validation acc: 0.703150\n",
      "Epoch: 89/200 Iteration: 26800 Train loss: 0.545817 Train acc: 0.755000\n",
      "Epoch: 89/200 Iteration: 26850 Train loss: 0.484611 Train acc: 0.780000\n",
      "Epoch: 89/200 Iteration: 26900 Train loss: 0.509792 Train acc: 0.755000\n",
      "Epoch: 89/200 Iteration: 26950 Train loss: 0.406497 Train acc: 0.815000\n",
      "Epoch: 89/200 Iteration: 27000 Train loss: 0.392920 Train acc: 0.820000\n",
      "Epoch: 89/200 Iteration: 27000 Validation loss: 0.534546 Validation acc: 0.701650\n",
      "Epoch: 90/200 Iteration: 27050 Train loss: 0.550558 Train acc: 0.755000\n",
      "Epoch: 90/200 Iteration: 27100 Train loss: 0.566368 Train acc: 0.780000\n",
      "Epoch: 90/200 Iteration: 27150 Train loss: 0.471717 Train acc: 0.775000\n",
      "Epoch: 90/200 Iteration: 27200 Train loss: 0.506150 Train acc: 0.720000\n",
      "Epoch: 90/200 Iteration: 27250 Train loss: 0.415109 Train acc: 0.810000\n",
      "Epoch: 90/200 Iteration: 27250 Validation loss: 0.536091 Validation acc: 0.702050\n",
      "Epoch: 90/200 Iteration: 27300 Train loss: 0.408370 Train acc: 0.810000\n",
      "Epoch: 91/200 Iteration: 27350 Train loss: 0.544352 Train acc: 0.715000\n",
      "Epoch: 91/200 Iteration: 27400 Train loss: 0.566658 Train acc: 0.765000\n",
      "Epoch: 91/200 Iteration: 27450 Train loss: 0.470850 Train acc: 0.775000\n",
      "Epoch: 91/200 Iteration: 27500 Train loss: 0.497924 Train acc: 0.745000\n",
      "Epoch: 91/200 Iteration: 27500 Validation loss: 0.535826 Validation acc: 0.701100\n",
      "Epoch: 91/200 Iteration: 27550 Train loss: 0.388944 Train acc: 0.820000\n",
      "Epoch: 91/200 Iteration: 27600 Train loss: 0.408022 Train acc: 0.785000\n",
      "Epoch: 92/200 Iteration: 27650 Train loss: 0.553785 Train acc: 0.720000\n",
      "Epoch: 92/200 Iteration: 27700 Train loss: 0.545979 Train acc: 0.765000\n",
      "Epoch: 92/200 Iteration: 27750 Train loss: 0.478517 Train acc: 0.780000\n",
      "Epoch: 92/200 Iteration: 27750 Validation loss: 0.534964 Validation acc: 0.703200\n",
      "Epoch: 92/200 Iteration: 27800 Train loss: 0.496158 Train acc: 0.750000\n",
      "Epoch: 92/200 Iteration: 27850 Train loss: 0.409671 Train acc: 0.800000\n",
      "Epoch: 92/200 Iteration: 27900 Train loss: 0.399099 Train acc: 0.815000\n",
      "Epoch: 93/200 Iteration: 27950 Train loss: 0.546664 Train acc: 0.720000\n",
      "Epoch: 93/200 Iteration: 28000 Train loss: 0.543892 Train acc: 0.775000\n",
      "Epoch: 93/200 Iteration: 28000 Validation loss: 0.534536 Validation acc: 0.702900\n",
      "Epoch: 93/200 Iteration: 28050 Train loss: 0.500645 Train acc: 0.755000\n",
      "Epoch: 93/200 Iteration: 28100 Train loss: 0.510586 Train acc: 0.725000\n",
      "Epoch: 93/200 Iteration: 28150 Train loss: 0.421366 Train acc: 0.820000\n",
      "Epoch: 93/200 Iteration: 28200 Train loss: 0.406697 Train acc: 0.815000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/200 Iteration: 28250 Train loss: 0.578331 Train acc: 0.715000\n",
      "Epoch: 94/200 Iteration: 28250 Validation loss: 0.533418 Validation acc: 0.704400\n",
      "Epoch: 94/200 Iteration: 28300 Train loss: 0.555733 Train acc: 0.770000\n",
      "Epoch: 94/200 Iteration: 28350 Train loss: 0.476035 Train acc: 0.750000\n",
      "Epoch: 94/200 Iteration: 28400 Train loss: 0.488918 Train acc: 0.760000\n",
      "Epoch: 94/200 Iteration: 28450 Train loss: 0.413513 Train acc: 0.795000\n",
      "Epoch: 94/200 Iteration: 28500 Train loss: 0.404355 Train acc: 0.805000\n",
      "Epoch: 94/200 Iteration: 28500 Validation loss: 0.535059 Validation acc: 0.702550\n",
      "Epoch: 95/200 Iteration: 28550 Train loss: 0.556974 Train acc: 0.750000\n",
      "Epoch: 95/200 Iteration: 28600 Train loss: 0.559098 Train acc: 0.755000\n",
      "Epoch: 95/200 Iteration: 28650 Train loss: 0.479896 Train acc: 0.795000\n",
      "Epoch: 95/200 Iteration: 28700 Train loss: 0.487374 Train acc: 0.750000\n",
      "Epoch: 95/200 Iteration: 28750 Train loss: 0.400551 Train acc: 0.795000\n",
      "Epoch: 95/200 Iteration: 28750 Validation loss: 0.536405 Validation acc: 0.702650\n",
      "Epoch: 95/200 Iteration: 28800 Train loss: 0.406543 Train acc: 0.815000\n",
      "Epoch: 96/200 Iteration: 28850 Train loss: 0.552730 Train acc: 0.700000\n",
      "Epoch: 96/200 Iteration: 28900 Train loss: 0.567212 Train acc: 0.760000\n",
      "Epoch: 96/200 Iteration: 28950 Train loss: 0.476870 Train acc: 0.750000\n",
      "Epoch: 96/200 Iteration: 29000 Train loss: 0.488862 Train acc: 0.750000\n",
      "Epoch: 96/200 Iteration: 29000 Validation loss: 0.536312 Validation acc: 0.701900\n",
      "Epoch: 96/200 Iteration: 29050 Train loss: 0.423101 Train acc: 0.820000\n",
      "Epoch: 96/200 Iteration: 29100 Train loss: 0.412087 Train acc: 0.795000\n",
      "Epoch: 97/200 Iteration: 29150 Train loss: 0.553415 Train acc: 0.705000\n",
      "Epoch: 97/200 Iteration: 29200 Train loss: 0.555348 Train acc: 0.760000\n",
      "Epoch: 97/200 Iteration: 29250 Train loss: 0.479969 Train acc: 0.765000\n",
      "Epoch: 97/200 Iteration: 29250 Validation loss: 0.535213 Validation acc: 0.701550\n",
      "Epoch: 97/200 Iteration: 29300 Train loss: 0.530927 Train acc: 0.725000\n",
      "Epoch: 97/200 Iteration: 29350 Train loss: 0.391222 Train acc: 0.825000\n",
      "Epoch: 97/200 Iteration: 29400 Train loss: 0.388040 Train acc: 0.820000\n",
      "Epoch: 98/200 Iteration: 29450 Train loss: 0.556367 Train acc: 0.690000\n",
      "Epoch: 98/200 Iteration: 29500 Train loss: 0.562620 Train acc: 0.755000\n",
      "Epoch: 98/200 Iteration: 29500 Validation loss: 0.534662 Validation acc: 0.703750\n",
      "Epoch: 98/200 Iteration: 29550 Train loss: 0.474100 Train acc: 0.770000\n",
      "Epoch: 98/200 Iteration: 29600 Train loss: 0.517158 Train acc: 0.745000\n",
      "Epoch: 98/200 Iteration: 29650 Train loss: 0.396193 Train acc: 0.810000\n",
      "Epoch: 98/200 Iteration: 29700 Train loss: 0.399456 Train acc: 0.800000\n",
      "Epoch: 99/200 Iteration: 29750 Train loss: 0.554483 Train acc: 0.720000\n",
      "Epoch: 99/200 Iteration: 29750 Validation loss: 0.533783 Validation acc: 0.704250\n",
      "Epoch: 99/200 Iteration: 29800 Train loss: 0.557710 Train acc: 0.765000\n",
      "Epoch: 99/200 Iteration: 29850 Train loss: 0.486251 Train acc: 0.760000\n",
      "Epoch: 99/200 Iteration: 29900 Train loss: 0.523841 Train acc: 0.730000\n",
      "Epoch: 99/200 Iteration: 29950 Train loss: 0.399278 Train acc: 0.805000\n",
      "Epoch: 99/200 Iteration: 30000 Train loss: 0.398677 Train acc: 0.775000\n",
      "Epoch: 99/200 Iteration: 30000 Validation loss: 0.534830 Validation acc: 0.703250\n",
      "Epoch: 100/200 Iteration: 30050 Train loss: 0.547195 Train acc: 0.740000\n",
      "Epoch: 100/200 Iteration: 30100 Train loss: 0.546640 Train acc: 0.765000\n",
      "Epoch: 100/200 Iteration: 30150 Train loss: 0.491009 Train acc: 0.770000\n",
      "Epoch: 100/200 Iteration: 30200 Train loss: 0.503850 Train acc: 0.740000\n",
      "Epoch: 100/200 Iteration: 30250 Train loss: 0.404918 Train acc: 0.820000\n",
      "Epoch: 100/200 Iteration: 30250 Validation loss: 0.536393 Validation acc: 0.701650\n",
      "Epoch: 100/200 Iteration: 30300 Train loss: 0.417339 Train acc: 0.775000\n",
      "Epoch: 101/200 Iteration: 30350 Train loss: 0.565378 Train acc: 0.690000\n",
      "Epoch: 101/200 Iteration: 30400 Train loss: 0.564486 Train acc: 0.765000\n",
      "Epoch: 101/200 Iteration: 30450 Train loss: 0.476550 Train acc: 0.760000\n",
      "Epoch: 101/200 Iteration: 30500 Train loss: 0.516429 Train acc: 0.735000\n",
      "Epoch: 101/200 Iteration: 30500 Validation loss: 0.537042 Validation acc: 0.701100\n",
      "Epoch: 101/200 Iteration: 30550 Train loss: 0.410539 Train acc: 0.785000\n",
      "Epoch: 101/200 Iteration: 30600 Train loss: 0.393185 Train acc: 0.810000\n",
      "Epoch: 102/200 Iteration: 30650 Train loss: 0.559225 Train acc: 0.710000\n",
      "Epoch: 102/200 Iteration: 30700 Train loss: 0.562305 Train acc: 0.765000\n",
      "Epoch: 102/200 Iteration: 30750 Train loss: 0.472015 Train acc: 0.765000\n",
      "Epoch: 102/200 Iteration: 30750 Validation loss: 0.535646 Validation acc: 0.700250\n",
      "Epoch: 102/200 Iteration: 30800 Train loss: 0.486184 Train acc: 0.750000\n",
      "Epoch: 102/200 Iteration: 30850 Train loss: 0.389423 Train acc: 0.825000\n",
      "Epoch: 102/200 Iteration: 30900 Train loss: 0.403163 Train acc: 0.800000\n",
      "Epoch: 103/200 Iteration: 30950 Train loss: 0.572170 Train acc: 0.750000\n",
      "Epoch: 103/200 Iteration: 31000 Train loss: 0.578089 Train acc: 0.730000\n",
      "Epoch: 103/200 Iteration: 31000 Validation loss: 0.534859 Validation acc: 0.702200\n",
      "Epoch: 103/200 Iteration: 31050 Train loss: 0.477110 Train acc: 0.775000\n",
      "Epoch: 103/200 Iteration: 31100 Train loss: 0.492688 Train acc: 0.755000\n",
      "Epoch: 103/200 Iteration: 31150 Train loss: 0.395369 Train acc: 0.810000\n",
      "Epoch: 103/200 Iteration: 31200 Train loss: 0.412315 Train acc: 0.785000\n",
      "Epoch: 104/200 Iteration: 31250 Train loss: 0.564895 Train acc: 0.730000\n",
      "Epoch: 104/200 Iteration: 31250 Validation loss: 0.534011 Validation acc: 0.702650\n",
      "Epoch: 104/200 Iteration: 31300 Train loss: 0.538682 Train acc: 0.800000\n",
      "Epoch: 104/200 Iteration: 31350 Train loss: 0.464863 Train acc: 0.785000\n",
      "Epoch: 104/200 Iteration: 31400 Train loss: 0.524471 Train acc: 0.745000\n",
      "Epoch: 104/200 Iteration: 31450 Train loss: 0.392561 Train acc: 0.805000\n",
      "Epoch: 104/200 Iteration: 31500 Train loss: 0.414885 Train acc: 0.810000\n",
      "Epoch: 104/200 Iteration: 31500 Validation loss: 0.535073 Validation acc: 0.702900\n",
      "Epoch: 105/200 Iteration: 31550 Train loss: 0.551041 Train acc: 0.710000\n",
      "Epoch: 105/200 Iteration: 31600 Train loss: 0.547133 Train acc: 0.805000\n",
      "Epoch: 105/200 Iteration: 31650 Train loss: 0.483318 Train acc: 0.790000\n",
      "Epoch: 105/200 Iteration: 31700 Train loss: 0.494679 Train acc: 0.745000\n",
      "Epoch: 105/200 Iteration: 31750 Train loss: 0.385661 Train acc: 0.790000\n",
      "Epoch: 105/200 Iteration: 31750 Validation loss: 0.536739 Validation acc: 0.701700\n",
      "Epoch: 105/200 Iteration: 31800 Train loss: 0.365962 Train acc: 0.855000\n",
      "Epoch: 106/200 Iteration: 31850 Train loss: 0.558977 Train acc: 0.725000\n",
      "Epoch: 106/200 Iteration: 31900 Train loss: 0.557924 Train acc: 0.785000\n",
      "Epoch: 106/200 Iteration: 31950 Train loss: 0.481063 Train acc: 0.755000\n",
      "Epoch: 106/200 Iteration: 32000 Train loss: 0.474166 Train acc: 0.750000\n",
      "Epoch: 106/200 Iteration: 32000 Validation loss: 0.537038 Validation acc: 0.702150\n",
      "Epoch: 106/200 Iteration: 32050 Train loss: 0.391654 Train acc: 0.815000\n",
      "Epoch: 106/200 Iteration: 32100 Train loss: 0.404801 Train acc: 0.790000\n",
      "Epoch: 107/200 Iteration: 32150 Train loss: 0.557204 Train acc: 0.720000\n",
      "Epoch: 107/200 Iteration: 32200 Train loss: 0.561348 Train acc: 0.765000\n",
      "Epoch: 107/200 Iteration: 32250 Train loss: 0.487207 Train acc: 0.770000\n",
      "Epoch: 107/200 Iteration: 32250 Validation loss: 0.535755 Validation acc: 0.700950\n",
      "Epoch: 107/200 Iteration: 32300 Train loss: 0.515834 Train acc: 0.745000\n",
      "Epoch: 107/200 Iteration: 32350 Train loss: 0.391438 Train acc: 0.820000\n",
      "Epoch: 107/200 Iteration: 32400 Train loss: 0.391569 Train acc: 0.815000\n",
      "Epoch: 108/200 Iteration: 32450 Train loss: 0.554688 Train acc: 0.725000\n",
      "Epoch: 108/200 Iteration: 32500 Train loss: 0.565877 Train acc: 0.775000\n",
      "Epoch: 108/200 Iteration: 32500 Validation loss: 0.535113 Validation acc: 0.701950\n",
      "Epoch: 108/200 Iteration: 32550 Train loss: 0.465922 Train acc: 0.770000\n",
      "Epoch: 108/200 Iteration: 32600 Train loss: 0.493035 Train acc: 0.740000\n",
      "Epoch: 108/200 Iteration: 32650 Train loss: 0.399258 Train acc: 0.810000\n",
      "Epoch: 108/200 Iteration: 32700 Train loss: 0.390242 Train acc: 0.805000\n",
      "Epoch: 109/200 Iteration: 32750 Train loss: 0.553020 Train acc: 0.720000\n",
      "Epoch: 109/200 Iteration: 32750 Validation loss: 0.534121 Validation acc: 0.702750\n",
      "Epoch: 109/200 Iteration: 32800 Train loss: 0.563879 Train acc: 0.785000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109/200 Iteration: 32850 Train loss: 0.465983 Train acc: 0.760000\n",
      "Epoch: 109/200 Iteration: 32900 Train loss: 0.495324 Train acc: 0.745000\n",
      "Epoch: 109/200 Iteration: 32950 Train loss: 0.405517 Train acc: 0.795000\n",
      "Epoch: 109/200 Iteration: 33000 Train loss: 0.385817 Train acc: 0.825000\n",
      "Epoch: 109/200 Iteration: 33000 Validation loss: 0.535075 Validation acc: 0.701950\n",
      "Epoch: 110/200 Iteration: 33050 Train loss: 0.555921 Train acc: 0.705000\n",
      "Epoch: 110/200 Iteration: 33100 Train loss: 0.568745 Train acc: 0.755000\n",
      "Epoch: 110/200 Iteration: 33150 Train loss: 0.489386 Train acc: 0.765000\n",
      "Epoch: 110/200 Iteration: 33200 Train loss: 0.476015 Train acc: 0.770000\n",
      "Epoch: 110/200 Iteration: 33250 Train loss: 0.405903 Train acc: 0.820000\n",
      "Epoch: 110/200 Iteration: 33250 Validation loss: 0.536635 Validation acc: 0.701300\n",
      "Epoch: 110/200 Iteration: 33300 Train loss: 0.394421 Train acc: 0.815000\n",
      "Epoch: 111/200 Iteration: 33350 Train loss: 0.549973 Train acc: 0.740000\n",
      "Epoch: 111/200 Iteration: 33400 Train loss: 0.548505 Train acc: 0.775000\n",
      "Epoch: 111/200 Iteration: 33450 Train loss: 0.479526 Train acc: 0.765000\n",
      "Epoch: 111/200 Iteration: 33500 Train loss: 0.515697 Train acc: 0.740000\n",
      "Epoch: 111/200 Iteration: 33500 Validation loss: 0.537236 Validation acc: 0.699700\n",
      "Epoch: 111/200 Iteration: 33550 Train loss: 0.397032 Train acc: 0.815000\n",
      "Epoch: 111/200 Iteration: 33600 Train loss: 0.408776 Train acc: 0.790000\n",
      "Epoch: 112/200 Iteration: 33650 Train loss: 0.545873 Train acc: 0.715000\n",
      "Epoch: 112/200 Iteration: 33700 Train loss: 0.559193 Train acc: 0.775000\n",
      "Epoch: 112/200 Iteration: 33750 Train loss: 0.484136 Train acc: 0.755000\n",
      "Epoch: 112/200 Iteration: 33750 Validation loss: 0.536127 Validation acc: 0.699300\n",
      "Epoch: 112/200 Iteration: 33800 Train loss: 0.498627 Train acc: 0.755000\n",
      "Epoch: 112/200 Iteration: 33850 Train loss: 0.396697 Train acc: 0.815000\n",
      "Epoch: 112/200 Iteration: 33900 Train loss: 0.400027 Train acc: 0.810000\n",
      "Epoch: 113/200 Iteration: 33950 Train loss: 0.554346 Train acc: 0.750000\n",
      "Epoch: 113/200 Iteration: 34000 Train loss: 0.548605 Train acc: 0.750000\n",
      "Epoch: 113/200 Iteration: 34000 Validation loss: 0.535235 Validation acc: 0.703300\n",
      "Epoch: 113/200 Iteration: 34050 Train loss: 0.457141 Train acc: 0.785000\n",
      "Epoch: 113/200 Iteration: 34100 Train loss: 0.492379 Train acc: 0.720000\n",
      "Epoch: 113/200 Iteration: 34150 Train loss: 0.393664 Train acc: 0.815000\n",
      "Epoch: 113/200 Iteration: 34200 Train loss: 0.412917 Train acc: 0.805000\n",
      "Epoch: 114/200 Iteration: 34250 Train loss: 0.565145 Train acc: 0.715000\n",
      "Epoch: 114/200 Iteration: 34250 Validation loss: 0.534203 Validation acc: 0.703650\n",
      "Epoch: 114/200 Iteration: 34300 Train loss: 0.573032 Train acc: 0.755000\n",
      "Epoch: 114/200 Iteration: 34350 Train loss: 0.463434 Train acc: 0.785000\n",
      "Epoch: 114/200 Iteration: 34400 Train loss: 0.498354 Train acc: 0.755000\n",
      "Epoch: 114/200 Iteration: 34450 Train loss: 0.383635 Train acc: 0.825000\n",
      "Epoch: 114/200 Iteration: 34500 Train loss: 0.410381 Train acc: 0.790000\n",
      "Epoch: 114/200 Iteration: 34500 Validation loss: 0.535529 Validation acc: 0.702650\n",
      "Epoch: 115/200 Iteration: 34550 Train loss: 0.535855 Train acc: 0.745000\n",
      "Epoch: 115/200 Iteration: 34600 Train loss: 0.552333 Train acc: 0.790000\n",
      "Epoch: 115/200 Iteration: 34650 Train loss: 0.451687 Train acc: 0.780000\n",
      "Epoch: 115/200 Iteration: 34700 Train loss: 0.500252 Train acc: 0.745000\n",
      "Epoch: 115/200 Iteration: 34750 Train loss: 0.381274 Train acc: 0.825000\n",
      "Epoch: 115/200 Iteration: 34750 Validation loss: 0.536844 Validation acc: 0.701550\n",
      "Epoch: 115/200 Iteration: 34800 Train loss: 0.409366 Train acc: 0.815000\n",
      "Epoch: 116/200 Iteration: 34850 Train loss: 0.546552 Train acc: 0.710000\n",
      "Epoch: 116/200 Iteration: 34900 Train loss: 0.546622 Train acc: 0.785000\n",
      "Epoch: 116/200 Iteration: 34950 Train loss: 0.454633 Train acc: 0.795000\n",
      "Epoch: 116/200 Iteration: 35000 Train loss: 0.497003 Train acc: 0.750000\n",
      "Epoch: 116/200 Iteration: 35000 Validation loss: 0.537408 Validation acc: 0.698850\n",
      "Epoch: 116/200 Iteration: 35050 Train loss: 0.385693 Train acc: 0.830000\n",
      "Epoch: 116/200 Iteration: 35100 Train loss: 0.392674 Train acc: 0.815000\n",
      "Epoch: 117/200 Iteration: 35150 Train loss: 0.541526 Train acc: 0.735000\n",
      "Epoch: 117/200 Iteration: 35200 Train loss: 0.529156 Train acc: 0.805000\n",
      "Epoch: 117/200 Iteration: 35250 Train loss: 0.457432 Train acc: 0.755000\n",
      "Epoch: 117/200 Iteration: 35250 Validation loss: 0.536302 Validation acc: 0.701950\n",
      "Epoch: 117/200 Iteration: 35300 Train loss: 0.496739 Train acc: 0.755000\n",
      "Epoch: 117/200 Iteration: 35350 Train loss: 0.386964 Train acc: 0.805000\n",
      "Epoch: 117/200 Iteration: 35400 Train loss: 0.395574 Train acc: 0.820000\n",
      "Epoch: 118/200 Iteration: 35450 Train loss: 0.553256 Train acc: 0.720000\n",
      "Epoch: 118/200 Iteration: 35500 Train loss: 0.553636 Train acc: 0.760000\n",
      "Epoch: 118/200 Iteration: 35500 Validation loss: 0.535234 Validation acc: 0.701650\n",
      "Epoch: 118/200 Iteration: 35550 Train loss: 0.463267 Train acc: 0.775000\n",
      "Epoch: 118/200 Iteration: 35600 Train loss: 0.482998 Train acc: 0.755000\n",
      "Epoch: 118/200 Iteration: 35650 Train loss: 0.395845 Train acc: 0.805000\n",
      "Epoch: 118/200 Iteration: 35700 Train loss: 0.398381 Train acc: 0.775000\n",
      "Epoch: 119/200 Iteration: 35750 Train loss: 0.556988 Train acc: 0.715000\n",
      "Epoch: 119/200 Iteration: 35750 Validation loss: 0.534150 Validation acc: 0.704600\n",
      "Epoch: 119/200 Iteration: 35800 Train loss: 0.522631 Train acc: 0.785000\n",
      "Epoch: 119/200 Iteration: 35850 Train loss: 0.490889 Train acc: 0.750000\n",
      "Epoch: 119/200 Iteration: 35900 Train loss: 0.516808 Train acc: 0.740000\n",
      "Epoch: 119/200 Iteration: 35950 Train loss: 0.396646 Train acc: 0.810000\n",
      "Epoch: 119/200 Iteration: 36000 Train loss: 0.392189 Train acc: 0.805000\n",
      "Epoch: 119/200 Iteration: 36000 Validation loss: 0.535401 Validation acc: 0.700750\n",
      "Epoch: 120/200 Iteration: 36050 Train loss: 0.538510 Train acc: 0.720000\n",
      "Epoch: 120/200 Iteration: 36100 Train loss: 0.519540 Train acc: 0.805000\n",
      "Epoch: 120/200 Iteration: 36150 Train loss: 0.463502 Train acc: 0.780000\n",
      "Epoch: 120/200 Iteration: 36200 Train loss: 0.494347 Train acc: 0.740000\n",
      "Epoch: 120/200 Iteration: 36250 Train loss: 0.379884 Train acc: 0.830000\n",
      "Epoch: 120/200 Iteration: 36250 Validation loss: 0.536891 Validation acc: 0.698850\n",
      "Epoch: 120/200 Iteration: 36300 Train loss: 0.386342 Train acc: 0.815000\n",
      "Epoch: 121/200 Iteration: 36350 Train loss: 0.522457 Train acc: 0.735000\n",
      "Epoch: 121/200 Iteration: 36400 Train loss: 0.534125 Train acc: 0.780000\n",
      "Epoch: 121/200 Iteration: 36450 Train loss: 0.466794 Train acc: 0.785000\n",
      "Epoch: 121/200 Iteration: 36500 Train loss: 0.487203 Train acc: 0.765000\n",
      "Epoch: 121/200 Iteration: 36500 Validation loss: 0.537465 Validation acc: 0.699700\n",
      "Epoch: 121/200 Iteration: 36550 Train loss: 0.381422 Train acc: 0.835000\n",
      "Epoch: 121/200 Iteration: 36600 Train loss: 0.403669 Train acc: 0.825000\n",
      "Epoch: 122/200 Iteration: 36650 Train loss: 0.538264 Train acc: 0.720000\n",
      "Epoch: 122/200 Iteration: 36700 Train loss: 0.531565 Train acc: 0.780000\n",
      "Epoch: 122/200 Iteration: 36750 Train loss: 0.455593 Train acc: 0.800000\n",
      "Epoch: 122/200 Iteration: 36750 Validation loss: 0.537006 Validation acc: 0.699200\n",
      "Epoch: 122/200 Iteration: 36800 Train loss: 0.504703 Train acc: 0.755000\n",
      "Epoch: 122/200 Iteration: 36850 Train loss: 0.388731 Train acc: 0.830000\n",
      "Epoch: 122/200 Iteration: 36900 Train loss: 0.416776 Train acc: 0.795000\n",
      "Epoch: 123/200 Iteration: 36950 Train loss: 0.552925 Train acc: 0.750000\n",
      "Epoch: 123/200 Iteration: 37000 Train loss: 0.540947 Train acc: 0.790000\n",
      "Epoch: 123/200 Iteration: 37000 Validation loss: 0.535860 Validation acc: 0.700800\n",
      "Epoch: 123/200 Iteration: 37050 Train loss: 0.446483 Train acc: 0.785000\n",
      "Epoch: 123/200 Iteration: 37100 Train loss: 0.502476 Train acc: 0.755000\n",
      "Epoch: 123/200 Iteration: 37150 Train loss: 0.386553 Train acc: 0.825000\n",
      "Epoch: 123/200 Iteration: 37200 Train loss: 0.394736 Train acc: 0.810000\n",
      "Epoch: 124/200 Iteration: 37250 Train loss: 0.544725 Train acc: 0.730000\n",
      "Epoch: 124/200 Iteration: 37250 Validation loss: 0.534591 Validation acc: 0.699450\n",
      "Epoch: 124/200 Iteration: 37300 Train loss: 0.581205 Train acc: 0.735000\n",
      "Epoch: 124/200 Iteration: 37350 Train loss: 0.441045 Train acc: 0.800000\n",
      "Epoch: 124/200 Iteration: 37400 Train loss: 0.486310 Train acc: 0.740000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124/200 Iteration: 37450 Train loss: 0.388011 Train acc: 0.805000\n",
      "Epoch: 124/200 Iteration: 37500 Train loss: 0.381697 Train acc: 0.835000\n",
      "Epoch: 124/200 Iteration: 37500 Validation loss: 0.535408 Validation acc: 0.700450\n",
      "Epoch: 125/200 Iteration: 37550 Train loss: 0.564108 Train acc: 0.745000\n",
      "Epoch: 125/200 Iteration: 37600 Train loss: 0.539233 Train acc: 0.785000\n",
      "Epoch: 125/200 Iteration: 37650 Train loss: 0.464078 Train acc: 0.785000\n",
      "Epoch: 125/200 Iteration: 37700 Train loss: 0.502096 Train acc: 0.735000\n",
      "Epoch: 125/200 Iteration: 37750 Train loss: 0.380982 Train acc: 0.820000\n",
      "Epoch: 125/200 Iteration: 37750 Validation loss: 0.536992 Validation acc: 0.700550\n",
      "Epoch: 125/200 Iteration: 37800 Train loss: 0.391196 Train acc: 0.790000\n",
      "Epoch: 126/200 Iteration: 37850 Train loss: 0.547669 Train acc: 0.720000\n",
      "Epoch: 126/200 Iteration: 37900 Train loss: 0.540858 Train acc: 0.770000\n",
      "Epoch: 126/200 Iteration: 37950 Train loss: 0.448224 Train acc: 0.820000\n",
      "Epoch: 126/200 Iteration: 38000 Train loss: 0.488790 Train acc: 0.745000\n",
      "Epoch: 126/200 Iteration: 38000 Validation loss: 0.536921 Validation acc: 0.700600\n",
      "Epoch: 126/200 Iteration: 38050 Train loss: 0.396656 Train acc: 0.800000\n",
      "Epoch: 126/200 Iteration: 38100 Train loss: 0.389950 Train acc: 0.800000\n",
      "Epoch: 127/200 Iteration: 38150 Train loss: 0.554071 Train acc: 0.730000\n",
      "Epoch: 127/200 Iteration: 38200 Train loss: 0.532776 Train acc: 0.780000\n",
      "Epoch: 127/200 Iteration: 38250 Train loss: 0.445416 Train acc: 0.790000\n",
      "Epoch: 127/200 Iteration: 38250 Validation loss: 0.536192 Validation acc: 0.700100\n",
      "Epoch: 127/200 Iteration: 38300 Train loss: 0.475210 Train acc: 0.765000\n",
      "Epoch: 127/200 Iteration: 38350 Train loss: 0.376575 Train acc: 0.830000\n",
      "Epoch: 127/200 Iteration: 38400 Train loss: 0.399678 Train acc: 0.810000\n",
      "Epoch: 128/200 Iteration: 38450 Train loss: 0.551810 Train acc: 0.720000\n",
      "Epoch: 128/200 Iteration: 38500 Train loss: 0.530057 Train acc: 0.805000\n",
      "Epoch: 128/200 Iteration: 38500 Validation loss: 0.535583 Validation acc: 0.700950\n",
      "Epoch: 128/200 Iteration: 38550 Train loss: 0.444197 Train acc: 0.790000\n",
      "Epoch: 128/200 Iteration: 38600 Train loss: 0.500341 Train acc: 0.735000\n",
      "Epoch: 128/200 Iteration: 38650 Train loss: 0.371088 Train acc: 0.835000\n",
      "Epoch: 128/200 Iteration: 38700 Train loss: 0.374984 Train acc: 0.830000\n",
      "Epoch: 129/200 Iteration: 38750 Train loss: 0.522668 Train acc: 0.725000\n",
      "Epoch: 129/200 Iteration: 38750 Validation loss: 0.534786 Validation acc: 0.699900\n",
      "Epoch: 129/200 Iteration: 38800 Train loss: 0.526765 Train acc: 0.795000\n",
      "Epoch: 129/200 Iteration: 38850 Train loss: 0.443804 Train acc: 0.810000\n",
      "Epoch: 129/200 Iteration: 38900 Train loss: 0.487765 Train acc: 0.760000\n",
      "Epoch: 129/200 Iteration: 38950 Train loss: 0.385357 Train acc: 0.820000\n",
      "Epoch: 129/200 Iteration: 39000 Train loss: 0.379098 Train acc: 0.820000\n",
      "Epoch: 129/200 Iteration: 39000 Validation loss: 0.536121 Validation acc: 0.699700\n",
      "Epoch: 130/200 Iteration: 39050 Train loss: 0.537637 Train acc: 0.730000\n",
      "Epoch: 130/200 Iteration: 39100 Train loss: 0.510660 Train acc: 0.785000\n",
      "Epoch: 130/200 Iteration: 39150 Train loss: 0.469264 Train acc: 0.770000\n",
      "Epoch: 130/200 Iteration: 39200 Train loss: 0.488210 Train acc: 0.760000\n",
      "Epoch: 130/200 Iteration: 39250 Train loss: 0.383644 Train acc: 0.820000\n",
      "Epoch: 130/200 Iteration: 39250 Validation loss: 0.537039 Validation acc: 0.700500\n",
      "Epoch: 130/200 Iteration: 39300 Train loss: 0.379115 Train acc: 0.835000\n",
      "Epoch: 131/200 Iteration: 39350 Train loss: 0.558997 Train acc: 0.725000\n",
      "Epoch: 131/200 Iteration: 39400 Train loss: 0.565626 Train acc: 0.755000\n",
      "Epoch: 131/200 Iteration: 39450 Train loss: 0.460314 Train acc: 0.760000\n",
      "Epoch: 131/200 Iteration: 39500 Train loss: 0.525772 Train acc: 0.725000\n",
      "Epoch: 131/200 Iteration: 39500 Validation loss: 0.537811 Validation acc: 0.700450\n",
      "Epoch: 131/200 Iteration: 39550 Train loss: 0.386936 Train acc: 0.825000\n",
      "Epoch: 131/200 Iteration: 39600 Train loss: 0.381014 Train acc: 0.805000\n",
      "Epoch: 132/200 Iteration: 39650 Train loss: 0.540124 Train acc: 0.745000\n",
      "Epoch: 132/200 Iteration: 39700 Train loss: 0.539629 Train acc: 0.770000\n",
      "Epoch: 132/200 Iteration: 39750 Train loss: 0.461844 Train acc: 0.805000\n",
      "Epoch: 132/200 Iteration: 39750 Validation loss: 0.537126 Validation acc: 0.697850\n",
      "Epoch: 132/200 Iteration: 39800 Train loss: 0.494559 Train acc: 0.755000\n",
      "Epoch: 132/200 Iteration: 39850 Train loss: 0.396480 Train acc: 0.835000\n",
      "Epoch: 132/200 Iteration: 39900 Train loss: 0.389342 Train acc: 0.805000\n",
      "Epoch: 133/200 Iteration: 39950 Train loss: 0.535967 Train acc: 0.695000\n",
      "Epoch: 133/200 Iteration: 40000 Train loss: 0.535100 Train acc: 0.780000\n",
      "Epoch: 133/200 Iteration: 40000 Validation loss: 0.536385 Validation acc: 0.702200\n",
      "Epoch: 133/200 Iteration: 40050 Train loss: 0.465678 Train acc: 0.775000\n",
      "Epoch: 133/200 Iteration: 40100 Train loss: 0.509729 Train acc: 0.725000\n",
      "Epoch: 133/200 Iteration: 40150 Train loss: 0.384321 Train acc: 0.820000\n",
      "Epoch: 133/200 Iteration: 40200 Train loss: 0.374303 Train acc: 0.825000\n",
      "Epoch: 134/200 Iteration: 40250 Train loss: 0.545076 Train acc: 0.725000\n",
      "Epoch: 134/200 Iteration: 40250 Validation loss: 0.535233 Validation acc: 0.697850\n",
      "Epoch: 134/200 Iteration: 40300 Train loss: 0.538635 Train acc: 0.780000\n",
      "Epoch: 134/200 Iteration: 40350 Train loss: 0.457982 Train acc: 0.800000\n",
      "Epoch: 134/200 Iteration: 40400 Train loss: 0.517003 Train acc: 0.745000\n",
      "Epoch: 134/200 Iteration: 40450 Train loss: 0.376226 Train acc: 0.845000\n",
      "Epoch: 134/200 Iteration: 40500 Train loss: 0.376231 Train acc: 0.835000\n",
      "Epoch: 134/200 Iteration: 40500 Validation loss: 0.536343 Validation acc: 0.695950\n",
      "Epoch: 135/200 Iteration: 40550 Train loss: 0.543367 Train acc: 0.740000\n",
      "Epoch: 135/200 Iteration: 40600 Train loss: 0.534545 Train acc: 0.785000\n",
      "Epoch: 135/200 Iteration: 40650 Train loss: 0.457511 Train acc: 0.770000\n",
      "Epoch: 135/200 Iteration: 40700 Train loss: 0.499463 Train acc: 0.750000\n",
      "Epoch: 135/200 Iteration: 40750 Train loss: 0.389161 Train acc: 0.810000\n",
      "Epoch: 135/200 Iteration: 40750 Validation loss: 0.537068 Validation acc: 0.699950\n",
      "Epoch: 135/200 Iteration: 40800 Train loss: 0.394142 Train acc: 0.755000\n",
      "Epoch: 136/200 Iteration: 40850 Train loss: 0.528849 Train acc: 0.725000\n",
      "Epoch: 136/200 Iteration: 40900 Train loss: 0.546287 Train acc: 0.785000\n",
      "Epoch: 136/200 Iteration: 40950 Train loss: 0.450704 Train acc: 0.795000\n",
      "Epoch: 136/200 Iteration: 41000 Train loss: 0.497838 Train acc: 0.750000\n",
      "Epoch: 136/200 Iteration: 41000 Validation loss: 0.537673 Validation acc: 0.693750\n",
      "Epoch: 136/200 Iteration: 41050 Train loss: 0.390151 Train acc: 0.820000\n",
      "Epoch: 136/200 Iteration: 41100 Train loss: 0.381950 Train acc: 0.785000\n",
      "Epoch: 137/200 Iteration: 41150 Train loss: 0.536503 Train acc: 0.760000\n",
      "Epoch: 137/200 Iteration: 41200 Train loss: 0.544317 Train acc: 0.765000\n",
      "Epoch: 137/200 Iteration: 41250 Train loss: 0.462832 Train acc: 0.770000\n",
      "Epoch: 137/200 Iteration: 41250 Validation loss: 0.537270 Validation acc: 0.696300\n",
      "Epoch: 137/200 Iteration: 41300 Train loss: 0.491477 Train acc: 0.740000\n",
      "Epoch: 137/200 Iteration: 41350 Train loss: 0.387579 Train acc: 0.805000\n",
      "Epoch: 137/200 Iteration: 41400 Train loss: 0.407565 Train acc: 0.800000\n",
      "Epoch: 138/200 Iteration: 41450 Train loss: 0.543835 Train acc: 0.745000\n",
      "Epoch: 138/200 Iteration: 41500 Train loss: 0.543389 Train acc: 0.785000\n",
      "Epoch: 138/200 Iteration: 41500 Validation loss: 0.536566 Validation acc: 0.695250\n",
      "Epoch: 138/200 Iteration: 41550 Train loss: 0.431332 Train acc: 0.785000\n",
      "Epoch: 138/200 Iteration: 41600 Train loss: 0.494017 Train acc: 0.760000\n",
      "Epoch: 138/200 Iteration: 41650 Train loss: 0.381880 Train acc: 0.840000\n",
      "Epoch: 138/200 Iteration: 41700 Train loss: 0.396217 Train acc: 0.815000\n",
      "Epoch: 139/200 Iteration: 41750 Train loss: 0.540155 Train acc: 0.730000\n",
      "Epoch: 139/200 Iteration: 41750 Validation loss: 0.535707 Validation acc: 0.694250\n",
      "Epoch: 139/200 Iteration: 41800 Train loss: 0.553418 Train acc: 0.805000\n",
      "Epoch: 139/200 Iteration: 41850 Train loss: 0.448279 Train acc: 0.800000\n",
      "Epoch: 139/200 Iteration: 41900 Train loss: 0.479999 Train acc: 0.755000\n",
      "Epoch: 139/200 Iteration: 41950 Train loss: 0.370617 Train acc: 0.830000\n",
      "Epoch: 139/200 Iteration: 42000 Train loss: 0.367569 Train acc: 0.825000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139/200 Iteration: 42000 Validation loss: 0.536324 Validation acc: 0.694150\n",
      "Epoch: 140/200 Iteration: 42050 Train loss: 0.558986 Train acc: 0.710000\n",
      "Epoch: 140/200 Iteration: 42100 Train loss: 0.552399 Train acc: 0.795000\n",
      "Epoch: 140/200 Iteration: 42150 Train loss: 0.451514 Train acc: 0.785000\n",
      "Epoch: 140/200 Iteration: 42200 Train loss: 0.513354 Train acc: 0.745000\n",
      "Epoch: 140/200 Iteration: 42250 Train loss: 0.370253 Train acc: 0.820000\n",
      "Epoch: 140/200 Iteration: 42250 Validation loss: 0.537117 Validation acc: 0.699050\n",
      "Epoch: 140/200 Iteration: 42300 Train loss: 0.389176 Train acc: 0.825000\n",
      "Epoch: 141/200 Iteration: 42350 Train loss: 0.544500 Train acc: 0.740000\n",
      "Epoch: 141/200 Iteration: 42400 Train loss: 0.531994 Train acc: 0.775000\n",
      "Epoch: 141/200 Iteration: 42450 Train loss: 0.459253 Train acc: 0.765000\n",
      "Epoch: 141/200 Iteration: 42500 Train loss: 0.499566 Train acc: 0.750000\n",
      "Epoch: 141/200 Iteration: 42500 Validation loss: 0.538149 Validation acc: 0.695700\n",
      "Epoch: 141/200 Iteration: 42550 Train loss: 0.374338 Train acc: 0.810000\n",
      "Epoch: 141/200 Iteration: 42600 Train loss: 0.393961 Train acc: 0.830000\n",
      "Epoch: 142/200 Iteration: 42650 Train loss: 0.534851 Train acc: 0.720000\n",
      "Epoch: 142/200 Iteration: 42700 Train loss: 0.525471 Train acc: 0.790000\n",
      "Epoch: 142/200 Iteration: 42750 Train loss: 0.449194 Train acc: 0.785000\n",
      "Epoch: 142/200 Iteration: 42750 Validation loss: 0.538175 Validation acc: 0.693500\n",
      "Epoch: 142/200 Iteration: 42800 Train loss: 0.505438 Train acc: 0.765000\n",
      "Epoch: 142/200 Iteration: 42850 Train loss: 0.394776 Train acc: 0.825000\n",
      "Epoch: 142/200 Iteration: 42900 Train loss: 0.379368 Train acc: 0.830000\n",
      "Epoch: 143/200 Iteration: 42950 Train loss: 0.537971 Train acc: 0.715000\n",
      "Epoch: 143/200 Iteration: 43000 Train loss: 0.517672 Train acc: 0.800000\n",
      "Epoch: 143/200 Iteration: 43000 Validation loss: 0.537542 Validation acc: 0.693100\n",
      "Epoch: 143/200 Iteration: 43050 Train loss: 0.465338 Train acc: 0.785000\n",
      "Epoch: 143/200 Iteration: 43100 Train loss: 0.500433 Train acc: 0.755000\n",
      "Epoch: 143/200 Iteration: 43150 Train loss: 0.400596 Train acc: 0.820000\n",
      "Epoch: 143/200 Iteration: 43200 Train loss: 0.389292 Train acc: 0.830000\n",
      "Epoch: 144/200 Iteration: 43250 Train loss: 0.532679 Train acc: 0.730000\n",
      "Epoch: 144/200 Iteration: 43250 Validation loss: 0.536013 Validation acc: 0.696550\n",
      "Epoch: 144/200 Iteration: 43300 Train loss: 0.541376 Train acc: 0.795000\n",
      "Epoch: 144/200 Iteration: 43350 Train loss: 0.470688 Train acc: 0.785000\n",
      "Epoch: 144/200 Iteration: 43400 Train loss: 0.501179 Train acc: 0.730000\n",
      "Epoch: 144/200 Iteration: 43450 Train loss: 0.382486 Train acc: 0.840000\n",
      "Epoch: 144/200 Iteration: 43500 Train loss: 0.392920 Train acc: 0.825000\n",
      "Epoch: 144/200 Iteration: 43500 Validation loss: 0.536850 Validation acc: 0.693850\n",
      "Epoch: 145/200 Iteration: 43550 Train loss: 0.531578 Train acc: 0.755000\n",
      "Epoch: 145/200 Iteration: 43600 Train loss: 0.524813 Train acc: 0.785000\n",
      "Epoch: 145/200 Iteration: 43650 Train loss: 0.432748 Train acc: 0.800000\n",
      "Epoch: 145/200 Iteration: 43700 Train loss: 0.512576 Train acc: 0.745000\n",
      "Epoch: 145/200 Iteration: 43750 Train loss: 0.403133 Train acc: 0.805000\n",
      "Epoch: 145/200 Iteration: 43750 Validation loss: 0.537724 Validation acc: 0.699900\n",
      "Epoch: 145/200 Iteration: 43800 Train loss: 0.381030 Train acc: 0.805000\n",
      "Epoch: 146/200 Iteration: 43850 Train loss: 0.537709 Train acc: 0.730000\n",
      "Epoch: 146/200 Iteration: 43900 Train loss: 0.517877 Train acc: 0.770000\n",
      "Epoch: 146/200 Iteration: 43950 Train loss: 0.436458 Train acc: 0.795000\n",
      "Epoch: 146/200 Iteration: 44000 Train loss: 0.488965 Train acc: 0.765000\n",
      "Epoch: 146/200 Iteration: 44000 Validation loss: 0.538067 Validation acc: 0.695400\n",
      "Epoch: 146/200 Iteration: 44050 Train loss: 0.380036 Train acc: 0.820000\n",
      "Epoch: 146/200 Iteration: 44100 Train loss: 0.391234 Train acc: 0.800000\n",
      "Epoch: 147/200 Iteration: 44150 Train loss: 0.524756 Train acc: 0.755000\n",
      "Epoch: 147/200 Iteration: 44200 Train loss: 0.524738 Train acc: 0.780000\n",
      "Epoch: 147/200 Iteration: 44250 Train loss: 0.451859 Train acc: 0.790000\n",
      "Epoch: 147/200 Iteration: 44250 Validation loss: 0.537850 Validation acc: 0.694850\n",
      "Epoch: 147/200 Iteration: 44300 Train loss: 0.501937 Train acc: 0.755000\n",
      "Epoch: 147/200 Iteration: 44350 Train loss: 0.369361 Train acc: 0.835000\n",
      "Epoch: 147/200 Iteration: 44400 Train loss: 0.379727 Train acc: 0.805000\n",
      "Epoch: 148/200 Iteration: 44450 Train loss: 0.553369 Train acc: 0.755000\n",
      "Epoch: 148/200 Iteration: 44500 Train loss: 0.534089 Train acc: 0.805000\n",
      "Epoch: 148/200 Iteration: 44500 Validation loss: 0.537431 Validation acc: 0.695600\n",
      "Epoch: 148/200 Iteration: 44550 Train loss: 0.443153 Train acc: 0.775000\n",
      "Epoch: 148/200 Iteration: 44600 Train loss: 0.498218 Train acc: 0.760000\n",
      "Epoch: 148/200 Iteration: 44650 Train loss: 0.375890 Train acc: 0.815000\n",
      "Epoch: 148/200 Iteration: 44700 Train loss: 0.390110 Train acc: 0.820000\n",
      "Epoch: 149/200 Iteration: 44750 Train loss: 0.556420 Train acc: 0.720000\n",
      "Epoch: 149/200 Iteration: 44750 Validation loss: 0.535994 Validation acc: 0.694000\n",
      "Epoch: 149/200 Iteration: 44800 Train loss: 0.523740 Train acc: 0.790000\n",
      "Epoch: 149/200 Iteration: 44850 Train loss: 0.451578 Train acc: 0.795000\n",
      "Epoch: 149/200 Iteration: 44900 Train loss: 0.480667 Train acc: 0.745000\n",
      "Epoch: 149/200 Iteration: 44950 Train loss: 0.377656 Train acc: 0.830000\n",
      "Epoch: 149/200 Iteration: 45000 Train loss: 0.385740 Train acc: 0.810000\n",
      "Epoch: 149/200 Iteration: 45000 Validation loss: 0.536964 Validation acc: 0.694450\n",
      "Epoch: 150/200 Iteration: 45050 Train loss: 0.525277 Train acc: 0.755000\n",
      "Epoch: 150/200 Iteration: 45100 Train loss: 0.512463 Train acc: 0.805000\n",
      "Epoch: 150/200 Iteration: 45150 Train loss: 0.434924 Train acc: 0.795000\n",
      "Epoch: 150/200 Iteration: 45200 Train loss: 0.485517 Train acc: 0.770000\n",
      "Epoch: 150/200 Iteration: 45250 Train loss: 0.387386 Train acc: 0.800000\n",
      "Epoch: 150/200 Iteration: 45250 Validation loss: 0.537990 Validation acc: 0.696100\n",
      "Epoch: 150/200 Iteration: 45300 Train loss: 0.393619 Train acc: 0.830000\n",
      "Epoch: 151/200 Iteration: 45350 Train loss: 0.551693 Train acc: 0.695000\n",
      "Epoch: 151/200 Iteration: 45400 Train loss: 0.543661 Train acc: 0.790000\n",
      "Epoch: 151/200 Iteration: 45450 Train loss: 0.441859 Train acc: 0.810000\n",
      "Epoch: 151/200 Iteration: 45500 Train loss: 0.464706 Train acc: 0.770000\n",
      "Epoch: 151/200 Iteration: 45500 Validation loss: 0.538475 Validation acc: 0.695100\n",
      "Epoch: 151/200 Iteration: 45550 Train loss: 0.366059 Train acc: 0.840000\n",
      "Epoch: 151/200 Iteration: 45600 Train loss: 0.399901 Train acc: 0.845000\n",
      "Epoch: 152/200 Iteration: 45650 Train loss: 0.535737 Train acc: 0.750000\n",
      "Epoch: 152/200 Iteration: 45700 Train loss: 0.516250 Train acc: 0.810000\n",
      "Epoch: 152/200 Iteration: 45750 Train loss: 0.453324 Train acc: 0.785000\n",
      "Epoch: 152/200 Iteration: 45750 Validation loss: 0.538202 Validation acc: 0.694300\n",
      "Epoch: 152/200 Iteration: 45800 Train loss: 0.476577 Train acc: 0.765000\n",
      "Epoch: 152/200 Iteration: 45850 Train loss: 0.366631 Train acc: 0.815000\n",
      "Epoch: 152/200 Iteration: 45900 Train loss: 0.398344 Train acc: 0.825000\n",
      "Epoch: 153/200 Iteration: 45950 Train loss: 0.563408 Train acc: 0.715000\n",
      "Epoch: 153/200 Iteration: 46000 Train loss: 0.523183 Train acc: 0.785000\n",
      "Epoch: 153/200 Iteration: 46000 Validation loss: 0.537546 Validation acc: 0.693500\n",
      "Epoch: 153/200 Iteration: 46050 Train loss: 0.444769 Train acc: 0.800000\n",
      "Epoch: 153/200 Iteration: 46100 Train loss: 0.491584 Train acc: 0.735000\n",
      "Epoch: 153/200 Iteration: 46150 Train loss: 0.372321 Train acc: 0.825000\n",
      "Epoch: 153/200 Iteration: 46200 Train loss: 0.383559 Train acc: 0.805000\n",
      "Epoch: 154/200 Iteration: 46250 Train loss: 0.537467 Train acc: 0.730000\n",
      "Epoch: 154/200 Iteration: 46250 Validation loss: 0.536381 Validation acc: 0.693750\n",
      "Epoch: 154/200 Iteration: 46300 Train loss: 0.494380 Train acc: 0.810000\n",
      "Epoch: 154/200 Iteration: 46350 Train loss: 0.447390 Train acc: 0.760000\n",
      "Epoch: 154/200 Iteration: 46400 Train loss: 0.488772 Train acc: 0.775000\n",
      "Epoch: 154/200 Iteration: 46450 Train loss: 0.372258 Train acc: 0.815000\n",
      "Epoch: 154/200 Iteration: 46500 Train loss: 0.390946 Train acc: 0.790000\n",
      "Epoch: 154/200 Iteration: 46500 Validation loss: 0.537063 Validation acc: 0.692300\n",
      "Epoch: 155/200 Iteration: 46550 Train loss: 0.542411 Train acc: 0.745000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/200 Iteration: 46600 Train loss: 0.534971 Train acc: 0.775000\n",
      "Epoch: 155/200 Iteration: 46650 Train loss: 0.439462 Train acc: 0.780000\n",
      "Epoch: 155/200 Iteration: 46700 Train loss: 0.474564 Train acc: 0.770000\n",
      "Epoch: 155/200 Iteration: 46750 Train loss: 0.357862 Train acc: 0.820000\n",
      "Epoch: 155/200 Iteration: 46750 Validation loss: 0.537854 Validation acc: 0.694050\n",
      "Epoch: 155/200 Iteration: 46800 Train loss: 0.379197 Train acc: 0.835000\n",
      "Epoch: 156/200 Iteration: 46850 Train loss: 0.532147 Train acc: 0.725000\n",
      "Epoch: 156/200 Iteration: 46900 Train loss: 0.508063 Train acc: 0.805000\n",
      "Epoch: 156/200 Iteration: 46950 Train loss: 0.431221 Train acc: 0.795000\n",
      "Epoch: 156/200 Iteration: 47000 Train loss: 0.478650 Train acc: 0.745000\n",
      "Epoch: 156/200 Iteration: 47000 Validation loss: 0.538786 Validation acc: 0.694700\n",
      "Epoch: 156/200 Iteration: 47050 Train loss: 0.371435 Train acc: 0.825000\n",
      "Epoch: 156/200 Iteration: 47100 Train loss: 0.383542 Train acc: 0.830000\n",
      "Epoch: 157/200 Iteration: 47150 Train loss: 0.531873 Train acc: 0.735000\n",
      "Epoch: 157/200 Iteration: 47200 Train loss: 0.519840 Train acc: 0.790000\n",
      "Epoch: 157/200 Iteration: 47250 Train loss: 0.447149 Train acc: 0.780000\n",
      "Epoch: 157/200 Iteration: 47250 Validation loss: 0.538814 Validation acc: 0.691900\n",
      "Epoch: 157/200 Iteration: 47300 Train loss: 0.487782 Train acc: 0.750000\n",
      "Epoch: 157/200 Iteration: 47350 Train loss: 0.362863 Train acc: 0.830000\n",
      "Epoch: 157/200 Iteration: 47400 Train loss: 0.385811 Train acc: 0.795000\n",
      "Epoch: 158/200 Iteration: 47450 Train loss: 0.554431 Train acc: 0.705000\n",
      "Epoch: 158/200 Iteration: 47500 Train loss: 0.519393 Train acc: 0.790000\n",
      "Epoch: 158/200 Iteration: 47500 Validation loss: 0.538773 Validation acc: 0.695850\n",
      "Epoch: 158/200 Iteration: 47550 Train loss: 0.423758 Train acc: 0.800000\n",
      "Epoch: 158/200 Iteration: 47600 Train loss: 0.501525 Train acc: 0.745000\n",
      "Epoch: 158/200 Iteration: 47650 Train loss: 0.367648 Train acc: 0.825000\n",
      "Epoch: 158/200 Iteration: 47700 Train loss: 0.404502 Train acc: 0.800000\n",
      "Epoch: 159/200 Iteration: 47750 Train loss: 0.545657 Train acc: 0.760000\n",
      "Epoch: 159/200 Iteration: 47750 Validation loss: 0.537066 Validation acc: 0.694350\n",
      "Epoch: 159/200 Iteration: 47800 Train loss: 0.520865 Train acc: 0.775000\n",
      "Epoch: 159/200 Iteration: 47850 Train loss: 0.429279 Train acc: 0.790000\n",
      "Epoch: 159/200 Iteration: 47900 Train loss: 0.489491 Train acc: 0.760000\n",
      "Epoch: 159/200 Iteration: 47950 Train loss: 0.352765 Train acc: 0.850000\n",
      "Epoch: 159/200 Iteration: 48000 Train loss: 0.403183 Train acc: 0.835000\n",
      "Epoch: 159/200 Iteration: 48000 Validation loss: 0.537990 Validation acc: 0.693200\n",
      "Epoch: 160/200 Iteration: 48050 Train loss: 0.536654 Train acc: 0.705000\n",
      "Epoch: 160/200 Iteration: 48100 Train loss: 0.508995 Train acc: 0.805000\n",
      "Epoch: 160/200 Iteration: 48150 Train loss: 0.451144 Train acc: 0.780000\n",
      "Epoch: 160/200 Iteration: 48200 Train loss: 0.497750 Train acc: 0.740000\n",
      "Epoch: 160/200 Iteration: 48250 Train loss: 0.366365 Train acc: 0.815000\n",
      "Epoch: 160/200 Iteration: 48250 Validation loss: 0.538353 Validation acc: 0.693800\n",
      "Epoch: 160/200 Iteration: 48300 Train loss: 0.371653 Train acc: 0.815000\n",
      "Epoch: 161/200 Iteration: 48350 Train loss: 0.531495 Train acc: 0.740000\n",
      "Epoch: 161/200 Iteration: 48400 Train loss: 0.520327 Train acc: 0.775000\n",
      "Epoch: 161/200 Iteration: 48450 Train loss: 0.424505 Train acc: 0.790000\n",
      "Epoch: 161/200 Iteration: 48500 Train loss: 0.490393 Train acc: 0.765000\n",
      "Epoch: 161/200 Iteration: 48500 Validation loss: 0.539138 Validation acc: 0.693350\n",
      "Epoch: 161/200 Iteration: 48550 Train loss: 0.352158 Train acc: 0.840000\n",
      "Epoch: 161/200 Iteration: 48600 Train loss: 0.407485 Train acc: 0.805000\n",
      "Epoch: 162/200 Iteration: 48650 Train loss: 0.533575 Train acc: 0.730000\n",
      "Epoch: 162/200 Iteration: 48700 Train loss: 0.517354 Train acc: 0.775000\n",
      "Epoch: 162/200 Iteration: 48750 Train loss: 0.446850 Train acc: 0.790000\n",
      "Epoch: 162/200 Iteration: 48750 Validation loss: 0.538845 Validation acc: 0.691900\n",
      "Epoch: 162/200 Iteration: 48800 Train loss: 0.492033 Train acc: 0.760000\n",
      "Epoch: 162/200 Iteration: 48850 Train loss: 0.373757 Train acc: 0.840000\n",
      "Epoch: 162/200 Iteration: 48900 Train loss: 0.382798 Train acc: 0.830000\n",
      "Epoch: 163/200 Iteration: 48950 Train loss: 0.549568 Train acc: 0.710000\n",
      "Epoch: 163/200 Iteration: 49000 Train loss: 0.531415 Train acc: 0.815000\n",
      "Epoch: 163/200 Iteration: 49000 Validation loss: 0.538093 Validation acc: 0.695200\n",
      "Epoch: 163/200 Iteration: 49050 Train loss: 0.457217 Train acc: 0.760000\n",
      "Epoch: 163/200 Iteration: 49100 Train loss: 0.492627 Train acc: 0.750000\n",
      "Epoch: 163/200 Iteration: 49150 Train loss: 0.365493 Train acc: 0.835000\n",
      "Epoch: 163/200 Iteration: 49200 Train loss: 0.394413 Train acc: 0.835000\n",
      "Epoch: 164/200 Iteration: 49250 Train loss: 0.543560 Train acc: 0.720000\n",
      "Epoch: 164/200 Iteration: 49250 Validation loss: 0.537268 Validation acc: 0.694050\n",
      "Epoch: 164/200 Iteration: 49300 Train loss: 0.528219 Train acc: 0.805000\n",
      "Epoch: 164/200 Iteration: 49350 Train loss: 0.442930 Train acc: 0.815000\n",
      "Epoch: 164/200 Iteration: 49400 Train loss: 0.505486 Train acc: 0.760000\n",
      "Epoch: 164/200 Iteration: 49450 Train loss: 0.386501 Train acc: 0.835000\n",
      "Epoch: 164/200 Iteration: 49500 Train loss: 0.397099 Train acc: 0.790000\n",
      "Epoch: 164/200 Iteration: 49500 Validation loss: 0.538460 Validation acc: 0.692150\n",
      "Epoch: 165/200 Iteration: 49550 Train loss: 0.510725 Train acc: 0.730000\n",
      "Epoch: 165/200 Iteration: 49600 Train loss: 0.531657 Train acc: 0.775000\n",
      "Epoch: 165/200 Iteration: 49650 Train loss: 0.443179 Train acc: 0.780000\n",
      "Epoch: 165/200 Iteration: 49700 Train loss: 0.492644 Train acc: 0.755000\n",
      "Epoch: 165/200 Iteration: 49750 Train loss: 0.370676 Train acc: 0.805000\n",
      "Epoch: 165/200 Iteration: 49750 Validation loss: 0.538886 Validation acc: 0.693100\n",
      "Epoch: 165/200 Iteration: 49800 Train loss: 0.397412 Train acc: 0.830000\n",
      "Epoch: 166/200 Iteration: 49850 Train loss: 0.523874 Train acc: 0.725000\n",
      "Epoch: 166/200 Iteration: 49900 Train loss: 0.521330 Train acc: 0.795000\n",
      "Epoch: 166/200 Iteration: 49950 Train loss: 0.453172 Train acc: 0.780000\n",
      "Epoch: 166/200 Iteration: 50000 Train loss: 0.474935 Train acc: 0.760000\n",
      "Epoch: 166/200 Iteration: 50000 Validation loss: 0.540057 Validation acc: 0.693800\n",
      "Epoch: 166/200 Iteration: 50050 Train loss: 0.362324 Train acc: 0.825000\n",
      "Epoch: 166/200 Iteration: 50100 Train loss: 0.387630 Train acc: 0.825000\n",
      "Epoch: 167/200 Iteration: 50150 Train loss: 0.526107 Train acc: 0.740000\n",
      "Epoch: 167/200 Iteration: 50200 Train loss: 0.503078 Train acc: 0.800000\n",
      "Epoch: 167/200 Iteration: 50250 Train loss: 0.435123 Train acc: 0.800000\n",
      "Epoch: 167/200 Iteration: 50250 Validation loss: 0.540134 Validation acc: 0.690000\n",
      "Epoch: 167/200 Iteration: 50300 Train loss: 0.502632 Train acc: 0.730000\n",
      "Epoch: 167/200 Iteration: 50350 Train loss: 0.372105 Train acc: 0.820000\n",
      "Epoch: 167/200 Iteration: 50400 Train loss: 0.391868 Train acc: 0.820000\n",
      "Epoch: 168/200 Iteration: 50450 Train loss: 0.541001 Train acc: 0.700000\n",
      "Epoch: 168/200 Iteration: 50500 Train loss: 0.499459 Train acc: 0.810000\n",
      "Epoch: 168/200 Iteration: 50500 Validation loss: 0.539374 Validation acc: 0.692600\n",
      "Epoch: 168/200 Iteration: 50550 Train loss: 0.433727 Train acc: 0.805000\n",
      "Epoch: 168/200 Iteration: 50600 Train loss: 0.478984 Train acc: 0.760000\n",
      "Epoch: 168/200 Iteration: 50650 Train loss: 0.370107 Train acc: 0.840000\n",
      "Epoch: 168/200 Iteration: 50700 Train loss: 0.370877 Train acc: 0.820000\n",
      "Epoch: 169/200 Iteration: 50750 Train loss: 0.534224 Train acc: 0.720000\n",
      "Epoch: 169/200 Iteration: 50750 Validation loss: 0.538574 Validation acc: 0.695300\n",
      "Epoch: 169/200 Iteration: 50800 Train loss: 0.531868 Train acc: 0.790000\n",
      "Epoch: 169/200 Iteration: 50850 Train loss: 0.435593 Train acc: 0.805000\n",
      "Epoch: 169/200 Iteration: 50900 Train loss: 0.492869 Train acc: 0.745000\n",
      "Epoch: 169/200 Iteration: 50950 Train loss: 0.366520 Train acc: 0.835000\n",
      "Epoch: 169/200 Iteration: 51000 Train loss: 0.369594 Train acc: 0.820000\n",
      "Epoch: 169/200 Iteration: 51000 Validation loss: 0.539051 Validation acc: 0.692950\n",
      "Epoch: 170/200 Iteration: 51050 Train loss: 0.508439 Train acc: 0.765000\n",
      "Epoch: 170/200 Iteration: 51100 Train loss: 0.516781 Train acc: 0.790000\n",
      "Epoch: 170/200 Iteration: 51150 Train loss: 0.431262 Train acc: 0.785000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170/200 Iteration: 51200 Train loss: 0.487507 Train acc: 0.745000\n",
      "Epoch: 170/200 Iteration: 51250 Train loss: 0.371891 Train acc: 0.815000\n",
      "Epoch: 170/200 Iteration: 51250 Validation loss: 0.539599 Validation acc: 0.691150\n",
      "Epoch: 170/200 Iteration: 51300 Train loss: 0.366085 Train acc: 0.825000\n",
      "Epoch: 171/200 Iteration: 51350 Train loss: 0.514579 Train acc: 0.775000\n",
      "Epoch: 171/200 Iteration: 51400 Train loss: 0.520875 Train acc: 0.815000\n",
      "Epoch: 171/200 Iteration: 51450 Train loss: 0.435753 Train acc: 0.795000\n",
      "Epoch: 171/200 Iteration: 51500 Train loss: 0.464933 Train acc: 0.765000\n",
      "Epoch: 171/200 Iteration: 51500 Validation loss: 0.539990 Validation acc: 0.691850\n",
      "Epoch: 171/200 Iteration: 51550 Train loss: 0.372142 Train acc: 0.835000\n",
      "Epoch: 171/200 Iteration: 51600 Train loss: 0.382201 Train acc: 0.825000\n",
      "Epoch: 172/200 Iteration: 51650 Train loss: 0.529341 Train acc: 0.750000\n",
      "Epoch: 172/200 Iteration: 51700 Train loss: 0.525500 Train acc: 0.800000\n",
      "Epoch: 172/200 Iteration: 51750 Train loss: 0.436145 Train acc: 0.805000\n",
      "Epoch: 172/200 Iteration: 51750 Validation loss: 0.540366 Validation acc: 0.690250\n",
      "Epoch: 172/200 Iteration: 51800 Train loss: 0.483989 Train acc: 0.765000\n",
      "Epoch: 172/200 Iteration: 51850 Train loss: 0.383897 Train acc: 0.805000\n",
      "Epoch: 172/200 Iteration: 51900 Train loss: 0.398013 Train acc: 0.815000\n",
      "Epoch: 173/200 Iteration: 51950 Train loss: 0.518369 Train acc: 0.730000\n",
      "Epoch: 173/200 Iteration: 52000 Train loss: 0.509984 Train acc: 0.775000\n",
      "Epoch: 173/200 Iteration: 52000 Validation loss: 0.540064 Validation acc: 0.692650\n",
      "Epoch: 173/200 Iteration: 52050 Train loss: 0.422983 Train acc: 0.815000\n",
      "Epoch: 173/200 Iteration: 52100 Train loss: 0.483945 Train acc: 0.755000\n",
      "Epoch: 173/200 Iteration: 52150 Train loss: 0.354213 Train acc: 0.825000\n",
      "Epoch: 173/200 Iteration: 52200 Train loss: 0.370154 Train acc: 0.825000\n",
      "Epoch: 174/200 Iteration: 52250 Train loss: 0.546975 Train acc: 0.705000\n",
      "Epoch: 174/200 Iteration: 52250 Validation loss: 0.538928 Validation acc: 0.693750\n",
      "Epoch: 174/200 Iteration: 52300 Train loss: 0.510406 Train acc: 0.805000\n",
      "Epoch: 174/200 Iteration: 52350 Train loss: 0.421228 Train acc: 0.830000\n",
      "Epoch: 174/200 Iteration: 52400 Train loss: 0.483285 Train acc: 0.765000\n",
      "Epoch: 174/200 Iteration: 52450 Train loss: 0.360365 Train acc: 0.820000\n",
      "Epoch: 174/200 Iteration: 52500 Train loss: 0.373733 Train acc: 0.815000\n",
      "Epoch: 174/200 Iteration: 52500 Validation loss: 0.539465 Validation acc: 0.692100\n",
      "Epoch: 175/200 Iteration: 52550 Train loss: 0.535528 Train acc: 0.735000\n",
      "Epoch: 175/200 Iteration: 52600 Train loss: 0.513277 Train acc: 0.790000\n",
      "Epoch: 175/200 Iteration: 52650 Train loss: 0.449332 Train acc: 0.790000\n",
      "Epoch: 175/200 Iteration: 52700 Train loss: 0.499847 Train acc: 0.775000\n",
      "Epoch: 175/200 Iteration: 52750 Train loss: 0.371972 Train acc: 0.835000\n",
      "Epoch: 175/200 Iteration: 52750 Validation loss: 0.539625 Validation acc: 0.691800\n",
      "Epoch: 175/200 Iteration: 52800 Train loss: 0.387878 Train acc: 0.815000\n",
      "Epoch: 176/200 Iteration: 52850 Train loss: 0.570815 Train acc: 0.680000\n",
      "Epoch: 176/200 Iteration: 52900 Train loss: 0.490625 Train acc: 0.785000\n",
      "Epoch: 176/200 Iteration: 52950 Train loss: 0.411675 Train acc: 0.785000\n",
      "Epoch: 176/200 Iteration: 53000 Train loss: 0.482777 Train acc: 0.760000\n",
      "Epoch: 176/200 Iteration: 53000 Validation loss: 0.540784 Validation acc: 0.692800\n",
      "Epoch: 176/200 Iteration: 53050 Train loss: 0.361806 Train acc: 0.815000\n",
      "Epoch: 176/200 Iteration: 53100 Train loss: 0.381541 Train acc: 0.820000\n",
      "Epoch: 177/200 Iteration: 53150 Train loss: 0.516063 Train acc: 0.730000\n",
      "Epoch: 177/200 Iteration: 53200 Train loss: 0.500423 Train acc: 0.800000\n",
      "Epoch: 177/200 Iteration: 53250 Train loss: 0.437426 Train acc: 0.805000\n",
      "Epoch: 177/200 Iteration: 53250 Validation loss: 0.541519 Validation acc: 0.691250\n",
      "Epoch: 177/200 Iteration: 53300 Train loss: 0.497115 Train acc: 0.750000\n",
      "Epoch: 177/200 Iteration: 53350 Train loss: 0.348998 Train acc: 0.845000\n",
      "Epoch: 177/200 Iteration: 53400 Train loss: 0.369703 Train acc: 0.820000\n",
      "Epoch: 178/200 Iteration: 53450 Train loss: 0.530324 Train acc: 0.720000\n",
      "Epoch: 178/200 Iteration: 53500 Train loss: 0.481214 Train acc: 0.800000\n",
      "Epoch: 178/200 Iteration: 53500 Validation loss: 0.540779 Validation acc: 0.693250\n",
      "Epoch: 178/200 Iteration: 53550 Train loss: 0.426789 Train acc: 0.805000\n",
      "Epoch: 178/200 Iteration: 53600 Train loss: 0.488513 Train acc: 0.740000\n",
      "Epoch: 178/200 Iteration: 53650 Train loss: 0.376768 Train acc: 0.825000\n",
      "Epoch: 178/200 Iteration: 53700 Train loss: 0.386551 Train acc: 0.845000\n",
      "Epoch: 179/200 Iteration: 53750 Train loss: 0.531858 Train acc: 0.755000\n",
      "Epoch: 179/200 Iteration: 53750 Validation loss: 0.539405 Validation acc: 0.692500\n",
      "Epoch: 179/200 Iteration: 53800 Train loss: 0.492676 Train acc: 0.805000\n",
      "Epoch: 179/200 Iteration: 53850 Train loss: 0.414723 Train acc: 0.810000\n",
      "Epoch: 179/200 Iteration: 53900 Train loss: 0.482131 Train acc: 0.755000\n",
      "Epoch: 179/200 Iteration: 53950 Train loss: 0.375048 Train acc: 0.825000\n",
      "Epoch: 179/200 Iteration: 54000 Train loss: 0.384709 Train acc: 0.795000\n",
      "Epoch: 179/200 Iteration: 54000 Validation loss: 0.539578 Validation acc: 0.692050\n",
      "Epoch: 180/200 Iteration: 54050 Train loss: 0.526824 Train acc: 0.755000\n",
      "Epoch: 180/200 Iteration: 54100 Train loss: 0.482981 Train acc: 0.790000\n",
      "Epoch: 180/200 Iteration: 54150 Train loss: 0.439001 Train acc: 0.785000\n",
      "Epoch: 180/200 Iteration: 54200 Train loss: 0.516415 Train acc: 0.735000\n",
      "Epoch: 180/200 Iteration: 54250 Train loss: 0.358668 Train acc: 0.825000\n",
      "Epoch: 180/200 Iteration: 54250 Validation loss: 0.539449 Validation acc: 0.691000\n",
      "Epoch: 180/200 Iteration: 54300 Train loss: 0.380016 Train acc: 0.795000\n",
      "Epoch: 181/200 Iteration: 54350 Train loss: 0.497404 Train acc: 0.750000\n",
      "Epoch: 181/200 Iteration: 54400 Train loss: 0.524265 Train acc: 0.795000\n",
      "Epoch: 181/200 Iteration: 54450 Train loss: 0.442641 Train acc: 0.800000\n",
      "Epoch: 181/200 Iteration: 54500 Train loss: 0.485345 Train acc: 0.730000\n",
      "Epoch: 181/200 Iteration: 54500 Validation loss: 0.540481 Validation acc: 0.692800\n",
      "Epoch: 181/200 Iteration: 54550 Train loss: 0.379100 Train acc: 0.845000\n",
      "Epoch: 181/200 Iteration: 54600 Train loss: 0.380191 Train acc: 0.845000\n",
      "Epoch: 182/200 Iteration: 54650 Train loss: 0.534248 Train acc: 0.740000\n",
      "Epoch: 182/200 Iteration: 54700 Train loss: 0.492180 Train acc: 0.800000\n",
      "Epoch: 182/200 Iteration: 54750 Train loss: 0.430861 Train acc: 0.800000\n",
      "Epoch: 182/200 Iteration: 54750 Validation loss: 0.541450 Validation acc: 0.691200\n",
      "Epoch: 182/200 Iteration: 54800 Train loss: 0.485927 Train acc: 0.725000\n",
      "Epoch: 182/200 Iteration: 54850 Train loss: 0.362997 Train acc: 0.840000\n",
      "Epoch: 182/200 Iteration: 54900 Train loss: 0.383957 Train acc: 0.815000\n",
      "Epoch: 183/200 Iteration: 54950 Train loss: 0.523558 Train acc: 0.730000\n",
      "Epoch: 183/200 Iteration: 55000 Train loss: 0.507604 Train acc: 0.795000\n",
      "Epoch: 183/200 Iteration: 55000 Validation loss: 0.541497 Validation acc: 0.691000\n",
      "Epoch: 183/200 Iteration: 55050 Train loss: 0.438007 Train acc: 0.790000\n",
      "Epoch: 183/200 Iteration: 55100 Train loss: 0.502024 Train acc: 0.750000\n",
      "Epoch: 183/200 Iteration: 55150 Train loss: 0.375787 Train acc: 0.840000\n",
      "Epoch: 183/200 Iteration: 55200 Train loss: 0.397076 Train acc: 0.835000\n",
      "Epoch: 184/200 Iteration: 55250 Train loss: 0.521117 Train acc: 0.740000\n",
      "Epoch: 184/200 Iteration: 55250 Validation loss: 0.540339 Validation acc: 0.693150\n",
      "Epoch: 184/200 Iteration: 55300 Train loss: 0.485175 Train acc: 0.785000\n",
      "Epoch: 184/200 Iteration: 55350 Train loss: 0.420278 Train acc: 0.805000\n",
      "Epoch: 184/200 Iteration: 55400 Train loss: 0.472937 Train acc: 0.765000\n",
      "Epoch: 184/200 Iteration: 55450 Train loss: 0.378969 Train acc: 0.835000\n",
      "Epoch: 184/200 Iteration: 55500 Train loss: 0.376500 Train acc: 0.815000\n",
      "Epoch: 184/200 Iteration: 55500 Validation loss: 0.540024 Validation acc: 0.691800\n",
      "Epoch: 185/200 Iteration: 55550 Train loss: 0.526351 Train acc: 0.725000\n",
      "Epoch: 185/200 Iteration: 55600 Train loss: 0.502920 Train acc: 0.795000\n",
      "Epoch: 185/200 Iteration: 55650 Train loss: 0.452684 Train acc: 0.790000\n",
      "Epoch: 185/200 Iteration: 55700 Train loss: 0.479887 Train acc: 0.760000\n",
      "Epoch: 185/200 Iteration: 55750 Train loss: 0.367688 Train acc: 0.845000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185/200 Iteration: 55750 Validation loss: 0.540213 Validation acc: 0.690650\n",
      "Epoch: 185/200 Iteration: 55800 Train loss: 0.378376 Train acc: 0.830000\n",
      "Epoch: 186/200 Iteration: 55850 Train loss: 0.527101 Train acc: 0.730000\n",
      "Epoch: 186/200 Iteration: 55900 Train loss: 0.500524 Train acc: 0.815000\n",
      "Epoch: 186/200 Iteration: 55950 Train loss: 0.441430 Train acc: 0.790000\n",
      "Epoch: 186/200 Iteration: 56000 Train loss: 0.476929 Train acc: 0.770000\n",
      "Epoch: 186/200 Iteration: 56000 Validation loss: 0.541057 Validation acc: 0.690950\n",
      "Epoch: 186/200 Iteration: 56050 Train loss: 0.354341 Train acc: 0.845000\n",
      "Epoch: 186/200 Iteration: 56100 Train loss: 0.366716 Train acc: 0.850000\n",
      "Epoch: 187/200 Iteration: 56150 Train loss: 0.510403 Train acc: 0.735000\n",
      "Epoch: 187/200 Iteration: 56200 Train loss: 0.524342 Train acc: 0.790000\n",
      "Epoch: 187/200 Iteration: 56250 Train loss: 0.429092 Train acc: 0.790000\n",
      "Epoch: 187/200 Iteration: 56250 Validation loss: 0.541828 Validation acc: 0.688950\n",
      "Epoch: 187/200 Iteration: 56300 Train loss: 0.472152 Train acc: 0.765000\n",
      "Epoch: 187/200 Iteration: 56350 Train loss: 0.361671 Train acc: 0.845000\n",
      "Epoch: 187/200 Iteration: 56400 Train loss: 0.379132 Train acc: 0.835000\n",
      "Epoch: 188/200 Iteration: 56450 Train loss: 0.517866 Train acc: 0.715000\n",
      "Epoch: 188/200 Iteration: 56500 Train loss: 0.505088 Train acc: 0.800000\n",
      "Epoch: 188/200 Iteration: 56500 Validation loss: 0.541469 Validation acc: 0.690400\n",
      "Epoch: 188/200 Iteration: 56550 Train loss: 0.437726 Train acc: 0.800000\n",
      "Epoch: 188/200 Iteration: 56600 Train loss: 0.493843 Train acc: 0.760000\n",
      "Epoch: 188/200 Iteration: 56650 Train loss: 0.347033 Train acc: 0.825000\n",
      "Epoch: 188/200 Iteration: 56700 Train loss: 0.385523 Train acc: 0.800000\n",
      "Epoch: 189/200 Iteration: 56750 Train loss: 0.519468 Train acc: 0.730000\n",
      "Epoch: 189/200 Iteration: 56750 Validation loss: 0.540788 Validation acc: 0.694200\n",
      "Epoch: 189/200 Iteration: 56800 Train loss: 0.475155 Train acc: 0.780000\n",
      "Epoch: 189/200 Iteration: 56850 Train loss: 0.420922 Train acc: 0.790000\n",
      "Epoch: 189/200 Iteration: 56900 Train loss: 0.478339 Train acc: 0.750000\n",
      "Epoch: 189/200 Iteration: 56950 Train loss: 0.357682 Train acc: 0.850000\n",
      "Epoch: 189/200 Iteration: 57000 Train loss: 0.383213 Train acc: 0.815000\n",
      "Epoch: 189/200 Iteration: 57000 Validation loss: 0.540970 Validation acc: 0.690700\n",
      "Epoch: 190/200 Iteration: 57050 Train loss: 0.525748 Train acc: 0.725000\n",
      "Epoch: 190/200 Iteration: 57100 Train loss: 0.539869 Train acc: 0.775000\n",
      "Epoch: 190/200 Iteration: 57150 Train loss: 0.424096 Train acc: 0.795000\n",
      "Epoch: 190/200 Iteration: 57200 Train loss: 0.480019 Train acc: 0.765000\n",
      "Epoch: 190/200 Iteration: 57250 Train loss: 0.380799 Train acc: 0.820000\n",
      "Epoch: 190/200 Iteration: 57250 Validation loss: 0.540899 Validation acc: 0.690300\n",
      "Epoch: 190/200 Iteration: 57300 Train loss: 0.384073 Train acc: 0.840000\n",
      "Epoch: 191/200 Iteration: 57350 Train loss: 0.527193 Train acc: 0.720000\n",
      "Epoch: 191/200 Iteration: 57400 Train loss: 0.498127 Train acc: 0.810000\n",
      "Epoch: 191/200 Iteration: 57450 Train loss: 0.420317 Train acc: 0.820000\n",
      "Epoch: 191/200 Iteration: 57500 Train loss: 0.487992 Train acc: 0.755000\n",
      "Epoch: 191/200 Iteration: 57500 Validation loss: 0.542299 Validation acc: 0.689350\n",
      "Epoch: 191/200 Iteration: 57550 Train loss: 0.358508 Train acc: 0.845000\n",
      "Epoch: 191/200 Iteration: 57600 Train loss: 0.377791 Train acc: 0.825000\n",
      "Epoch: 192/200 Iteration: 57650 Train loss: 0.517408 Train acc: 0.755000\n",
      "Epoch: 192/200 Iteration: 57700 Train loss: 0.491030 Train acc: 0.815000\n",
      "Epoch: 192/200 Iteration: 57750 Train loss: 0.440185 Train acc: 0.800000\n",
      "Epoch: 192/200 Iteration: 57750 Validation loss: 0.542198 Validation acc: 0.689400\n",
      "Epoch: 192/200 Iteration: 57800 Train loss: 0.471251 Train acc: 0.765000\n",
      "Epoch: 192/200 Iteration: 57850 Train loss: 0.357705 Train acc: 0.840000\n",
      "Epoch: 192/200 Iteration: 57900 Train loss: 0.390822 Train acc: 0.820000\n",
      "Epoch: 193/200 Iteration: 57950 Train loss: 0.525178 Train acc: 0.755000\n",
      "Epoch: 193/200 Iteration: 58000 Train loss: 0.511128 Train acc: 0.805000\n",
      "Epoch: 193/200 Iteration: 58000 Validation loss: 0.541894 Validation acc: 0.693000\n",
      "Epoch: 193/200 Iteration: 58050 Train loss: 0.442162 Train acc: 0.795000\n",
      "Epoch: 193/200 Iteration: 58100 Train loss: 0.483044 Train acc: 0.765000\n",
      "Epoch: 193/200 Iteration: 58150 Train loss: 0.362597 Train acc: 0.825000\n",
      "Epoch: 193/200 Iteration: 58200 Train loss: 0.375076 Train acc: 0.840000\n",
      "Epoch: 194/200 Iteration: 58250 Train loss: 0.532117 Train acc: 0.735000\n",
      "Epoch: 194/200 Iteration: 58250 Validation loss: 0.540687 Validation acc: 0.692200\n",
      "Epoch: 194/200 Iteration: 58300 Train loss: 0.482841 Train acc: 0.810000\n",
      "Epoch: 194/200 Iteration: 58350 Train loss: 0.424644 Train acc: 0.805000\n",
      "Epoch: 194/200 Iteration: 58400 Train loss: 0.469547 Train acc: 0.760000\n",
      "Epoch: 194/200 Iteration: 58450 Train loss: 0.367741 Train acc: 0.825000\n",
      "Epoch: 194/200 Iteration: 58500 Train loss: 0.383612 Train acc: 0.830000\n",
      "Epoch: 194/200 Iteration: 58500 Validation loss: 0.540191 Validation acc: 0.691550\n",
      "Epoch: 195/200 Iteration: 58550 Train loss: 0.504283 Train acc: 0.735000\n",
      "Epoch: 195/200 Iteration: 58600 Train loss: 0.501537 Train acc: 0.810000\n",
      "Epoch: 195/200 Iteration: 58650 Train loss: 0.432183 Train acc: 0.785000\n",
      "Epoch: 195/200 Iteration: 58700 Train loss: 0.490059 Train acc: 0.760000\n",
      "Epoch: 195/200 Iteration: 58750 Train loss: 0.385272 Train acc: 0.825000\n",
      "Epoch: 195/200 Iteration: 58750 Validation loss: 0.540446 Validation acc: 0.691200\n",
      "Epoch: 195/200 Iteration: 58800 Train loss: 0.368937 Train acc: 0.820000\n",
      "Epoch: 196/200 Iteration: 58850 Train loss: 0.495944 Train acc: 0.740000\n",
      "Epoch: 196/200 Iteration: 58900 Train loss: 0.510219 Train acc: 0.790000\n",
      "Epoch: 196/200 Iteration: 58950 Train loss: 0.445391 Train acc: 0.785000\n",
      "Epoch: 196/200 Iteration: 59000 Train loss: 0.471725 Train acc: 0.770000\n",
      "Epoch: 196/200 Iteration: 59000 Validation loss: 0.541588 Validation acc: 0.689550\n",
      "Epoch: 196/200 Iteration: 59050 Train loss: 0.366744 Train acc: 0.850000\n",
      "Epoch: 196/200 Iteration: 59100 Train loss: 0.377548 Train acc: 0.845000\n",
      "Epoch: 197/200 Iteration: 59150 Train loss: 0.500147 Train acc: 0.740000\n",
      "Epoch: 197/200 Iteration: 59200 Train loss: 0.493701 Train acc: 0.795000\n",
      "Epoch: 197/200 Iteration: 59250 Train loss: 0.452547 Train acc: 0.790000\n",
      "Epoch: 197/200 Iteration: 59250 Validation loss: 0.542689 Validation acc: 0.690950\n",
      "Epoch: 197/200 Iteration: 59300 Train loss: 0.489499 Train acc: 0.745000\n",
      "Epoch: 197/200 Iteration: 59350 Train loss: 0.351372 Train acc: 0.845000\n",
      "Epoch: 197/200 Iteration: 59400 Train loss: 0.396946 Train acc: 0.825000\n",
      "Epoch: 198/200 Iteration: 59450 Train loss: 0.522755 Train acc: 0.745000\n",
      "Epoch: 198/200 Iteration: 59500 Train loss: 0.516195 Train acc: 0.800000\n",
      "Epoch: 198/200 Iteration: 59500 Validation loss: 0.542519 Validation acc: 0.692750\n",
      "Epoch: 198/200 Iteration: 59550 Train loss: 0.437101 Train acc: 0.795000\n",
      "Epoch: 198/200 Iteration: 59600 Train loss: 0.492280 Train acc: 0.760000\n",
      "Epoch: 198/200 Iteration: 59650 Train loss: 0.394929 Train acc: 0.790000\n",
      "Epoch: 198/200 Iteration: 59700 Train loss: 0.380177 Train acc: 0.825000\n",
      "Epoch: 199/200 Iteration: 59750 Train loss: 0.498196 Train acc: 0.765000\n",
      "Epoch: 199/200 Iteration: 59750 Validation loss: 0.541327 Validation acc: 0.691550\n",
      "Epoch: 199/200 Iteration: 59800 Train loss: 0.485652 Train acc: 0.815000\n",
      "Epoch: 199/200 Iteration: 59850 Train loss: 0.401168 Train acc: 0.800000\n",
      "Epoch: 199/200 Iteration: 59900 Train loss: 0.493093 Train acc: 0.745000\n",
      "Epoch: 199/200 Iteration: 59950 Train loss: 0.360864 Train acc: 0.830000\n",
      "Epoch: 199/200 Iteration: 60000 Train loss: 0.348477 Train acc: 0.855000\n",
      "Epoch: 199/200 Iteration: 60000 Validation loss: 0.541009 Validation acc: 0.692550\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 50 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%250 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "  \n",
    "    \n",
    "    saver.save(sess,\"checkpoints/har-lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5801c5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 250 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d543c9e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 250 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\har-lstm.ckpt\n",
      "Test accuracy: 0.673333\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
