{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "G:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "G:\\Anaconda\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold,cross_val_score,train_test_split\n",
    "import tensorflow as tf\n",
    "from dateutil import parser\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should read billboard chart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = pd.read_csv(\"billboard_chart_data.csv\")  #read the billboard chart data which we use billboard API to get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = billboard[['title','artist','rank', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                               artist  rank        date\n",
       "0  God's Plan                                Drake     1  2018-02-17\n",
       "1     Perfect                           Ed Sheeran     2  2018-02-17\n",
       "2     Finesse                 Bruno Mars & Cardi B     3  2018-02-17\n",
       "3      Havana  Camila Cabello Featuring Young Thug     4  2018-02-17\n",
       "4    Rockstar      Post Malone Featuring 21 Savage     5  2018-02-17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100066 entries, 0 to 100065\n",
      "Data columns (total 4 columns):\n",
      "title     100066 non-null object\n",
      "artist    100066 non-null object\n",
      "rank      100066 non-null int64\n",
      "date      100066 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "billboard.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read all the track's audion feature infomation and artist's information, there are about 6237 songs which has audio feature and it's rank, artist's popularity, follwers information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = pd.read_csv(\"Full_data.csv\")  # read all the track's audio feature data(Use Spotify API to get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3Vo4wInECJQuz9BIBMOu8i</td>\n",
       "      <td>0du5cEVh5yTK9QJze8zA0C</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>11594549</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.926</td>\n",
       "      <td>105.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>1rfofaqEpACxVEHIZBJe6W</td>\n",
       "      <td>4nDoRrQiYLoBzwC5BhVJzF</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>3144195</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.394</td>\n",
       "      <td>104.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>0OAAAdiHJKa2wlCKqaYXV7</td>\n",
       "      <td>246dkjvS1zLTtiykXe5h60</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>3309267</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.127</td>\n",
       "      <td>159.764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       title                               artist  \\\n",
       "0   1  God's Plan                                Drake   \n",
       "1   2     Perfect                           Ed Sheeran   \n",
       "2   3     Finesse                 Bruno Mars & Cardi B   \n",
       "3   4      Havana  Camila Cabello Featuring Young Thug   \n",
       "4   5    Rockstar      Post Malone Featuring 21 Savage   \n",
       "\n",
       "                 track_id               artist_id  rank  hit  \\\n",
       "0  2XW4DbS6NddZxRPm5rMCeY  3TVXtAsR1Inumwj472S9r4     1  Yes   \n",
       "1  0tgVpDi06FyKpA1z0VMD4v  6eUKZXaKkcviH0Ku9w2n3V     1  Yes   \n",
       "2  3Vo4wInECJQuz9BIBMOu8i  0du5cEVh5yTK9QJze8zA0C     3  Yes   \n",
       "3  1rfofaqEpACxVEHIZBJe6W  4nDoRrQiYLoBzwC5BhVJzF     1  Yes   \n",
       "4  0OAAAdiHJKa2wlCKqaYXV7  246dkjvS1zLTtiykXe5h60     1  Yes   \n",
       "\n",
       "   artist_popularity  followers  popularity   ...     energy  key  loudness  \\\n",
       "0                100   18311322          99   ...      0.454    7    -9.488   \n",
       "1                 96   21431294          95   ...      0.448    8    -6.312   \n",
       "2                 91   11594549          95   ...      0.859    5    -4.877   \n",
       "3                 91    3144195          97   ...      0.523    2    -4.333   \n",
       "4                 96    3309267          90   ...      0.535    5    -6.637   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0     1       0.0963        0.0244          0.000056    0.4980    0.344   \n",
       "1     1       0.0232        0.1630          0.000000    0.1060    0.168   \n",
       "2     0       0.0996        0.0185          0.000000    0.0215    0.926   \n",
       "3     1       0.0300        0.1840          0.000036    0.1320    0.394   \n",
       "4     0       0.0776        0.1300          0.000130    0.1430    0.127   \n",
       "\n",
       "     tempo  \n",
       "0   77.170  \n",
       "1   95.050  \n",
       "2  105.115  \n",
       "3  104.988  \n",
       "4  159.764  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6237 entries, 0 to 6236\n",
      "Data columns (total 21 columns):\n",
      "id                   6237 non-null int64\n",
      "title                6237 non-null object\n",
      "artist               6237 non-null object\n",
      "track_id             6237 non-null object\n",
      "artist_id            6237 non-null object\n",
      "rank                 6237 non-null int64\n",
      "hit                  6237 non-null object\n",
      "artist_popularity    6237 non-null int64\n",
      "followers            6237 non-null int64\n",
      "popularity           6237 non-null int64\n",
      "danceability         6237 non-null float64\n",
      "energy               6237 non-null float64\n",
      "key                  6237 non-null int64\n",
      "loudness             6237 non-null float64\n",
      "mode                 6237 non-null int64\n",
      "speechiness          6237 non-null float64\n",
      "acousticness         6237 non-null float64\n",
      "instrumentalness     6237 non-null float64\n",
      "liveness             6237 non-null float64\n",
      "valence              6237 non-null float64\n",
      "tempo                6237 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 1023.3+ KB\n"
     ]
    }
   ],
   "source": [
    "track.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to merge the two table into one table, which will contain the track's audio feature and it's rank information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.merge(billboard,track,on=['title','artist']) #merge the two table by title and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merge_data['rank_y']\n",
    "del merge_data['hit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \".head()\" to view the head of this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>1</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>2</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title      artist  rank_x        date  id                track_id  \\\n",
       "0  God's Plan       Drake       1  2018-02-17   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "1  God's Plan       Drake       1  2018-02-10   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "2  God's Plan       Drake       1  2018-02-03   1  2XW4DbS6NddZxRPm5rMCeY   \n",
       "3     Perfect  Ed Sheeran       2  2018-02-17   2  0tgVpDi06FyKpA1z0VMD4v   \n",
       "4     Perfect  Ed Sheeran       2  2018-02-10   2  0tgVpDi06FyKpA1z0VMD4v   \n",
       "\n",
       "                artist_id  artist_popularity  followers  popularity  ...    \\\n",
       "0  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "1  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "2  3TVXtAsR1Inumwj472S9r4                100   18311322          99  ...     \n",
       "3  6eUKZXaKkcviH0Ku9w2n3V                 96   21431294          95  ...     \n",
       "4  6eUKZXaKkcviH0Ku9w2n3V                 96   21431294          95  ...     \n",
       "\n",
       "   energy  key  loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "0   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "1   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "2   0.454    7    -9.488     1       0.0963        0.0244          0.000056   \n",
       "3   0.448    8    -6.312     1       0.0232        0.1630          0.000000   \n",
       "4   0.448    8    -6.312     1       0.0232        0.1630          0.000000   \n",
       "\n",
       "   liveness  valence  tempo  \n",
       "0     0.498    0.344  77.17  \n",
       "1     0.498    0.344  77.17  \n",
       "2     0.498    0.344  77.17  \n",
       "3     0.106    0.168  95.05  \n",
       "4     0.106    0.168  95.05  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 89689 entries, 0 to 89688\n",
      "Data columns (total 21 columns):\n",
      "title                89689 non-null object\n",
      "artist               89689 non-null object\n",
      "rank_x               89689 non-null int64\n",
      "date                 89689 non-null object\n",
      "id                   89689 non-null int64\n",
      "track_id             89689 non-null object\n",
      "artist_id            89689 non-null object\n",
      "artist_popularity    89689 non-null int64\n",
      "followers            89689 non-null int64\n",
      "popularity           89689 non-null int64\n",
      "danceability         89689 non-null float64\n",
      "energy               89689 non-null float64\n",
      "key                  89689 non-null int64\n",
      "loudness             89689 non-null float64\n",
      "mode                 89689 non-null int64\n",
      "speechiness          89689 non-null float64\n",
      "acousticness         89689 non-null float64\n",
      "instrumentalness     89689 non-null float64\n",
      "liveness             89689 non-null float64\n",
      "valence              89689 non-null float64\n",
      "tempo                89689 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 15.1+ MB\n"
     ]
    }
   ],
   "source": [
    "merge_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that the order of the table above is random after we merged the two table, so we need to sort these songs by their date and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = merge_data.sort_values(by=['date','rank_x'],ascending=True)  #sort the merger data according to date and rank_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.to_csv(\"merge_data.csv\", index = None)  #save data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merge_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity   ...     energy  key  loudness  mode  speechiness  \\\n",
       "0    1465113          44   ...      0.504    0    -7.260     1       0.0301   \n",
       "1     160121          56   ...      0.531    8    -5.300     1       0.0361   \n",
       "2     816631          74   ...      0.505    2    -8.926     0       0.2450   \n",
       "3     865667          50   ...      0.514    7    -5.021     1       0.0271   \n",
       "4     372926          60   ...      0.551    0    -7.328     1       0.0616   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  \n",
       "0       0.66900          0.000000    0.1100    0.266  112.681  \n",
       "1       0.20200          0.000012    0.5950    0.399  119.957  \n",
       "2       0.03930          0.000000    0.0923    0.495   99.935  \n",
       "3       0.38200          0.000000    0.1860    0.148  135.819  \n",
       "4       0.00364          0.000000    0.0451    0.796  100.904  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label every songs “Yse” or \"No\" by their rank in given date, if rank lower than 30, label \"Yes\", otherwise, label \"No\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak = []\n",
    "for i in range(0, len(df)):\n",
    "    tmp = []\n",
    "    c = df['rank_x'][i]\n",
    "    if c < 30:\n",
    "        tmp.append(\"Yes\")\n",
    "    else:\n",
    "        tmp.append(\"No\")\n",
    "    peak.append(tmp)\n",
    "df_peak = pd.DataFrame(peak)\n",
    "df_peak.columns = ['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89689 entries, 0 to 89688\n",
      "Data columns (total 1 columns):\n",
      "hit    89689 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 700.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_peak.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat df with df_peak to get new result which cotains track audio feature and it's artist popularity and follower, and hit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df, df_peak], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity ...   key  loudness  mode  speechiness  acousticness  \\\n",
       "0    1465113          44 ...     0    -7.260     1       0.0301       0.66900   \n",
       "1     160121          56 ...     8    -5.300     1       0.0361       0.20200   \n",
       "2     816631          74 ...     2    -8.926     0       0.2450       0.03930   \n",
       "3     865667          50 ...     7    -5.021     1       0.0271       0.38200   \n",
       "4     372926          60 ...     0    -7.328     1       0.0616       0.00364   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  hit  \n",
       "0          0.000000    0.1100    0.266  112.681  Yes  \n",
       "1          0.000012    0.5950    0.399  119.957  Yes  \n",
       "2          0.000000    0.0923    0.495   99.935  Yes  \n",
       "3          0.000000    0.1860    0.148  135.819  Yes  \n",
       "4          0.000000    0.0451    0.796  100.904  Yes  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"billboard_charts.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of days with the first day, because we want to use LSTM to predict the song's rank, so we have to calculate the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days with the first day\n",
    "time = []\n",
    "for i in range(0, len(result)):\n",
    "    tmp = []\n",
    "    if i == 0:\n",
    "        tmp.append(\"0\")\n",
    "    else:\n",
    "        df1 = parser.parse(result['date'][0])\n",
    "        df2 = parser.parse(result['date'][i])\n",
    "        df = (df2-df1).days\n",
    "        #print(df)\n",
    "        tmp.append(df)\n",
    "    time.append(tmp)\n",
    "df_days = pd.DataFrame(time)\n",
    "df_days.columns= ['days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get new columns \"days\", we concat this columns with result by using \"concat()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_data = pd.concat([result, df_days], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>rank_x</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>hit</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm Your Angel</td>\n",
       "      <td>R. Kelly &amp; Celine Dion</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7132</td>\n",
       "      <td>0QWFiyhOqFu9RP8qzP6z5L</td>\n",
       "      <td>2mxe0TnaNL039ysAj51xPQ</td>\n",
       "      <td>77</td>\n",
       "      <td>1465113</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>112.681</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody's Supposed To Be Here</td>\n",
       "      <td>Deborah Cox</td>\n",
       "      <td>2</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7121</td>\n",
       "      <td>51QxenFmXlJXUN9mpvxlaL</td>\n",
       "      <td>601893mmW5hl1FBOykWZHG</td>\n",
       "      <td>55</td>\n",
       "      <td>160121</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.399</td>\n",
       "      <td>119.957</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doo Wop (That Thing)</td>\n",
       "      <td>Lauryn Hill</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7134</td>\n",
       "      <td>2Uu8IiLkLY0UXhCHka4Dlr</td>\n",
       "      <td>2Mu5NfyYm8n5iTomuKAEHl</td>\n",
       "      <td>70</td>\n",
       "      <td>816631</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.495</td>\n",
       "      <td>99.935</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From This Moment On</td>\n",
       "      <td>Shania Twain</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7161</td>\n",
       "      <td>7n2vETKqnlDt4670aVs5n5</td>\n",
       "      <td>5e4Dhzv426EvQe3aDb64jL</td>\n",
       "      <td>71</td>\n",
       "      <td>865667</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.38200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.148</td>\n",
       "      <td>135.819</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love Like This</td>\n",
       "      <td>Faith Evans</td>\n",
       "      <td>7</td>\n",
       "      <td>1998-12-12</td>\n",
       "      <td>7140</td>\n",
       "      <td>7MQywXGHEev7JmwwIzMcao</td>\n",
       "      <td>5NDMothbpdpq2xHqSjrrWn</td>\n",
       "      <td>70</td>\n",
       "      <td>372926</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.796</td>\n",
       "      <td>100.904</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title                  artist  rank_x        date  \\\n",
       "0                I'm Your Angel  R. Kelly & Celine Dion       1  1998-12-12   \n",
       "1  Nobody's Supposed To Be Here             Deborah Cox       2  1998-12-12   \n",
       "2          Doo Wop (That Thing)             Lauryn Hill       4  1998-12-12   \n",
       "3           From This Moment On            Shania Twain       5  1998-12-12   \n",
       "4                Love Like This             Faith Evans       7  1998-12-12   \n",
       "\n",
       "     id                track_id               artist_id  artist_popularity  \\\n",
       "0  7132  0QWFiyhOqFu9RP8qzP6z5L  2mxe0TnaNL039ysAj51xPQ                 77   \n",
       "1  7121  51QxenFmXlJXUN9mpvxlaL  601893mmW5hl1FBOykWZHG                 55   \n",
       "2  7134  2Uu8IiLkLY0UXhCHka4Dlr  2Mu5NfyYm8n5iTomuKAEHl                 70   \n",
       "3  7161  7n2vETKqnlDt4670aVs5n5  5e4Dhzv426EvQe3aDb64jL                 71   \n",
       "4  7140  7MQywXGHEev7JmwwIzMcao  5NDMothbpdpq2xHqSjrrWn                 70   \n",
       "\n",
       "   followers  popularity  ...   loudness  mode  speechiness  acousticness  \\\n",
       "0    1465113          44  ...     -7.260     1       0.0301       0.66900   \n",
       "1     160121          56  ...     -5.300     1       0.0361       0.20200   \n",
       "2     816631          74  ...     -8.926     0       0.2450       0.03930   \n",
       "3     865667          50  ...     -5.021     1       0.0271       0.38200   \n",
       "4     372926          60  ...     -7.328     1       0.0616       0.00364   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  hit  days  \n",
       "0          0.000000    0.1100    0.266  112.681  Yes     0  \n",
       "1          0.000012    0.5950    0.399  119.957  Yes     0  \n",
       "2          0.000000    0.0923    0.495   99.935  Yes     0  \n",
       "3          0.000000    0.1860    0.148  135.819  Yes     0  \n",
       "4          0.000000    0.0451    0.796  100.904  Yes     0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LSTM_data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_data.to_csv('LSTM_data.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to read data, and then normalize these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Full_data.csv\") # read data\n",
    "df = df[['rank','artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    "         ,'instrumentalness','liveness','valence','tempo']]\n",
    "data_zs = preprocessing.normalize(df, norm='l2') # normalization\n",
    "data_zs = pd.DataFrame(data_zs)\n",
    "data_zs.columns = ['rank','artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    "         ,'instrumentalness','liveness','valence','tempo'] # add columns name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we get correlation matrix, which is used to investigate the dependence between multiple variables at the same time. The result is a table containing the correlation coefficients between each variable and the others  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>-0.796614</td>\n",
       "      <td>0.770435</td>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.947498</td>\n",
       "      <td>0.860511</td>\n",
       "      <td>-0.763978</td>\n",
       "      <td>0.744715</td>\n",
       "      <td>0.630041</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.799281</td>\n",
       "      <td>0.877603</td>\n",
       "      <td>0.890368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist_popularity</th>\n",
       "      <td>0.705908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.581229</td>\n",
       "      <td>0.941691</td>\n",
       "      <td>0.767426</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>-0.621750</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.619311</td>\n",
       "      <td>0.529364</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>0.792695</td>\n",
       "      <td>0.796074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>followers</th>\n",
       "      <td>-0.796614</td>\n",
       "      <td>-0.581229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.702855</td>\n",
       "      <td>-0.899792</td>\n",
       "      <td>-0.802854</td>\n",
       "      <td>-0.831456</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>-0.707081</td>\n",
       "      <td>-0.702654</td>\n",
       "      <td>-0.628552</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.772676</td>\n",
       "      <td>-0.805326</td>\n",
       "      <td>-0.909881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popularity</th>\n",
       "      <td>0.770435</td>\n",
       "      <td>0.941691</td>\n",
       "      <td>-0.702855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848601</td>\n",
       "      <td>0.835431</td>\n",
       "      <td>0.752888</td>\n",
       "      <td>-0.711136</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.618129</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.866959</td>\n",
       "      <td>0.856565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.767426</td>\n",
       "      <td>-0.899792</td>\n",
       "      <td>0.848601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953093</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>-0.907028</td>\n",
       "      <td>0.776749</td>\n",
       "      <td>0.803351</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.131064</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>0.943912</td>\n",
       "      <td>0.958921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.947498</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>-0.802854</td>\n",
       "      <td>0.835431</td>\n",
       "      <td>0.953093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857838</td>\n",
       "      <td>-0.800408</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.729046</td>\n",
       "      <td>0.594591</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>0.853236</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>0.923472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <td>0.860511</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>-0.831456</td>\n",
       "      <td>0.752888</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>0.857838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.867666</td>\n",
       "      <td>0.706227</td>\n",
       "      <td>0.657342</td>\n",
       "      <td>0.703256</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>0.872165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>-0.763978</td>\n",
       "      <td>-0.621750</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>-0.711136</td>\n",
       "      <td>-0.907028</td>\n",
       "      <td>-0.800408</td>\n",
       "      <td>-0.867666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.765798</td>\n",
       "      <td>-0.783093</td>\n",
       "      <td>-0.588246</td>\n",
       "      <td>-0.210410</td>\n",
       "      <td>-0.783172</td>\n",
       "      <td>-0.775187</td>\n",
       "      <td>-0.911791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>0.744715</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>-0.707081</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>0.776749</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.706227</td>\n",
       "      <td>-0.765798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.593416</td>\n",
       "      <td>0.377448</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.771454</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>0.765360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.630041</td>\n",
       "      <td>0.619311</td>\n",
       "      <td>-0.702654</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.803351</td>\n",
       "      <td>0.729046</td>\n",
       "      <td>0.657342</td>\n",
       "      <td>-0.783093</td>\n",
       "      <td>0.593416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503829</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.730178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.529364</td>\n",
       "      <td>-0.628552</td>\n",
       "      <td>0.618129</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.594591</td>\n",
       "      <td>0.703256</td>\n",
       "      <td>-0.588246</td>\n",
       "      <td>0.377448</td>\n",
       "      <td>0.503829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.538177</td>\n",
       "      <td>0.652968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.131064</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>-0.210410</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.162468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>0.799281</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>-0.772676</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>0.853236</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>-0.783172</td>\n",
       "      <td>0.771454</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785476</td>\n",
       "      <td>0.801417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.877603</td>\n",
       "      <td>0.792695</td>\n",
       "      <td>-0.805326</td>\n",
       "      <td>0.866959</td>\n",
       "      <td>0.943912</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>-0.775187</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.538177</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.785476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>0.890368</td>\n",
       "      <td>0.796074</td>\n",
       "      <td>-0.909881</td>\n",
       "      <td>0.856565</td>\n",
       "      <td>0.958921</td>\n",
       "      <td>0.923472</td>\n",
       "      <td>0.872165</td>\n",
       "      <td>-0.911791</td>\n",
       "      <td>0.765360</td>\n",
       "      <td>0.730178</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.801417</td>\n",
       "      <td>0.881241</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       rank  artist_popularity  followers  popularity  \\\n",
       "rank               1.000000           0.705908  -0.796614    0.770435   \n",
       "artist_popularity  0.705908           1.000000  -0.581229    0.941691   \n",
       "followers         -0.796614          -0.581229   1.000000   -0.702855   \n",
       "popularity         0.770435           0.941691  -0.702855    1.000000   \n",
       "danceability       0.907101           0.767426  -0.899792    0.848601   \n",
       "energy             0.947498           0.771203  -0.802854    0.835431   \n",
       "key                0.860511           0.676481  -0.831456    0.752888   \n",
       "loudness          -0.763978          -0.621750   0.907869   -0.711136   \n",
       "mode               0.744715           0.471224  -0.707081    0.548270   \n",
       "speechiness        0.630041           0.619311  -0.702654    0.707142   \n",
       "acousticness       0.694758           0.529364  -0.628552    0.618129   \n",
       "instrumentalness   0.138772           0.025657  -0.069452    0.012811   \n",
       "liveness           0.799281           0.541195  -0.772676    0.644572   \n",
       "valence            0.877603           0.792695  -0.805326    0.866959   \n",
       "tempo              0.890368           0.796074  -0.909881    0.856565   \n",
       "\n",
       "                   danceability    energy       key  loudness      mode  \\\n",
       "rank                   0.907101  0.947498  0.860511 -0.763978  0.744715   \n",
       "artist_popularity      0.767426  0.771203  0.676481 -0.621750  0.471224   \n",
       "followers             -0.899792 -0.802854 -0.831456  0.907869 -0.707081   \n",
       "popularity             0.848601  0.835431  0.752888 -0.711136  0.548270   \n",
       "danceability           1.000000  0.953093  0.886522 -0.907028  0.776749   \n",
       "energy                 0.953093  1.000000  0.857838 -0.800408  0.785333   \n",
       "key                    0.886522  0.857838  1.000000 -0.867666  0.706227   \n",
       "loudness              -0.907028 -0.800408 -0.867666  1.000000 -0.765798   \n",
       "mode                   0.776749  0.785333  0.706227 -0.765798  1.000000   \n",
       "speechiness            0.803351  0.729046  0.657342 -0.783093  0.593416   \n",
       "acousticness           0.592107  0.594591  0.703256 -0.588246  0.377448   \n",
       "instrumentalness       0.131064  0.141285  0.098196 -0.210410  0.006030   \n",
       "liveness               0.857685  0.853236  0.767902 -0.783172  0.771454   \n",
       "valence                0.943912  0.939267  0.828896 -0.775187  0.717171   \n",
       "tempo                  0.958921  0.923472  0.872165 -0.911791  0.765360   \n",
       "\n",
       "                   speechiness  acousticness  instrumentalness  liveness  \\\n",
       "rank                  0.630041      0.694758          0.138772  0.799281   \n",
       "artist_popularity     0.619311      0.529364          0.025657  0.541195   \n",
       "followers            -0.702654     -0.628552         -0.069452 -0.772676   \n",
       "popularity            0.707142      0.618129          0.012811  0.644572   \n",
       "danceability          0.803351      0.592107          0.131064  0.857685   \n",
       "energy                0.729046      0.594591          0.141285  0.853236   \n",
       "key                   0.657342      0.703256          0.098196  0.767902   \n",
       "loudness             -0.783093     -0.588246         -0.210410 -0.783172   \n",
       "mode                  0.593416      0.377448          0.006030  0.771454   \n",
       "speechiness           1.000000      0.503829          0.023720  0.763651   \n",
       "acousticness          0.503829      1.000000          0.010102  0.508264   \n",
       "instrumentalness      0.023720      0.010102          1.000000  0.049136   \n",
       "liveness              0.763651      0.508264          0.049136  1.000000   \n",
       "valence               0.790136      0.538177          0.033807  0.785476   \n",
       "tempo                 0.730178      0.652968          0.162468  0.801417   \n",
       "\n",
       "                    valence     tempo  \n",
       "rank               0.877603  0.890368  \n",
       "artist_popularity  0.792695  0.796074  \n",
       "followers         -0.805326 -0.909881  \n",
       "popularity         0.866959  0.856565  \n",
       "danceability       0.943912  0.958921  \n",
       "energy             0.939267  0.923472  \n",
       "key                0.828896  0.872165  \n",
       "loudness          -0.775187 -0.911791  \n",
       "mode               0.717171  0.765360  \n",
       "speechiness        0.790136  0.730178  \n",
       "acousticness       0.538177  0.652968  \n",
       "instrumentalness   0.033807  0.162468  \n",
       "liveness           0.785476  0.801417  \n",
       "valence            1.000000  0.881241  \n",
       "tempo              0.881241  1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_zs.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. From the heat map of correlation matrix, it is obvious that \"followers\" and \"loudness\" are negative relationship to rank, and \"energy\", \"tempo\",\"valence\" are significant to rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15011f45c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15011fe8588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "cmap = sns.diverging_palette(0, 255, sep=1, n=256, as_cmap=True)\n",
    "sns.heatmap(data_zs.corr(), cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE: Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use RFE(recursive feature elimination) and heatmap to select the variable which we want to use them to do linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True  True False  True  True  True  True  True  True\n",
      "  True False]\n",
      "[2 4 1 1 1 3 1 1 1 1 1 1 1 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logreg = LinearRegression()\n",
    "rfe = RFE(logreg, 10)\n",
    "X = ['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness'\n",
    ",'instrumentalness','liveness','valence','tempo']\n",
    "y = ['rank']\n",
    "rfe = rfe.fit(data_zs[X], data_zs[y] )\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 variables are chosen by RFE, such as \"popularity\", \"danceability\", \"energy\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>rank</td>       <th>  R-squared:         </th>  <td>   0.952</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.952</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.226e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 27 Apr 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:31:29</td>     <th>  Log-Likelihood:    </th>  <td>  23824.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6237</td>      <th>  AIC:               </th> <td>-4.763e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6226</td>      <th>  BIC:               </th> <td>-4.755e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 8.294e-05</td> <td> 6.78e-05</td> <td>    1.223</td> <td> 0.221</td> <td>   -5e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>   -0.5476</td> <td>    0.021</td> <td>  -26.554</td> <td> 0.000</td> <td>   -0.588</td> <td>   -0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>   56.7904</td> <td>    2.094</td> <td>   27.117</td> <td> 0.000</td> <td>   52.685</td> <td>   60.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>           <td>   55.3036</td> <td>    1.547</td> <td>   35.738</td> <td> 0.000</td> <td>   52.270</td> <td>   58.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>    1.9943</td> <td>    0.101</td> <td>   19.742</td> <td> 0.000</td> <td>    1.796</td> <td>    2.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>    7.5297</td> <td>    0.558</td> <td>   13.496</td> <td> 0.000</td> <td>    6.436</td> <td>    8.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>  -77.7716</td> <td>    2.478</td> <td>  -31.388</td> <td> 0.000</td> <td>  -82.629</td> <td>  -72.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>   66.4199</td> <td>    0.942</td> <td>   70.479</td> <td> 0.000</td> <td>   64.572</td> <td>   68.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>   17.0909</td> <td>    1.760</td> <td>    9.709</td> <td> 0.000</td> <td>   13.640</td> <td>   20.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>   -2.8278</td> <td>    1.913</td> <td>   -1.478</td> <td> 0.139</td> <td>   -6.579</td> <td>    0.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>    9.0594</td> <td>    1.368</td> <td>    6.623</td> <td> 0.000</td> <td>    6.378</td> <td>   11.741</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4645.281</td> <th>  Durbin-Watson:     </th>   <td>   1.999</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14857327.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.952</td>  <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>242.073</td> <th>  Cond. No.          </th>   <td>4.40e+04</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   rank   R-squared:                       0.952\n",
       "Model:                            OLS   Adj. R-squared:                  0.952\n",
       "Method:                 Least Squares   F-statistic:                 1.226e+04\n",
       "Date:                Fri, 27 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:31:29   Log-Likelihood:                 23824.\n",
       "No. Observations:                6237   AIC:                        -4.763e+04\n",
       "Df Residuals:                    6226   BIC:                        -4.755e+04\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             8.294e-05   6.78e-05      1.223      0.221      -5e-05       0.000\n",
       "popularity          -0.5476      0.021    -26.554      0.000      -0.588      -0.507\n",
       "danceability        56.7904      2.094     27.117      0.000      52.685      60.896\n",
       "energy              55.3036      1.547     35.738      0.000      52.270      58.337\n",
       "loudness             1.9943      0.101     19.742      0.000       1.796       2.192\n",
       "mode                 7.5297      0.558     13.496      0.000       6.436       8.623\n",
       "speechiness        -77.7716      2.478    -31.388      0.000     -82.629     -72.914\n",
       "acousticness        66.4199      0.942     70.479      0.000      64.572      68.267\n",
       "instrumentalness    17.0909      1.760      9.709      0.000      13.640      20.542\n",
       "liveness            -2.8278      1.913     -1.478      0.139      -6.579       0.923\n",
       "valence              9.0594      1.368      6.623      0.000       6.378      11.741\n",
       "==============================================================================\n",
       "Omnibus:                     4645.281   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         14857327.439\n",
       "Skew:                           1.952   Prob(JB):                         0.00\n",
       "Kurtosis:                     242.073   Cond. No.                     4.40e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.4e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_zs[[\"popularity\",\"danceability\",'energy','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence']]\n",
    "X = sm.add_constant(x)\n",
    "y = data_zs[\"rank\"]\n",
    "model = sm.OLS(y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value: In statistical hypothesis testing, the p-value or probability value or asymptotic significance is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two compared groups) would be the same as or of greater magnitude than the actual observed results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add one variable accroding to the heat map, we can find that besides \"liveness\", all other feature's p-value are all lower than 0.05, which means they are significant to rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>rank</td>       <th>  R-squared:         </th>  <td>   0.953</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.952</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.136e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 27 Apr 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:31:29</td>     <th>  Log-Likelihood:    </th>  <td>  23879.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6237</td>      <th>  AIC:               </th> <td>-4.773e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6225</td>      <th>  BIC:               </th> <td>-4.765e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.0431</td> <td>    0.008</td> <td>   -5.462</td> <td> 0.000</td> <td>   -0.059</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artist_popularity</th> <td>    0.3096</td> <td>    0.045</td> <td>    6.944</td> <td> 0.000</td> <td>    0.222</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>followers</th>         <td>    0.0431</td> <td>    0.008</td> <td>    5.464</td> <td> 0.000</td> <td>    0.028</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>        <td>   -0.7865</td> <td>    0.034</td> <td>  -22.928</td> <td> 0.000</td> <td>   -0.854</td> <td>   -0.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>      <td>   61.5355</td> <td>    2.137</td> <td>   28.801</td> <td> 0.000</td> <td>   57.347</td> <td>   65.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>            <td>   51.0012</td> <td>    1.481</td> <td>   34.428</td> <td> 0.000</td> <td>   48.097</td> <td>   53.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>          <td>    1.7610</td> <td>    0.111</td> <td>   15.893</td> <td> 0.000</td> <td>    1.544</td> <td>    1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>              <td>    7.4794</td> <td>    0.565</td> <td>   13.246</td> <td> 0.000</td> <td>    6.373</td> <td>    8.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>       <td>  -83.2583</td> <td>    2.401</td> <td>  -34.679</td> <td> 0.000</td> <td>  -87.965</td> <td>  -78.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>      <td>   69.8124</td> <td>    0.995</td> <td>   70.177</td> <td> 0.000</td> <td>   67.862</td> <td>   71.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th>  <td>   14.9734</td> <td>    1.798</td> <td>    8.327</td> <td> 0.000</td> <td>   11.448</td> <td>   18.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>           <td>   11.9002</td> <td>    1.296</td> <td>    9.183</td> <td> 0.000</td> <td>    9.360</td> <td>   14.440</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4732.546</td> <th>  Durbin-Watson:     </th>   <td>   1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>16243740.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.017</td>  <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>252.979</td> <th>  Cond. No.          </th>   <td>5.39e+04</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   rank   R-squared:                       0.953\n",
       "Model:                            OLS   Adj. R-squared:                  0.952\n",
       "Method:                 Least Squares   F-statistic:                 1.136e+04\n",
       "Date:                Fri, 27 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:31:29   Log-Likelihood:                 23879.\n",
       "No. Observations:                6237   AIC:                        -4.773e+04\n",
       "Df Residuals:                    6225   BIC:                        -4.765e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.0431      0.008     -5.462      0.000      -0.059      -0.028\n",
       "artist_popularity     0.3096      0.045      6.944      0.000       0.222       0.397\n",
       "followers             0.0431      0.008      5.464      0.000       0.028       0.059\n",
       "popularity           -0.7865      0.034    -22.928      0.000      -0.854      -0.719\n",
       "danceability         61.5355      2.137     28.801      0.000      57.347      65.724\n",
       "energy               51.0012      1.481     34.428      0.000      48.097      53.905\n",
       "loudness              1.7610      0.111     15.893      0.000       1.544       1.978\n",
       "mode                  7.4794      0.565     13.246      0.000       6.373       8.586\n",
       "speechiness         -83.2583      2.401    -34.679      0.000     -87.965     -78.552\n",
       "acousticness         69.8124      0.995     70.177      0.000      67.862      71.763\n",
       "instrumentalness     14.9734      1.798      8.327      0.000      11.448      18.499\n",
       "valence              11.9002      1.296      9.183      0.000       9.360      14.440\n",
       "==============================================================================\n",
       "Omnibus:                     4732.546   Durbin-Watson:                   1.998\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         16243740.217\n",
       "Skew:                           2.017   Prob(JB):                         0.00\n",
       "Kurtosis:                     252.979   Cond. No.                     5.39e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.39e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_zs[['artist_popularity','followers','popularity','danceability','energy','loudness','mode','speechiness','acousticness'\n",
    ",'instrumentalness','valence']]\n",
    "X = sm.add_constant(x)\n",
    "y = data_zs[\"rank\"]\n",
    "model = sm.OLS(y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is primarily a way of measuring the predictive performance of a statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91060056 0.89483569 0.65432276 0.88873608 0.96179543 0.96873287]\n",
      "Accuracy: 0.88 (+/- 0.21)\n"
     ]
    }
   ],
   "source": [
    "x = data_zs[['artist_popularity','followers','popularity','danceability','energy','loudness','mode','speechiness','acousticness'\n",
    ",'instrumentalness','valence']]\n",
    "y = data_zs[\"rank\"]\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "clf = LinearRegression()\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=6)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by using Cross-validation, the Accuracy of this model is 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two labels(\"Yes\" and \"No\") in our model, which we can use Logistic Regression to predict whether the track can be hit or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to build Logistic Regression is to replace \"Yes\" or \"No\" to 1 and 0,then what we need to do is to scaler the data, which is significant to build Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Full_data.csv\")\n",
    "data.hit.replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "data_zs = max_abs_scaler.fit_transform(data[['artist_popularity','followers','popularity','energy','loudness','mode','tempo']])\n",
    "data_zs = pd.DataFrame(data_zs)\n",
    "data_zs.columns = ['artist_popularity','followers','popularity','energy','loudness','mode','tempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.854395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455823</td>\n",
       "      <td>-0.430979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.449799</td>\n",
       "      <td>-0.286714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.540994</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.862450</td>\n",
       "      <td>-0.221531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.146706</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>-0.196820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.491202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.154408</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.537149</td>\n",
       "      <td>-0.301476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist_popularity  followers  popularity    energy  loudness  mode  \\\n",
       "0               1.00   0.854395    1.000000  0.455823 -0.430979   1.0   \n",
       "1               0.96   0.999971    0.959596  0.449799 -0.286714   1.0   \n",
       "2               0.91   0.540994    0.959596  0.862450 -0.221531   0.0   \n",
       "3               0.91   0.146706    0.979798  0.525100 -0.196820   1.0   \n",
       "4               0.96   0.154408    0.909091  0.537149 -0.301476   0.0   \n",
       "\n",
       "      tempo  \n",
       "0  0.361051  \n",
       "1  0.444705  \n",
       "2  0.491796  \n",
       "3  0.491202  \n",
       "4  0.747479  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_zs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582265\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>hit</td>       <th>  No. Observations:  </th>   <td>  6237</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  6230</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 27 Apr 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.09279</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>14:26:02</td>     <th>  Log-Likelihood:    </th>  <td> -3631.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -4003.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>3.328e-157</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artist_popularity</th> <td>   -3.0881</td> <td>    0.286</td> <td>  -10.784</td> <td> 0.000</td> <td>   -3.649</td> <td>   -2.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>followers</th>         <td>    1.6158</td> <td>    0.179</td> <td>    9.044</td> <td> 0.000</td> <td>    1.266</td> <td>    1.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>        <td>    6.0623</td> <td>    0.276</td> <td>   21.999</td> <td> 0.000</td> <td>    5.522</td> <td>    6.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>energy</th>            <td>   -0.9523</td> <td>    0.167</td> <td>   -5.716</td> <td> 0.000</td> <td>   -1.279</td> <td>   -0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>          <td>    2.7656</td> <td>    0.309</td> <td>    8.946</td> <td> 0.000</td> <td>    2.160</td> <td>    3.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>              <td>   -0.1689</td> <td>    0.060</td> <td>   -2.824</td> <td> 0.005</td> <td>   -0.286</td> <td>   -0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>             <td>   -1.2393</td> <td>    0.205</td> <td>   -6.043</td> <td> 0.000</td> <td>   -1.641</td> <td>   -0.837</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    hit   No. Observations:                 6237\n",
       "Model:                          Logit   Df Residuals:                     6230\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Fri, 27 Apr 2018   Pseudo R-squ.:                 0.09279\n",
       "Time:                        14:26:02   Log-Likelihood:                -3631.6\n",
       "converged:                       True   LL-Null:                       -4003.0\n",
       "                                        LLR p-value:                3.328e-157\n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "artist_popularity    -3.0881      0.286    -10.784      0.000      -3.649      -2.527\n",
       "followers             1.6158      0.179      9.044      0.000       1.266       1.966\n",
       "popularity            6.0623      0.276     21.999      0.000       5.522       6.602\n",
       "energy               -0.9523      0.167     -5.716      0.000      -1.279      -0.626\n",
       "loudness              2.7656      0.309      8.946      0.000       2.160       3.371\n",
       "mode                 -0.1689      0.060     -2.824      0.005      -0.286      -0.052\n",
       "tempo                -1.2393      0.205     -6.043      0.000      -1.641      -0.837\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_model =sm.Logit(data['hit'],data_zs).fit()\n",
    "L_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary, we can see that all variables which we selected are significant for predict whether the track can be hit or not sicne all the p-value are lower than 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross-validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64       0.688      0.664      0.696      0.672      0.688\n",
      " 0.736      0.656      0.69354839 0.69354839]\n",
      "Accuracy: 0.68 (+/- 0.05)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['popularity', 'danceability', 'energy', 'key', 'loudness', 'valence', 'tempo']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_zs\n",
    "y = data[\"hit\"]\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=123)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "['popularity','danceability','energy','key','loudness','valence','tempo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy of this model is 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[730  90]\n",
      " [319 109]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Confusion Matrix, we can find that the TP(true positive) is 730, the TN(true negative) is 319, so the accuracy of predicting the song can be hit is 69.5%, and the FP(false positive) is 90, FN(false negative) is 109, so the accuracy of the song can not be hit is 54.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15016a26eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC plot shows that the area is only about 0.57, and the blue line is close to the diagonal， so the Logistic Regression do not fit very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning model（CNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increased the accuracy of our model, We want to use CNN to predict whether a given track can reach top 30 by using their artist's popularity, followers, and audion feature, audio analysis information which we get on spotify api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>God's Plan</td>\n",
       "      <td>Drake</td>\n",
       "      <td>2XW4DbS6NddZxRPm5rMCeY</td>\n",
       "      <td>3TVXtAsR1Inumwj472S9r4</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "      <td>18311322</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.344</td>\n",
       "      <td>77.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "      <td>6eUKZXaKkcviH0Ku9w2n3V</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>21431294</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.168</td>\n",
       "      <td>95.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Finesse</td>\n",
       "      <td>Bruno Mars &amp; Cardi B</td>\n",
       "      <td>3Vo4wInECJQuz9BIBMOu8i</td>\n",
       "      <td>0du5cEVh5yTK9QJze8zA0C</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>11594549</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.926</td>\n",
       "      <td>105.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Havana</td>\n",
       "      <td>Camila Cabello Featuring Young Thug</td>\n",
       "      <td>1rfofaqEpACxVEHIZBJe6W</td>\n",
       "      <td>4nDoRrQiYLoBzwC5BhVJzF</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91</td>\n",
       "      <td>3144195</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.394</td>\n",
       "      <td>104.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockstar</td>\n",
       "      <td>Post Malone Featuring 21 Savage</td>\n",
       "      <td>0OAAAdiHJKa2wlCKqaYXV7</td>\n",
       "      <td>246dkjvS1zLTtiykXe5h60</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>96</td>\n",
       "      <td>3309267</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.637</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.127</td>\n",
       "      <td>159.764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       title                               artist  \\\n",
       "0   1  God's Plan                                Drake   \n",
       "1   2     Perfect                           Ed Sheeran   \n",
       "2   3     Finesse                 Bruno Mars & Cardi B   \n",
       "3   4      Havana  Camila Cabello Featuring Young Thug   \n",
       "4   5    Rockstar      Post Malone Featuring 21 Savage   \n",
       "\n",
       "                 track_id               artist_id  rank  hit  \\\n",
       "0  2XW4DbS6NddZxRPm5rMCeY  3TVXtAsR1Inumwj472S9r4     1  Yes   \n",
       "1  0tgVpDi06FyKpA1z0VMD4v  6eUKZXaKkcviH0Ku9w2n3V     1  Yes   \n",
       "2  3Vo4wInECJQuz9BIBMOu8i  0du5cEVh5yTK9QJze8zA0C     3  Yes   \n",
       "3  1rfofaqEpACxVEHIZBJe6W  4nDoRrQiYLoBzwC5BhVJzF     1  Yes   \n",
       "4  0OAAAdiHJKa2wlCKqaYXV7  246dkjvS1zLTtiykXe5h60     1  Yes   \n",
       "\n",
       "   artist_popularity  followers  popularity   ...     energy  key  loudness  \\\n",
       "0                100   18311322          99   ...      0.454    7    -9.488   \n",
       "1                 96   21431294          95   ...      0.448    8    -6.312   \n",
       "2                 91   11594549          95   ...      0.859    5    -4.877   \n",
       "3                 91    3144195          97   ...      0.523    2    -4.333   \n",
       "4                 96    3309267          90   ...      0.535    5    -6.637   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0     1       0.0963        0.0244          0.000056    0.4980    0.344   \n",
       "1     1       0.0232        0.1630          0.000000    0.1060    0.168   \n",
       "2     0       0.0996        0.0185          0.000000    0.0215    0.926   \n",
       "3     1       0.0300        0.1840          0.000036    0.1320    0.394   \n",
       "4     0       0.0776        0.1300          0.000130    0.1430    0.127   \n",
       "\n",
       "     tempo  \n",
       "0   77.170  \n",
       "1   95.050  \n",
       "2  105.115  \n",
       "3  104.988  \n",
       "4  159.764  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6237 entries, 0 to 6236\n",
      "Data columns (total 21 columns):\n",
      "id                   6237 non-null int64\n",
      "title                6237 non-null object\n",
      "artist               6237 non-null object\n",
      "track_id             6237 non-null object\n",
      "artist_id            6237 non-null object\n",
      "rank                 6237 non-null int64\n",
      "hit                  6237 non-null object\n",
      "artist_popularity    6237 non-null int64\n",
      "followers            6237 non-null int64\n",
      "popularity           6237 non-null int64\n",
      "danceability         6237 non-null float64\n",
      "energy               6237 non-null float64\n",
      "key                  6237 non-null int64\n",
      "loudness             6237 non-null float64\n",
      "mode                 6237 non-null int64\n",
      "speechiness          6237 non-null float64\n",
      "acousticness         6237 non-null float64\n",
      "instrumentalness     6237 non-null float64\n",
      "liveness             6237 non-null float64\n",
      "valence              6237 non-null float64\n",
      "tempo                6237 non-null float64\n",
      "dtypes: float64(9), int64(7), object(5)\n",
      "memory usage: 1023.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rank</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6.237000e+03</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "      <td>6237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3515.977233</td>\n",
       "      <td>46.833093</td>\n",
       "      <td>72.287959</td>\n",
       "      <td>2.818100e+06</td>\n",
       "      <td>57.089145</td>\n",
       "      <td>0.625627</td>\n",
       "      <td>0.698398</td>\n",
       "      <td>5.255892</td>\n",
       "      <td>-5.833668</td>\n",
       "      <td>0.678211</td>\n",
       "      <td>0.099974</td>\n",
       "      <td>0.161373</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.185531</td>\n",
       "      <td>0.518813</td>\n",
       "      <td>121.944461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2095.961730</td>\n",
       "      <td>29.967952</td>\n",
       "      <td>14.870323</td>\n",
       "      <td>4.272673e+06</td>\n",
       "      <td>14.185772</td>\n",
       "      <td>0.144532</td>\n",
       "      <td>0.170741</td>\n",
       "      <td>3.563445</td>\n",
       "      <td>2.146704</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.101689</td>\n",
       "      <td>0.203845</td>\n",
       "      <td>0.066063</td>\n",
       "      <td>0.141792</td>\n",
       "      <td>0.223985</td>\n",
       "      <td>28.966124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22.015000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>51.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1683.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>3.556930e+05</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-6.965000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>98.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3454.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.083347e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.531000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>120.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5329.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.019306e+06</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.341000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>140.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7234.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.143193e+07</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-0.463000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         rank  artist_popularity     followers   popularity  \\\n",
       "count  6237.000000  6237.000000        6237.000000  6.237000e+03  6237.000000   \n",
       "mean   3515.977233    46.833093          72.287959  2.818100e+06    57.089145   \n",
       "std    2095.961730    29.967952          14.870323  4.272673e+06    14.185772   \n",
       "min       1.000000     1.000000           0.000000  2.000000e+00     0.000000   \n",
       "25%    1683.000000    20.000000          64.000000  3.556930e+05    48.000000   \n",
       "50%    3454.000000    46.000000          75.000000  1.083347e+06    58.000000   \n",
       "75%    5329.000000    73.000000          82.000000  3.019306e+06    67.000000   \n",
       "max    7234.000000   100.000000         100.000000  2.143193e+07    99.000000   \n",
       "\n",
       "       danceability       energy          key     loudness         mode  \\\n",
       "count   6237.000000  6237.000000  6237.000000  6237.000000  6237.000000   \n",
       "mean       0.625627     0.698398     5.255892    -5.833668     0.678211   \n",
       "std        0.144532     0.170741     3.563445     2.146704     0.467200   \n",
       "min        0.113000     0.056500     0.000000   -22.015000     0.000000   \n",
       "25%        0.529000     0.587000     2.000000    -6.965000     0.000000   \n",
       "50%        0.628000     0.723000     5.000000    -5.531000     1.000000   \n",
       "75%        0.725000     0.832000     8.000000    -4.341000     1.000000   \n",
       "max        0.994000     0.996000    11.000000    -0.463000     1.000000   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness     liveness      valence  \\\n",
       "count  6237.000000   6237.000000       6237.000000  6237.000000  6237.000000   \n",
       "mean      0.099974      0.161373          0.008434     0.185531     0.518813   \n",
       "std       0.101689      0.203845          0.066063     0.141792     0.223985   \n",
       "min       0.022400      0.000003          0.000000     0.016400     0.034900   \n",
       "25%       0.035100      0.017300          0.000000     0.094400     0.338000   \n",
       "50%       0.052900      0.072600          0.000000     0.129000     0.520000   \n",
       "75%       0.120000      0.231000          0.000017     0.243000     0.694000   \n",
       "max       0.765000      0.986000          0.982000     0.987000     0.976000   \n",
       "\n",
       "             tempo  \n",
       "count  6237.000000  \n",
       "mean    121.944461  \n",
       "std      28.966124  \n",
       "min      51.316000  \n",
       "25%      98.011000  \n",
       "50%     120.637000  \n",
       "75%     140.171000  \n",
       "max     213.737000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split our data into Train data(75%) and Test data(25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train ,x_test = train_test_split(data,test_size=0.25)  #split data into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>187</td>\n",
       "      <td>Transportin'</td>\n",
       "      <td>Kodak Black</td>\n",
       "      <td>1WIZiOuNO3woKfdlSK2gNn</td>\n",
       "      <td>46SHBwWsqBkxI7EeeBEQG7</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>87</td>\n",
       "      <td>2744896</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>0.03190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.257</td>\n",
       "      <td>94.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>457</td>\n",
       "      <td>Hard Times</td>\n",
       "      <td>Paramore</td>\n",
       "      <td>0w5Bdu51Ka25Pf3hojsKHh</td>\n",
       "      <td>74XFHRwlV6OrjEM0A2NCMF</td>\n",
       "      <td>90</td>\n",
       "      <td>No</td>\n",
       "      <td>80</td>\n",
       "      <td>2815000</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.379</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.00647</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.916</td>\n",
       "      <td>119.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>4193</td>\n",
       "      <td>When You Look Me In The Eyes</td>\n",
       "      <td>Jonas Brothers</td>\n",
       "      <td>6YnauKvhKMFOX63RwmiCwH</td>\n",
       "      <td>7gOdHgIoIKoe4i9Tta6qdD</td>\n",
       "      <td>25</td>\n",
       "      <td>Yes</td>\n",
       "      <td>66</td>\n",
       "      <td>941209</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.685</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.02690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.506</td>\n",
       "      <td>136.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>911</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Adele</td>\n",
       "      <td>0ENSn4fwAbCGeFGVUbXEU3</td>\n",
       "      <td>4dpARuHxo51G3z768sgnrY</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>85</td>\n",
       "      <td>8650583</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.33600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.289</td>\n",
       "      <td>157.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>6925</td>\n",
       "      <td>So Anxious</td>\n",
       "      <td>Ginuwine</td>\n",
       "      <td>2zbjlcLi1VQWqDNYdhyx1l</td>\n",
       "      <td>7r8RF1tN2A4CiGEplkp1oP</td>\n",
       "      <td>16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>70</td>\n",
       "      <td>817542</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395</td>\n",
       "      <td>6</td>\n",
       "      <td>-11.959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.661</td>\n",
       "      <td>96.085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         title          artist  \\\n",
       "173    187                  Transportin'     Kodak Black   \n",
       "423    457                    Hard Times        Paramore   \n",
       "3740  4193  When You Look Me In The Eyes  Jonas Brothers   \n",
       "843    911                         Hello           Adele   \n",
       "5983  6925                    So Anxious        Ginuwine   \n",
       "\n",
       "                    track_id               artist_id  rank  hit  \\\n",
       "173   1WIZiOuNO3woKfdlSK2gNn  46SHBwWsqBkxI7EeeBEQG7    46   No   \n",
       "423   0w5Bdu51Ka25Pf3hojsKHh  74XFHRwlV6OrjEM0A2NCMF    90   No   \n",
       "3740  6YnauKvhKMFOX63RwmiCwH  7gOdHgIoIKoe4i9Tta6qdD    25  Yes   \n",
       "843   0ENSn4fwAbCGeFGVUbXEU3  4dpARuHxo51G3z768sgnrY     1  Yes   \n",
       "5983  2zbjlcLi1VQWqDNYdhyx1l  7r8RF1tN2A4CiGEplkp1oP    16  Yes   \n",
       "\n",
       "      artist_popularity  followers  popularity   ...     energy  key  \\\n",
       "173                  87    2744896          81   ...      0.660   10   \n",
       "423                  80    2815000          75   ...      0.818    5   \n",
       "3740                 66     941209          57   ...      0.767    2   \n",
       "843                  85    8650583          69   ...      0.451    5   \n",
       "5983                 70     817542          60   ...      0.395    6   \n",
       "\n",
       "      loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "173     -4.860     0       0.0726       0.03190          0.000000    0.1030   \n",
       "423     -5.379     0       0.0334       0.00647          0.000005    0.0219   \n",
       "3740    -3.685     1       0.0337       0.02690          0.000000    0.2700   \n",
       "843     -6.095     0       0.0347       0.33600          0.000000    0.0872   \n",
       "5983   -11.959     0       0.0366       0.27900          0.000002    0.4680   \n",
       "\n",
       "      valence    tempo  \n",
       "173     0.257   94.053  \n",
       "423     0.916  119.965  \n",
       "3740    0.506  136.047  \n",
       "843     0.289  157.966  \n",
       "5983    0.661   96.085  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>hit</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>1671</td>\n",
       "      <td>Burn</td>\n",
       "      <td>Ellie Goulding</td>\n",
       "      <td>0xMd5bcWTbyXS7wPrBtZA6</td>\n",
       "      <td>0X2BH1fck6amBIoJhDVmmJ</td>\n",
       "      <td>13</td>\n",
       "      <td>Yes</td>\n",
       "      <td>81</td>\n",
       "      <td>4487371</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.29600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.323</td>\n",
       "      <td>87.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>3018</td>\n",
       "      <td>I Like It</td>\n",
       "      <td>Enrique Iglesias Featuring Pitbull</td>\n",
       "      <td>4nVyHATevhl5RC6Qmoko5H</td>\n",
       "      <td>7qG3b048QCHVRO5Pv1T5lw</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>84</td>\n",
       "      <td>5710383</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.02530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.730</td>\n",
       "      <td>129.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>4794</td>\n",
       "      <td>A Public Affair</td>\n",
       "      <td>Jessica Simpson</td>\n",
       "      <td>1gFMesXcHHtIUhwzxYFOtz</td>\n",
       "      <td>2tFN9ubMXEhdAQvdQxcsma</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>54</td>\n",
       "      <td>338802</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860</td>\n",
       "      <td>11</td>\n",
       "      <td>-3.878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.00525</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.835</td>\n",
       "      <td>124.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>2630</td>\n",
       "      <td>Cost Of Livin'</td>\n",
       "      <td>Ronnie Dunn</td>\n",
       "      <td>64SgS8XyiJWFSz3YoMyYcE</td>\n",
       "      <td>1iLuTmIgxCEiEllDSWkeIy</td>\n",
       "      <td>86</td>\n",
       "      <td>No</td>\n",
       "      <td>53</td>\n",
       "      <td>31417</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.89500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.283</td>\n",
       "      <td>148.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6025</th>\n",
       "      <td>6976</td>\n",
       "      <td>I Will Remember You (Live)</td>\n",
       "      <td>Sarah McLachlan</td>\n",
       "      <td>7fEfH6kqMu1Gjes7yi5BHO</td>\n",
       "      <td>4NgNsOXSwIzXlUIJcpnNUp</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>63</td>\n",
       "      <td>301721</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394</td>\n",
       "      <td>9</td>\n",
       "      <td>-11.749</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.51500</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>0.355</td>\n",
       "      <td>151.799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                       title                              artist  \\\n",
       "1548  1671                        Burn                      Ellie Goulding   \n",
       "2739  3018                   I Like It  Enrique Iglesias Featuring Pitbull   \n",
       "4232  4794             A Public Affair                     Jessica Simpson   \n",
       "2401  2630              Cost Of Livin'                         Ronnie Dunn   \n",
       "6025  6976  I Will Remember You (Live)                     Sarah McLachlan   \n",
       "\n",
       "                    track_id               artist_id  rank  hit  \\\n",
       "1548  0xMd5bcWTbyXS7wPrBtZA6  0X2BH1fck6amBIoJhDVmmJ    13  Yes   \n",
       "2739  4nVyHATevhl5RC6Qmoko5H  7qG3b048QCHVRO5Pv1T5lw     4  Yes   \n",
       "4232  1gFMesXcHHtIUhwzxYFOtz  2tFN9ubMXEhdAQvdQxcsma    14  Yes   \n",
       "2401  64SgS8XyiJWFSz3YoMyYcE  1iLuTmIgxCEiEllDSWkeIy    86   No   \n",
       "6025  7fEfH6kqMu1Gjes7yi5BHO  4NgNsOXSwIzXlUIJcpnNUp    14  Yes   \n",
       "\n",
       "      artist_popularity  followers  popularity   ...     energy  key  \\\n",
       "1548                 81    4487371          68   ...      0.772   10   \n",
       "2739                 84    5710383          63   ...      0.935   10   \n",
       "4232                 54     338802          41   ...      0.860   11   \n",
       "2401                 53      31417          49   ...      0.290    4   \n",
       "6025                 63     301721          36   ...      0.394    9   \n",
       "\n",
       "      loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "1548    -5.031     0       0.0476       0.29600          0.000000    0.1010   \n",
       "2739    -2.739     0       0.1070       0.02530          0.000000    0.0653   \n",
       "4232    -3.878     0       0.0339       0.00525          0.000192    0.4040   \n",
       "2401    -7.896     1       0.0349       0.89500          0.000000    0.0959   \n",
       "6025   -11.749     1       0.0292       0.51500          0.000005    0.9780   \n",
       "\n",
       "      valence    tempo  \n",
       "1548    0.323   87.038  \n",
       "2739    0.730  129.048  \n",
       "4232    0.835  124.052  \n",
       "2401    0.283  148.393  \n",
       "6025    0.355  151.799  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('Train_data.csv', index = False)\n",
    "x_test.to_csv(\"Test_data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build CNN model, we need to initialise weights and bias for the filter, the weight_variable is truncated_normal. and the bias_variable is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name = \"ConV\"):    \n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.1) )  \n",
    "def bias_variable(shape,name = \"Fc\"):    \n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the filter input shape for tf.nn.conv_2d, the strides is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):    \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs the max pooling on the input, the kernel size is 2*2, and the strides is 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):    \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the read data function, we choose 14 features of each track to predict whether the track is hit or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    path = file_name + \".csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.hit.replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "    \n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 14\n",
    "    n_channels = 14\n",
    "    \n",
    "    \"labels\"\n",
    "    labels = df[\"hit\"]\n",
    "    \n",
    "    # Initiate array\n",
    "    X = np.zeros((len(labels), n_steps, n_channels))\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        for j in range(0, n_steps):\n",
    "            a = df.loc[i][7:21].tolist()\n",
    "            tmp = []\n",
    "            tmp = tmp + a\n",
    "            tmp = np.asarray(tmp)\n",
    "            X[i,j,:] = tmp\n",
    "        if i%100 == 0:\n",
    "            print(i)  \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "    # Standardize train and test\n",
    "    X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "    X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use one hot to label 1 and 0, this function Return a 2-D array with ones on the diagonal and zeros elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 2):\n",
    "    expansion = np.eye(n_class) #Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
    "    y = expansion[:, labels-1].T\n",
    "    assert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(train_data, train_target, batch_size):  \n",
    "    idx = [ i for i in range(0,len(train_target)) ]  \n",
    "    np.random.shuffle(idx);  \n",
    "    batch_data = []; batch_target = [];  \n",
    "    for i in range(0,batch_size):  \n",
    "        batch_data.append(train_data[idx[i]]);  \n",
    "        batch_target.append(train_target[idx[i]])  \n",
    "    return batch_data, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "x_train, labels_train = read_data(\"Train_data\") # train\n",
    "x_test, labels_test = read_data(\"Test_data\") # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: N = 4677, steps = 14, channels = 14\n",
      "Test data shape: N = 1560, steps = 14, channels = 14\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data shape: N = {:d}, steps = {:d}, channels = {:d}\".format(x_train.shape[0],\n",
    "                                                                             x_train.shape[1],\n",
    "                                                                             x_train.shape[2]))\n",
    "print (\"Test data shape: N = {:d}, steps = {:d}, channels = {:d}\".format(x_test.shape[0],\n",
    "                                                                   \n",
    "                                                                         x_test.shape[1],\n",
    "                                                                         x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardize the training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_test = standardize(x_train, x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot label, One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = one_hot(labels_train)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train  \n",
    "train_target = y_tr  \n",
    "test_data = Y_test     \n",
    "test_target = y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 14  # seq_len = steps\n",
    "n_channels = 14 # feature\n",
    "n_classes = 2 # Yes or No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the training data placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep') # keep_probability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_num = 5000   \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Convolutional Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the filter size(2*2), feature map to 20, then define the bias size, which is the number of convolution kernel, use Relu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([3, 3, 1, 20],name = \"W\")     \n",
    "b_conv1 = bias_variable([20], name = \"b\")  \n",
    "x_image = tf.reshape(x, [-1, 14, 14, 1])  # reshape  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # relu activation function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output is [7,7,20,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool1 = max_pool_2x2(h_conv1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second conv2 kernal size is 3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([3, 3, 20, 40])    \n",
    "b_conv2 = bias_variable([40])    \n",
    "h_conv2 = tf.nn.relu( conv2d(h_pool1, W_conv2) + b_conv2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2 = max_pool_2x2(h_conv2) # 输出[4，4,40,100]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the second pooling layer, the output shape is [4,4,40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([4 * 4 * 40, 100], name =\"W\")    \n",
    "b_fc1 = bias_variable([100], name = \"b\")    \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*40])    \n",
    "h_fc1 = tf.nn.relu( tf.matmul(h_pool2_flat, W_fc1) + b_fc1 )     \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  #some hidden node weight is not kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([100, 2], name = \"W\")    \n",
    "b_fc2 = bias_variable([2], name = \"b\")    \n",
    "y_conv=tf.nn.softmax( tf.matmul(h_fc1_drop, W_fc2) + b_fc2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, training accuracy 0.280\n",
      "step 100, training accuracy 0.700\n",
      "step 200, training accuracy 0.700\n",
      "step 300, training accuracy 0.640\n",
      "step 400, training accuracy 0.840\n",
      "step 500, training accuracy 0.660\n",
      "step 600, training accuracy 0.760\n",
      "step 700, training accuracy 0.780\n",
      "step 800, training accuracy 0.720\n",
      "step 900, training accuracy 0.740\n",
      "step 1000, training accuracy 0.660\n",
      "step 1100, training accuracy 0.720\n",
      "step 1200, training accuracy 0.720\n",
      "step 1300, training accuracy 0.700\n",
      "step 1400, training accuracy 0.720\n",
      "step 1500, training accuracy 0.740\n",
      "step 1600, training accuracy 0.660\n",
      "step 1700, training accuracy 0.800\n",
      "step 1800, training accuracy 0.780\n",
      "step 1900, training accuracy 0.820\n",
      "step 2000, training accuracy 0.720\n",
      "step 2100, training accuracy 0.780\n",
      "step 2200, training accuracy 0.740\n",
      "step 2300, training accuracy 0.840\n",
      "step 2400, training accuracy 0.660\n",
      "step 2500, training accuracy 0.820\n",
      "step 2600, training accuracy 0.640\n",
      "step 2700, training accuracy 0.740\n",
      "step 2800, training accuracy 0.700\n",
      "step 2900, training accuracy 0.780\n",
      "step 3000, training accuracy 0.720\n",
      "step 3100, training accuracy 0.680\n",
      "step 3200, training accuracy 0.760\n",
      "step 3300, training accuracy 0.700\n",
      "step 3400, training accuracy 0.800\n",
      "step 3500, training accuracy 0.660\n",
      "step 3600, training accuracy 0.760\n",
      "step 3700, training accuracy 0.700\n",
      "step 3800, training accuracy 0.680\n",
      "step 3900, training accuracy 0.700\n",
      "step 4000, training accuracy 0.740\n",
      "step 4100, training accuracy 0.680\n",
      "step 4200, training accuracy 0.760\n",
      "step 4300, training accuracy 0.740\n",
      "step 4400, training accuracy 0.800\n",
      "step 4500, training accuracy 0.660\n",
      "step 4600, training accuracy 0.760\n",
      "step 4700, training accuracy 0.700\n",
      "step 4800, training accuracy 0.840\n",
      "step 4900, training accuracy 0.800\n",
      "Training finished\n",
      "test accuracy 0.717\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession() \n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv)) #cost function\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # Gradient estimation  \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))  \n",
    "\n",
    "probabilities=y_conv\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))   \n",
    "\n",
    "sess.run(tf.initialize_all_variables()) \n",
    "\n",
    "train_acc = []\n",
    "for i in range(epochs_num): \n",
    "\n",
    "    batch_data, batch_target = next_batch(train_data,train_target,batch_size) \n",
    "    \n",
    "    if i%100 == 0:    \n",
    "        train_accuracy = accuracy.eval(feed_dict={ x:batch_data, y: batch_target, keep_prob: 1.0} )    \n",
    "        print (\"step %d, training accuracy %.3f\"%(i, train_accuracy))   \n",
    "    if i%10 ==0:    \n",
    "        train_accuracy = accuracy.eval(feed_dict={ x:batch_data, y: batch_target, keep_prob: 1.0} )\n",
    "        \n",
    "    train_step.run(feed_dict={x: batch_data, y:batch_target, keep_prob: 0.5})   # keep_prob is 0.5  \n",
    "  \n",
    "print(\"Training finished\")    \n",
    "print(\"test accuracy %.3f\" % accuracy.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0}) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = correct_prediction.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False ...  True False False]\n"
     ]
    }
   ],
   "source": [
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pd.DataFrame(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1560 entries, 0 to 1559\n",
      "Data columns (total 1 columns):\n",
      "0    1560 non-null bool\n",
      "dtypes: bool(1)\n",
      "memory usage: 1.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.DataFrame(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target .to_csv(\"test_target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform one hot to True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for i in range(0,len(test_target)):\n",
    "    tmp = []\n",
    "    if test_target.loc[i][0] == 1:\n",
    "        tmp.append(False)\n",
    "    else:\n",
    "        tmp.append(True)\n",
    "    target.append(tmp)\n",
    "test_1 = pd.DataFrame(target)\n",
    "test_1.columns= ['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1560 entries, 0 to 1559\n",
      "Data columns (total 1 columns):\n",
      "hit    1560 non-null bool\n",
      "dtypes: bool(1)\n",
      "memory usage: 1.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_rf = confusion_matrix(test_1,test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[324 221]\n",
      " [118 897]]\n"
     ]
    }
   ],
   "source": [
    "print(cnf_rf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities=y_conv\n",
    "pro = probabilities.eval(feed_dict={ x: test_data, y:test_target , keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1501a95ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(test_1,pro[:,1] )\n",
    "fpr, tpr, thresholds = roc_curve(test_1, pro[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is 0.74, which is much better than 0.57."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Some part of code are copyed from (https://github.com/healthDataScience/deep-learning-HAR) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we select 15 features to predict the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"LSTM_data.csv\")\n",
    "data = data[['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode',\n",
    "        'speechiness','acousticness','instrumentalness','liveness','valence','tempo','days','hit']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use time series data, we choose the first 80000 rows data as training, the left 9000 rows data are chosen as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:80000].to_csv(\"Train_data_lstm.csv\",index = None)\n",
    "data[80000:len(data)].to_csv(\"Test_data_lstm.csv\",index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    path = file_name + \".csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['artist_popularity','followers','popularity','danceability','energy','key','loudness','mode',\n",
    "        'speechiness','acousticness','instrumentalness','liveness','valence','tempo','days','hit']]\n",
    "    df.hit.replace((\"Yes\", \"No\"), (1, 0), inplace=True)\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 1\n",
    "    n_channels = 15\n",
    "    \n",
    "    \"labels\"\n",
    "    labels = df[\"hit\"]\n",
    "    \n",
    "    # Initiate array\n",
    "    X = np.zeros((len(labels), n_steps, n_channels))\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        for j in range(0, n_steps):\n",
    "            #seg = segments[j]\n",
    "            a = df.loc[i][0:15].tolist() # 15 channels\n",
    "            tmp = []\n",
    "            tmp = tmp + a\n",
    "            tmp = np.asarray(tmp)\n",
    "            X[i,j,:] = tmp\n",
    "        if i%20000 == 0:\n",
    "            print(i)  \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "    # Standardize train and test\n",
    "    X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "    X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 2):\n",
    "    expansion = np.eye(n_class)  # generate diagonal matrix\n",
    "    y = expansion[:, labels-1].T  # Transpose the matrix\n",
    "    assert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x_train, labels_train = read_data(\"Train_data_lstm\") # train\n",
    "x_test, labels_test = read_data(\"Test_data_lstm\") # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the order of the data, so we trun of the shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
    "                                                shuffle=False,\n",
    "                                                stratify = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 45         # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 200       # Batch size\n",
    "seq_len = 1          # Number of steps\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 200\n",
    "\n",
    "# Fixed\n",
    "n_classes = 2\n",
    "n_channels = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "   \n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # （N, seq_len, channels ）reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=tf.nn.relu) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)  # Add basic LTSM Cell\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers) # Add multipul RNN Cell\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a recurrent neural network specified by RNNCell cell, we only need the last output tensor to pass into a classifier. To avoid gradient explosion, we have three steps: 1.Compute the gradients with compute_gradients() 2.Process the gradients as you wish 3.Apply the processed gradients with apply_gradients()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-123-8c3c177c07e1>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    #Creates a recurrent neural network specified by RNNCell cell\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits',activation=tf.nn.relu)\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping: to avoid gradient explosion\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "    \n",
    "    #1.Compute the gradients with compute_gradients()\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    \n",
    "    #2.Process the gradients as you wish\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    #Given a tensor t, this operation returns a tensor of the same type and shape as t with its values clipped \n",
    "    #toclip_value_min and clip_value_max. Any values less than clip_value_min are set to clip_value_min. \n",
    "    #Any values greater than clip_value_max are set to clip_value_max.\n",
    "    \n",
    "    #3.Apply the processed gradients with apply_gradients().\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 Iteration: 50 Train loss: 0.631658 Train acc: 0.715000\n",
      "Epoch: 0/200 Iteration: 100 Train loss: 0.694300 Train acc: 0.600000\n",
      "Epoch: 0/200 Iteration: 150 Train loss: 0.600649 Train acc: 0.725000\n",
      "Epoch: 0/200 Iteration: 200 Train loss: 0.607597 Train acc: 0.730000\n",
      "Epoch: 0/200 Iteration: 250 Train loss: 0.609325 Train acc: 0.705000\n",
      "Epoch: 0/200 Iteration: 250 Validation loss: 0.683463 Validation acc: 0.717450\n",
      "Epoch: 0/200 Iteration: 300 Train loss: 0.606040 Train acc: 0.720000\n",
      "Epoch: 1/200 Iteration: 350 Train loss: 0.598391 Train acc: 0.725000\n",
      "Epoch: 1/200 Iteration: 400 Train loss: 0.701605 Train acc: 0.600000\n",
      "Epoch: 1/200 Iteration: 450 Train loss: 0.581432 Train acc: 0.725000\n",
      "Epoch: 1/200 Iteration: 500 Train loss: 0.583516 Train acc: 0.730000\n",
      "Epoch: 1/200 Iteration: 500 Validation loss: 0.680266 Validation acc: 0.679350\n",
      "Epoch: 1/200 Iteration: 550 Train loss: 0.631494 Train acc: 0.705000\n",
      "Epoch: 1/200 Iteration: 600 Train loss: 0.613757 Train acc: 0.710000\n",
      "Epoch: 2/200 Iteration: 650 Train loss: 0.583834 Train acc: 0.730000\n",
      "Epoch: 2/200 Iteration: 700 Train loss: 0.713020 Train acc: 0.605000\n",
      "Epoch: 2/200 Iteration: 750 Train loss: 0.594224 Train acc: 0.725000\n",
      "Epoch: 2/200 Iteration: 750 Validation loss: 0.672743 Validation acc: 0.639600\n",
      "Epoch: 2/200 Iteration: 800 Train loss: 0.578470 Train acc: 0.720000\n",
      "Epoch: 2/200 Iteration: 850 Train loss: 0.598118 Train acc: 0.690000\n",
      "Epoch: 2/200 Iteration: 900 Train loss: 0.558181 Train acc: 0.680000\n",
      "Epoch: 3/200 Iteration: 950 Train loss: 0.582603 Train acc: 0.735000\n",
      "Epoch: 3/200 Iteration: 1000 Train loss: 0.681955 Train acc: 0.635000\n",
      "Epoch: 3/200 Iteration: 1000 Validation loss: 0.655156 Validation acc: 0.603600\n",
      "Epoch: 3/200 Iteration: 1050 Train loss: 0.544687 Train acc: 0.755000\n",
      "Epoch: 3/200 Iteration: 1100 Train loss: 0.521881 Train acc: 0.730000\n",
      "Epoch: 3/200 Iteration: 1150 Train loss: 0.527644 Train acc: 0.690000\n",
      "Epoch: 3/200 Iteration: 1200 Train loss: 0.499872 Train acc: 0.730000\n",
      "Epoch: 4/200 Iteration: 1250 Train loss: 0.561332 Train acc: 0.685000\n",
      "Epoch: 4/200 Iteration: 1250 Validation loss: 0.634770 Validation acc: 0.592150\n",
      "Epoch: 4/200 Iteration: 1300 Train loss: 0.635485 Train acc: 0.650000\n",
      "Epoch: 4/200 Iteration: 1350 Train loss: 0.531431 Train acc: 0.715000\n",
      "Epoch: 4/200 Iteration: 1400 Train loss: 0.502879 Train acc: 0.705000\n",
      "Epoch: 4/200 Iteration: 1450 Train loss: 0.520917 Train acc: 0.725000\n",
      "Epoch: 4/200 Iteration: 1500 Train loss: 0.484765 Train acc: 0.760000\n",
      "Epoch: 4/200 Iteration: 1500 Validation loss: 0.617490 Validation acc: 0.609100\n",
      "Epoch: 5/200 Iteration: 1550 Train loss: 0.562902 Train acc: 0.680000\n",
      "Epoch: 5/200 Iteration: 1600 Train loss: 0.648721 Train acc: 0.690000\n",
      "Epoch: 5/200 Iteration: 1650 Train loss: 0.528934 Train acc: 0.705000\n",
      "Epoch: 5/200 Iteration: 1700 Train loss: 0.514547 Train acc: 0.725000\n",
      "Epoch: 5/200 Iteration: 1750 Train loss: 0.499597 Train acc: 0.765000\n",
      "Epoch: 5/200 Iteration: 1750 Validation loss: 0.616812 Validation acc: 0.603300\n",
      "Epoch: 5/200 Iteration: 1800 Train loss: 0.490724 Train acc: 0.770000\n",
      "Epoch: 6/200 Iteration: 1850 Train loss: 0.576017 Train acc: 0.680000\n",
      "Epoch: 6/200 Iteration: 1900 Train loss: 0.664255 Train acc: 0.675000\n",
      "Epoch: 6/200 Iteration: 1950 Train loss: 0.523025 Train acc: 0.705000\n",
      "Epoch: 6/200 Iteration: 2000 Train loss: 0.522846 Train acc: 0.730000\n",
      "Epoch: 6/200 Iteration: 2000 Validation loss: 0.613637 Validation acc: 0.608000\n",
      "Epoch: 6/200 Iteration: 2050 Train loss: 0.499459 Train acc: 0.750000\n",
      "Epoch: 6/200 Iteration: 2100 Train loss: 0.469401 Train acc: 0.730000\n",
      "Epoch: 7/200 Iteration: 2150 Train loss: 0.591738 Train acc: 0.665000\n",
      "Epoch: 7/200 Iteration: 2200 Train loss: 0.664168 Train acc: 0.690000\n",
      "Epoch: 7/200 Iteration: 2250 Train loss: 0.525824 Train acc: 0.685000\n",
      "Epoch: 7/200 Iteration: 2250 Validation loss: 0.610342 Validation acc: 0.609300\n",
      "Epoch: 7/200 Iteration: 2300 Train loss: 0.513765 Train acc: 0.695000\n",
      "Epoch: 7/200 Iteration: 2350 Train loss: 0.495099 Train acc: 0.790000\n",
      "Epoch: 7/200 Iteration: 2400 Train loss: 0.470120 Train acc: 0.735000\n",
      "Epoch: 8/200 Iteration: 2450 Train loss: 0.600944 Train acc: 0.645000\n",
      "Epoch: 8/200 Iteration: 2500 Train loss: 0.655107 Train acc: 0.695000\n",
      "Epoch: 8/200 Iteration: 2500 Validation loss: 0.608944 Validation acc: 0.606250\n",
      "Epoch: 8/200 Iteration: 2550 Train loss: 0.526731 Train acc: 0.685000\n",
      "Epoch: 8/200 Iteration: 2600 Train loss: 0.529745 Train acc: 0.710000\n",
      "Epoch: 8/200 Iteration: 2650 Train loss: 0.485798 Train acc: 0.780000\n",
      "Epoch: 8/200 Iteration: 2700 Train loss: 0.473548 Train acc: 0.770000\n",
      "Epoch: 9/200 Iteration: 2750 Train loss: 0.588656 Train acc: 0.645000\n",
      "Epoch: 9/200 Iteration: 2750 Validation loss: 0.602783 Validation acc: 0.612850\n",
      "Epoch: 9/200 Iteration: 2800 Train loss: 0.643391 Train acc: 0.690000\n",
      "Epoch: 9/200 Iteration: 2850 Train loss: 0.514067 Train acc: 0.720000\n",
      "Epoch: 9/200 Iteration: 2900 Train loss: 0.521638 Train acc: 0.735000\n",
      "Epoch: 9/200 Iteration: 2950 Train loss: 0.496968 Train acc: 0.755000\n",
      "Epoch: 9/200 Iteration: 3000 Train loss: 0.470432 Train acc: 0.790000\n",
      "Epoch: 9/200 Iteration: 3000 Validation loss: 0.595969 Validation acc: 0.616100\n",
      "Epoch: 10/200 Iteration: 3050 Train loss: 0.564166 Train acc: 0.645000\n",
      "Epoch: 10/200 Iteration: 3100 Train loss: 0.650335 Train acc: 0.700000\n",
      "Epoch: 10/200 Iteration: 3150 Train loss: 0.540278 Train acc: 0.700000\n",
      "Epoch: 10/200 Iteration: 3200 Train loss: 0.538788 Train acc: 0.710000\n",
      "Epoch: 10/200 Iteration: 3250 Train loss: 0.499579 Train acc: 0.740000\n",
      "Epoch: 10/200 Iteration: 3250 Validation loss: 0.595474 Validation acc: 0.620650\n",
      "Epoch: 10/200 Iteration: 3300 Train loss: 0.463352 Train acc: 0.770000\n",
      "Epoch: 11/200 Iteration: 3350 Train loss: 0.594025 Train acc: 0.635000\n",
      "Epoch: 11/200 Iteration: 3400 Train loss: 0.650320 Train acc: 0.700000\n",
      "Epoch: 11/200 Iteration: 3450 Train loss: 0.522921 Train acc: 0.690000\n",
      "Epoch: 11/200 Iteration: 3500 Train loss: 0.526101 Train acc: 0.720000\n",
      "Epoch: 11/200 Iteration: 3500 Validation loss: 0.592912 Validation acc: 0.629250\n",
      "Epoch: 11/200 Iteration: 3550 Train loss: 0.482962 Train acc: 0.765000\n",
      "Epoch: 11/200 Iteration: 3600 Train loss: 0.465752 Train acc: 0.770000\n",
      "Epoch: 12/200 Iteration: 3650 Train loss: 0.614728 Train acc: 0.620000\n",
      "Epoch: 12/200 Iteration: 3700 Train loss: 0.637712 Train acc: 0.705000\n",
      "Epoch: 12/200 Iteration: 3750 Train loss: 0.518460 Train acc: 0.695000\n",
      "Epoch: 12/200 Iteration: 3750 Validation loss: 0.590415 Validation acc: 0.636650\n",
      "Epoch: 12/200 Iteration: 3800 Train loss: 0.521200 Train acc: 0.735000\n",
      "Epoch: 12/200 Iteration: 3850 Train loss: 0.467731 Train acc: 0.765000\n",
      "Epoch: 12/200 Iteration: 3900 Train loss: 0.445886 Train acc: 0.785000\n",
      "Epoch: 13/200 Iteration: 3950 Train loss: 0.594306 Train acc: 0.625000\n",
      "Epoch: 13/200 Iteration: 4000 Train loss: 0.585585 Train acc: 0.730000\n",
      "Epoch: 13/200 Iteration: 4000 Validation loss: 0.587060 Validation acc: 0.648900\n",
      "Epoch: 13/200 Iteration: 4050 Train loss: 0.519585 Train acc: 0.710000\n",
      "Epoch: 13/200 Iteration: 4100 Train loss: 0.531948 Train acc: 0.710000\n",
      "Epoch: 13/200 Iteration: 4150 Train loss: 0.469522 Train acc: 0.750000\n",
      "Epoch: 13/200 Iteration: 4200 Train loss: 0.428240 Train acc: 0.790000\n",
      "Epoch: 14/200 Iteration: 4250 Train loss: 0.574796 Train acc: 0.670000\n",
      "Epoch: 14/200 Iteration: 4250 Validation loss: 0.581818 Validation acc: 0.652600\n",
      "Epoch: 14/200 Iteration: 4300 Train loss: 0.620468 Train acc: 0.715000\n",
      "Epoch: 14/200 Iteration: 4350 Train loss: 0.523681 Train acc: 0.730000\n",
      "Epoch: 14/200 Iteration: 4400 Train loss: 0.541911 Train acc: 0.725000\n",
      "Epoch: 14/200 Iteration: 4450 Train loss: 0.459437 Train acc: 0.780000\n",
      "Epoch: 14/200 Iteration: 4500 Train loss: 0.436253 Train acc: 0.785000\n",
      "Epoch: 14/200 Iteration: 4500 Validation loss: 0.575166 Validation acc: 0.661500\n",
      "Epoch: 15/200 Iteration: 4550 Train loss: 0.594351 Train acc: 0.660000\n",
      "Epoch: 15/200 Iteration: 4600 Train loss: 0.599799 Train acc: 0.730000\n",
      "Epoch: 15/200 Iteration: 4650 Train loss: 0.518720 Train acc: 0.725000\n",
      "Epoch: 15/200 Iteration: 4700 Train loss: 0.528472 Train acc: 0.730000\n",
      "Epoch: 15/200 Iteration: 4750 Train loss: 0.471435 Train acc: 0.790000\n",
      "Epoch: 15/200 Iteration: 4750 Validation loss: 0.573908 Validation acc: 0.664400\n",
      "Epoch: 15/200 Iteration: 4800 Train loss: 0.430636 Train acc: 0.770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/200 Iteration: 4850 Train loss: 0.604854 Train acc: 0.680000\n",
      "Epoch: 16/200 Iteration: 4900 Train loss: 0.658559 Train acc: 0.725000\n",
      "Epoch: 16/200 Iteration: 4950 Train loss: 0.509756 Train acc: 0.735000\n",
      "Epoch: 16/200 Iteration: 5000 Train loss: 0.508601 Train acc: 0.735000\n",
      "Epoch: 16/200 Iteration: 5000 Validation loss: 0.570279 Validation acc: 0.670700\n",
      "Epoch: 16/200 Iteration: 5050 Train loss: 0.464241 Train acc: 0.790000\n",
      "Epoch: 16/200 Iteration: 5100 Train loss: 0.427872 Train acc: 0.790000\n",
      "Epoch: 17/200 Iteration: 5150 Train loss: 0.588277 Train acc: 0.655000\n",
      "Epoch: 17/200 Iteration: 5200 Train loss: 0.638775 Train acc: 0.695000\n",
      "Epoch: 17/200 Iteration: 5250 Train loss: 0.517060 Train acc: 0.695000\n",
      "Epoch: 17/200 Iteration: 5250 Validation loss: 0.568497 Validation acc: 0.674400\n",
      "Epoch: 17/200 Iteration: 5300 Train loss: 0.542267 Train acc: 0.725000\n",
      "Epoch: 17/200 Iteration: 5350 Train loss: 0.451987 Train acc: 0.800000\n",
      "Epoch: 17/200 Iteration: 5400 Train loss: 0.441622 Train acc: 0.805000\n",
      "Epoch: 18/200 Iteration: 5450 Train loss: 0.608013 Train acc: 0.655000\n",
      "Epoch: 18/200 Iteration: 5500 Train loss: 0.618034 Train acc: 0.745000\n",
      "Epoch: 18/200 Iteration: 5500 Validation loss: 0.566681 Validation acc: 0.676400\n",
      "Epoch: 18/200 Iteration: 5550 Train loss: 0.522404 Train acc: 0.735000\n",
      "Epoch: 18/200 Iteration: 5600 Train loss: 0.547488 Train acc: 0.730000\n",
      "Epoch: 18/200 Iteration: 5650 Train loss: 0.474449 Train acc: 0.775000\n",
      "Epoch: 18/200 Iteration: 5700 Train loss: 0.433239 Train acc: 0.810000\n",
      "Epoch: 19/200 Iteration: 5750 Train loss: 0.599876 Train acc: 0.690000\n",
      "Epoch: 19/200 Iteration: 5750 Validation loss: 0.563328 Validation acc: 0.679900\n",
      "Epoch: 19/200 Iteration: 5800 Train loss: 0.643419 Train acc: 0.725000\n",
      "Epoch: 19/200 Iteration: 5850 Train loss: 0.523922 Train acc: 0.715000\n",
      "Epoch: 19/200 Iteration: 5900 Train loss: 0.531847 Train acc: 0.720000\n",
      "Epoch: 19/200 Iteration: 5950 Train loss: 0.458326 Train acc: 0.790000\n",
      "Epoch: 19/200 Iteration: 6000 Train loss: 0.408895 Train acc: 0.810000\n",
      "Epoch: 19/200 Iteration: 6000 Validation loss: 0.560147 Validation acc: 0.682600\n",
      "Epoch: 20/200 Iteration: 6050 Train loss: 0.608406 Train acc: 0.660000\n",
      "Epoch: 20/200 Iteration: 6100 Train loss: 0.614060 Train acc: 0.755000\n",
      "Epoch: 20/200 Iteration: 6150 Train loss: 0.511954 Train acc: 0.710000\n",
      "Epoch: 20/200 Iteration: 6200 Train loss: 0.539344 Train acc: 0.715000\n",
      "Epoch: 20/200 Iteration: 6250 Train loss: 0.460065 Train acc: 0.785000\n",
      "Epoch: 20/200 Iteration: 6250 Validation loss: 0.561064 Validation acc: 0.681000\n",
      "Epoch: 20/200 Iteration: 6300 Train loss: 0.412103 Train acc: 0.775000\n",
      "Epoch: 21/200 Iteration: 6350 Train loss: 0.582637 Train acc: 0.680000\n",
      "Epoch: 21/200 Iteration: 6400 Train loss: 0.648628 Train acc: 0.705000\n",
      "Epoch: 21/200 Iteration: 6450 Train loss: 0.530542 Train acc: 0.710000\n",
      "Epoch: 21/200 Iteration: 6500 Train loss: 0.523960 Train acc: 0.730000\n",
      "Epoch: 21/200 Iteration: 6500 Validation loss: 0.557057 Validation acc: 0.687950\n",
      "Epoch: 21/200 Iteration: 6550 Train loss: 0.458472 Train acc: 0.810000\n",
      "Epoch: 21/200 Iteration: 6600 Train loss: 0.434041 Train acc: 0.770000\n",
      "Epoch: 22/200 Iteration: 6650 Train loss: 0.606498 Train acc: 0.675000\n",
      "Epoch: 22/200 Iteration: 6700 Train loss: 0.595437 Train acc: 0.745000\n",
      "Epoch: 22/200 Iteration: 6750 Train loss: 0.515828 Train acc: 0.730000\n",
      "Epoch: 22/200 Iteration: 6750 Validation loss: 0.554834 Validation acc: 0.692600\n",
      "Epoch: 22/200 Iteration: 6800 Train loss: 0.547835 Train acc: 0.720000\n",
      "Epoch: 22/200 Iteration: 6850 Train loss: 0.447229 Train acc: 0.775000\n",
      "Epoch: 22/200 Iteration: 6900 Train loss: 0.411752 Train acc: 0.800000\n",
      "Epoch: 23/200 Iteration: 6950 Train loss: 0.591084 Train acc: 0.695000\n",
      "Epoch: 23/200 Iteration: 7000 Train loss: 0.609229 Train acc: 0.725000\n",
      "Epoch: 23/200 Iteration: 7000 Validation loss: 0.553617 Validation acc: 0.695100\n",
      "Epoch: 23/200 Iteration: 7050 Train loss: 0.523007 Train acc: 0.745000\n",
      "Epoch: 23/200 Iteration: 7100 Train loss: 0.540318 Train acc: 0.725000\n",
      "Epoch: 23/200 Iteration: 7150 Train loss: 0.448524 Train acc: 0.790000\n",
      "Epoch: 23/200 Iteration: 7200 Train loss: 0.412350 Train acc: 0.815000\n",
      "Epoch: 24/200 Iteration: 7250 Train loss: 0.603559 Train acc: 0.660000\n",
      "Epoch: 24/200 Iteration: 7250 Validation loss: 0.551642 Validation acc: 0.693600\n",
      "Epoch: 24/200 Iteration: 7300 Train loss: 0.611957 Train acc: 0.710000\n",
      "Epoch: 24/200 Iteration: 7350 Train loss: 0.494961 Train acc: 0.730000\n",
      "Epoch: 24/200 Iteration: 7400 Train loss: 0.534139 Train acc: 0.735000\n",
      "Epoch: 24/200 Iteration: 7450 Train loss: 0.451358 Train acc: 0.785000\n",
      "Epoch: 24/200 Iteration: 7500 Train loss: 0.418097 Train acc: 0.800000\n",
      "Epoch: 24/200 Iteration: 7500 Validation loss: 0.549281 Validation acc: 0.695050\n",
      "Epoch: 25/200 Iteration: 7550 Train loss: 0.609831 Train acc: 0.660000\n",
      "Epoch: 25/200 Iteration: 7600 Train loss: 0.626161 Train acc: 0.690000\n",
      "Epoch: 25/200 Iteration: 7650 Train loss: 0.507000 Train acc: 0.725000\n",
      "Epoch: 25/200 Iteration: 7700 Train loss: 0.525267 Train acc: 0.725000\n",
      "Epoch: 25/200 Iteration: 7750 Train loss: 0.431032 Train acc: 0.805000\n",
      "Epoch: 25/200 Iteration: 7750 Validation loss: 0.551130 Validation acc: 0.690100\n",
      "Epoch: 25/200 Iteration: 7800 Train loss: 0.416922 Train acc: 0.800000\n",
      "Epoch: 26/200 Iteration: 7850 Train loss: 0.593153 Train acc: 0.680000\n",
      "Epoch: 26/200 Iteration: 7900 Train loss: 0.616030 Train acc: 0.725000\n",
      "Epoch: 26/200 Iteration: 7950 Train loss: 0.509212 Train acc: 0.735000\n",
      "Epoch: 26/200 Iteration: 8000 Train loss: 0.534385 Train acc: 0.730000\n",
      "Epoch: 26/200 Iteration: 8000 Validation loss: 0.548086 Validation acc: 0.697400\n",
      "Epoch: 26/200 Iteration: 8050 Train loss: 0.445215 Train acc: 0.810000\n",
      "Epoch: 26/200 Iteration: 8100 Train loss: 0.420663 Train acc: 0.785000\n",
      "Epoch: 27/200 Iteration: 8150 Train loss: 0.611064 Train acc: 0.675000\n",
      "Epoch: 27/200 Iteration: 8200 Train loss: 0.583960 Train acc: 0.745000\n",
      "Epoch: 27/200 Iteration: 8250 Train loss: 0.526998 Train acc: 0.725000\n",
      "Epoch: 27/200 Iteration: 8250 Validation loss: 0.545230 Validation acc: 0.699600\n",
      "Epoch: 27/200 Iteration: 8300 Train loss: 0.499978 Train acc: 0.735000\n",
      "Epoch: 27/200 Iteration: 8350 Train loss: 0.440882 Train acc: 0.795000\n",
      "Epoch: 27/200 Iteration: 8400 Train loss: 0.420238 Train acc: 0.795000\n",
      "Epoch: 28/200 Iteration: 8450 Train loss: 0.614291 Train acc: 0.675000\n",
      "Epoch: 28/200 Iteration: 8500 Train loss: 0.600696 Train acc: 0.750000\n",
      "Epoch: 28/200 Iteration: 8500 Validation loss: 0.544748 Validation acc: 0.701000\n",
      "Epoch: 28/200 Iteration: 8550 Train loss: 0.519670 Train acc: 0.720000\n",
      "Epoch: 28/200 Iteration: 8600 Train loss: 0.532155 Train acc: 0.730000\n",
      "Epoch: 28/200 Iteration: 8650 Train loss: 0.448491 Train acc: 0.810000\n",
      "Epoch: 28/200 Iteration: 8700 Train loss: 0.417905 Train acc: 0.770000\n",
      "Epoch: 29/200 Iteration: 8750 Train loss: 0.622015 Train acc: 0.665000\n",
      "Epoch: 29/200 Iteration: 8750 Validation loss: 0.542032 Validation acc: 0.702500\n",
      "Epoch: 29/200 Iteration: 8800 Train loss: 0.584431 Train acc: 0.755000\n",
      "Epoch: 29/200 Iteration: 8850 Train loss: 0.507872 Train acc: 0.730000\n",
      "Epoch: 29/200 Iteration: 8900 Train loss: 0.530529 Train acc: 0.720000\n",
      "Epoch: 29/200 Iteration: 8950 Train loss: 0.461418 Train acc: 0.790000\n",
      "Epoch: 29/200 Iteration: 9000 Train loss: 0.399484 Train acc: 0.815000\n",
      "Epoch: 29/200 Iteration: 9000 Validation loss: 0.540847 Validation acc: 0.703450\n",
      "Epoch: 30/200 Iteration: 9050 Train loss: 0.610044 Train acc: 0.680000\n",
      "Epoch: 30/200 Iteration: 9100 Train loss: 0.621903 Train acc: 0.730000\n",
      "Epoch: 30/200 Iteration: 9150 Train loss: 0.522079 Train acc: 0.740000\n",
      "Epoch: 30/200 Iteration: 9200 Train loss: 0.505253 Train acc: 0.740000\n",
      "Epoch: 30/200 Iteration: 9250 Train loss: 0.445318 Train acc: 0.775000\n",
      "Epoch: 30/200 Iteration: 9250 Validation loss: 0.541427 Validation acc: 0.703350\n",
      "Epoch: 30/200 Iteration: 9300 Train loss: 0.420846 Train acc: 0.785000\n",
      "Epoch: 31/200 Iteration: 9350 Train loss: 0.593463 Train acc: 0.705000\n",
      "Epoch: 31/200 Iteration: 9400 Train loss: 0.615582 Train acc: 0.740000\n",
      "Epoch: 31/200 Iteration: 9450 Train loss: 0.511808 Train acc: 0.715000\n",
      "Epoch: 31/200 Iteration: 9500 Train loss: 0.548131 Train acc: 0.720000\n",
      "Epoch: 31/200 Iteration: 9500 Validation loss: 0.540062 Validation acc: 0.704150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/200 Iteration: 9550 Train loss: 0.443343 Train acc: 0.800000\n",
      "Epoch: 31/200 Iteration: 9600 Train loss: 0.405591 Train acc: 0.800000\n",
      "Epoch: 32/200 Iteration: 9650 Train loss: 0.608403 Train acc: 0.670000\n",
      "Epoch: 32/200 Iteration: 9700 Train loss: 0.600165 Train acc: 0.740000\n",
      "Epoch: 32/200 Iteration: 9750 Train loss: 0.525392 Train acc: 0.735000\n",
      "Epoch: 32/200 Iteration: 9750 Validation loss: 0.537845 Validation acc: 0.703200\n",
      "Epoch: 32/200 Iteration: 9800 Train loss: 0.528241 Train acc: 0.725000\n",
      "Epoch: 32/200 Iteration: 9850 Train loss: 0.433140 Train acc: 0.825000\n",
      "Epoch: 32/200 Iteration: 9900 Train loss: 0.406181 Train acc: 0.790000\n",
      "Epoch: 33/200 Iteration: 9950 Train loss: 0.597614 Train acc: 0.695000\n",
      "Epoch: 33/200 Iteration: 10000 Train loss: 0.609388 Train acc: 0.695000\n",
      "Epoch: 33/200 Iteration: 10000 Validation loss: 0.537053 Validation acc: 0.703500\n",
      "Epoch: 33/200 Iteration: 10050 Train loss: 0.491090 Train acc: 0.745000\n",
      "Epoch: 33/200 Iteration: 10100 Train loss: 0.535545 Train acc: 0.715000\n",
      "Epoch: 33/200 Iteration: 10150 Train loss: 0.429965 Train acc: 0.790000\n",
      "Epoch: 33/200 Iteration: 10200 Train loss: 0.412463 Train acc: 0.800000\n",
      "Epoch: 34/200 Iteration: 10250 Train loss: 0.590321 Train acc: 0.695000\n",
      "Epoch: 34/200 Iteration: 10250 Validation loss: 0.536526 Validation acc: 0.702350\n",
      "Epoch: 34/200 Iteration: 10300 Train loss: 0.614336 Train acc: 0.740000\n",
      "Epoch: 34/200 Iteration: 10350 Train loss: 0.522730 Train acc: 0.730000\n",
      "Epoch: 34/200 Iteration: 10400 Train loss: 0.536656 Train acc: 0.735000\n",
      "Epoch: 34/200 Iteration: 10450 Train loss: 0.445951 Train acc: 0.785000\n",
      "Epoch: 34/200 Iteration: 10500 Train loss: 0.393030 Train acc: 0.790000\n",
      "Epoch: 34/200 Iteration: 10500 Validation loss: 0.536032 Validation acc: 0.702800\n",
      "Epoch: 35/200 Iteration: 10550 Train loss: 0.589182 Train acc: 0.710000\n",
      "Epoch: 35/200 Iteration: 10600 Train loss: 0.634781 Train acc: 0.750000\n",
      "Epoch: 35/200 Iteration: 10650 Train loss: 0.509689 Train acc: 0.730000\n",
      "Epoch: 35/200 Iteration: 10700 Train loss: 0.541103 Train acc: 0.730000\n",
      "Epoch: 35/200 Iteration: 10750 Train loss: 0.446837 Train acc: 0.790000\n",
      "Epoch: 35/200 Iteration: 10750 Validation loss: 0.537191 Validation acc: 0.703800\n",
      "Epoch: 35/200 Iteration: 10800 Train loss: 0.446048 Train acc: 0.800000\n",
      "Epoch: 36/200 Iteration: 10850 Train loss: 0.603714 Train acc: 0.680000\n",
      "Epoch: 36/200 Iteration: 10900 Train loss: 0.623390 Train acc: 0.715000\n",
      "Epoch: 36/200 Iteration: 10950 Train loss: 0.493571 Train acc: 0.725000\n",
      "Epoch: 36/200 Iteration: 11000 Train loss: 0.523774 Train acc: 0.750000\n",
      "Epoch: 36/200 Iteration: 11000 Validation loss: 0.536278 Validation acc: 0.704550\n",
      "Epoch: 36/200 Iteration: 11050 Train loss: 0.446837 Train acc: 0.800000\n",
      "Epoch: 36/200 Iteration: 11100 Train loss: 0.401775 Train acc: 0.815000\n",
      "Epoch: 37/200 Iteration: 11150 Train loss: 0.595748 Train acc: 0.695000\n",
      "Epoch: 37/200 Iteration: 11200 Train loss: 0.625871 Train acc: 0.715000\n",
      "Epoch: 37/200 Iteration: 11250 Train loss: 0.506982 Train acc: 0.730000\n",
      "Epoch: 37/200 Iteration: 11250 Validation loss: 0.534617 Validation acc: 0.703900\n",
      "Epoch: 37/200 Iteration: 11300 Train loss: 0.499729 Train acc: 0.735000\n",
      "Epoch: 37/200 Iteration: 11350 Train loss: 0.438252 Train acc: 0.800000\n",
      "Epoch: 37/200 Iteration: 11400 Train loss: 0.404075 Train acc: 0.800000\n",
      "Epoch: 38/200 Iteration: 11450 Train loss: 0.610278 Train acc: 0.675000\n",
      "Epoch: 38/200 Iteration: 11500 Train loss: 0.599546 Train acc: 0.745000\n",
      "Epoch: 38/200 Iteration: 11500 Validation loss: 0.534219 Validation acc: 0.704550\n",
      "Epoch: 38/200 Iteration: 11550 Train loss: 0.505810 Train acc: 0.730000\n",
      "Epoch: 38/200 Iteration: 11600 Train loss: 0.527984 Train acc: 0.730000\n",
      "Epoch: 38/200 Iteration: 11650 Train loss: 0.442134 Train acc: 0.795000\n",
      "Epoch: 38/200 Iteration: 11700 Train loss: 0.419679 Train acc: 0.815000\n",
      "Epoch: 39/200 Iteration: 11750 Train loss: 0.602351 Train acc: 0.685000\n",
      "Epoch: 39/200 Iteration: 11750 Validation loss: 0.534175 Validation acc: 0.703200\n",
      "Epoch: 39/200 Iteration: 11800 Train loss: 0.601797 Train acc: 0.725000\n",
      "Epoch: 39/200 Iteration: 11850 Train loss: 0.507488 Train acc: 0.745000\n",
      "Epoch: 39/200 Iteration: 11900 Train loss: 0.518663 Train acc: 0.750000\n",
      "Epoch: 39/200 Iteration: 11950 Train loss: 0.438954 Train acc: 0.800000\n",
      "Epoch: 39/200 Iteration: 12000 Train loss: 0.395518 Train acc: 0.810000\n",
      "Epoch: 39/200 Iteration: 12000 Validation loss: 0.534589 Validation acc: 0.703700\n",
      "Epoch: 40/200 Iteration: 12050 Train loss: 0.618402 Train acc: 0.670000\n",
      "Epoch: 40/200 Iteration: 12100 Train loss: 0.606697 Train acc: 0.750000\n",
      "Epoch: 40/200 Iteration: 12150 Train loss: 0.498778 Train acc: 0.750000\n",
      "Epoch: 40/200 Iteration: 12200 Train loss: 0.519835 Train acc: 0.735000\n",
      "Epoch: 40/200 Iteration: 12250 Train loss: 0.436749 Train acc: 0.810000\n",
      "Epoch: 40/200 Iteration: 12250 Validation loss: 0.536009 Validation acc: 0.703450\n",
      "Epoch: 40/200 Iteration: 12300 Train loss: 0.400596 Train acc: 0.805000\n",
      "Epoch: 41/200 Iteration: 12350 Train loss: 0.597083 Train acc: 0.675000\n",
      "Epoch: 41/200 Iteration: 12400 Train loss: 0.612410 Train acc: 0.710000\n",
      "Epoch: 41/200 Iteration: 12450 Train loss: 0.502317 Train acc: 0.730000\n",
      "Epoch: 41/200 Iteration: 12500 Train loss: 0.518262 Train acc: 0.725000\n",
      "Epoch: 41/200 Iteration: 12500 Validation loss: 0.535206 Validation acc: 0.704400\n",
      "Epoch: 41/200 Iteration: 12550 Train loss: 0.443949 Train acc: 0.800000\n",
      "Epoch: 41/200 Iteration: 12600 Train loss: 0.417281 Train acc: 0.770000\n",
      "Epoch: 42/200 Iteration: 12650 Train loss: 0.605342 Train acc: 0.685000\n",
      "Epoch: 42/200 Iteration: 12700 Train loss: 0.621553 Train acc: 0.715000\n",
      "Epoch: 42/200 Iteration: 12750 Train loss: 0.481053 Train acc: 0.750000\n",
      "Epoch: 42/200 Iteration: 12750 Validation loss: 0.533513 Validation acc: 0.709850\n",
      "Epoch: 42/200 Iteration: 12800 Train loss: 0.513436 Train acc: 0.745000\n",
      "Epoch: 42/200 Iteration: 12850 Train loss: 0.425291 Train acc: 0.790000\n",
      "Epoch: 42/200 Iteration: 12900 Train loss: 0.404326 Train acc: 0.815000\n",
      "Epoch: 43/200 Iteration: 12950 Train loss: 0.594399 Train acc: 0.690000\n",
      "Epoch: 43/200 Iteration: 13000 Train loss: 0.596160 Train acc: 0.745000\n",
      "Epoch: 43/200 Iteration: 13000 Validation loss: 0.532876 Validation acc: 0.708950\n",
      "Epoch: 43/200 Iteration: 13050 Train loss: 0.492948 Train acc: 0.760000\n",
      "Epoch: 43/200 Iteration: 13100 Train loss: 0.501658 Train acc: 0.760000\n",
      "Epoch: 43/200 Iteration: 13150 Train loss: 0.434116 Train acc: 0.790000\n",
      "Epoch: 43/200 Iteration: 13200 Train loss: 0.390576 Train acc: 0.815000\n",
      "Epoch: 44/200 Iteration: 13250 Train loss: 0.590086 Train acc: 0.685000\n",
      "Epoch: 44/200 Iteration: 13250 Validation loss: 0.532330 Validation acc: 0.711050\n",
      "Epoch: 44/200 Iteration: 13300 Train loss: 0.614803 Train acc: 0.725000\n",
      "Epoch: 44/200 Iteration: 13350 Train loss: 0.496251 Train acc: 0.745000\n",
      "Epoch: 44/200 Iteration: 13400 Train loss: 0.496556 Train acc: 0.715000\n",
      "Epoch: 44/200 Iteration: 13450 Train loss: 0.424627 Train acc: 0.800000\n",
      "Epoch: 44/200 Iteration: 13500 Train loss: 0.402871 Train acc: 0.800000\n",
      "Epoch: 44/200 Iteration: 13500 Validation loss: 0.532969 Validation acc: 0.709900\n",
      "Epoch: 45/200 Iteration: 13550 Train loss: 0.594194 Train acc: 0.680000\n",
      "Epoch: 45/200 Iteration: 13600 Train loss: 0.612957 Train acc: 0.765000\n",
      "Epoch: 45/200 Iteration: 13650 Train loss: 0.524568 Train acc: 0.730000\n",
      "Epoch: 45/200 Iteration: 13700 Train loss: 0.520245 Train acc: 0.735000\n",
      "Epoch: 45/200 Iteration: 13750 Train loss: 0.435664 Train acc: 0.795000\n",
      "Epoch: 45/200 Iteration: 13750 Validation loss: 0.533955 Validation acc: 0.708050\n",
      "Epoch: 45/200 Iteration: 13800 Train loss: 0.410042 Train acc: 0.810000\n",
      "Epoch: 46/200 Iteration: 13850 Train loss: 0.594647 Train acc: 0.700000\n",
      "Epoch: 46/200 Iteration: 13900 Train loss: 0.624895 Train acc: 0.735000\n",
      "Epoch: 46/200 Iteration: 13950 Train loss: 0.496517 Train acc: 0.755000\n",
      "Epoch: 46/200 Iteration: 14000 Train loss: 0.514081 Train acc: 0.745000\n",
      "Epoch: 46/200 Iteration: 14000 Validation loss: 0.533117 Validation acc: 0.712150\n",
      "Epoch: 46/200 Iteration: 14050 Train loss: 0.434753 Train acc: 0.780000\n",
      "Epoch: 46/200 Iteration: 14100 Train loss: 0.406012 Train acc: 0.795000\n",
      "Epoch: 47/200 Iteration: 14150 Train loss: 0.588353 Train acc: 0.680000\n",
      "Epoch: 47/200 Iteration: 14200 Train loss: 0.607065 Train acc: 0.720000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/200 Iteration: 14250 Train loss: 0.502826 Train acc: 0.745000\n",
      "Epoch: 47/200 Iteration: 14250 Validation loss: 0.531359 Validation acc: 0.712000\n",
      "Epoch: 47/200 Iteration: 14300 Train loss: 0.526464 Train acc: 0.740000\n",
      "Epoch: 47/200 Iteration: 14350 Train loss: 0.427026 Train acc: 0.805000\n",
      "Epoch: 47/200 Iteration: 14400 Train loss: 0.402005 Train acc: 0.825000\n",
      "Epoch: 48/200 Iteration: 14450 Train loss: 0.585745 Train acc: 0.710000\n",
      "Epoch: 48/200 Iteration: 14500 Train loss: 0.600060 Train acc: 0.770000\n",
      "Epoch: 48/200 Iteration: 14500 Validation loss: 0.531117 Validation acc: 0.711900\n",
      "Epoch: 48/200 Iteration: 14550 Train loss: 0.497622 Train acc: 0.780000\n",
      "Epoch: 48/200 Iteration: 14600 Train loss: 0.509458 Train acc: 0.725000\n",
      "Epoch: 48/200 Iteration: 14650 Train loss: 0.431881 Train acc: 0.810000\n",
      "Epoch: 48/200 Iteration: 14700 Train loss: 0.410863 Train acc: 0.800000\n",
      "Epoch: 49/200 Iteration: 14750 Train loss: 0.595944 Train acc: 0.670000\n",
      "Epoch: 49/200 Iteration: 14750 Validation loss: 0.530827 Validation acc: 0.712800\n",
      "Epoch: 49/200 Iteration: 14800 Train loss: 0.610714 Train acc: 0.735000\n",
      "Epoch: 49/200 Iteration: 14850 Train loss: 0.498710 Train acc: 0.765000\n",
      "Epoch: 49/200 Iteration: 14900 Train loss: 0.529370 Train acc: 0.735000\n",
      "Epoch: 49/200 Iteration: 14950 Train loss: 0.427838 Train acc: 0.815000\n",
      "Epoch: 49/200 Iteration: 15000 Train loss: 0.416471 Train acc: 0.795000\n",
      "Epoch: 49/200 Iteration: 15000 Validation loss: 0.530997 Validation acc: 0.712200\n",
      "Epoch: 50/200 Iteration: 15050 Train loss: 0.608105 Train acc: 0.680000\n",
      "Epoch: 50/200 Iteration: 15100 Train loss: 0.613070 Train acc: 0.745000\n",
      "Epoch: 50/200 Iteration: 15150 Train loss: 0.498002 Train acc: 0.755000\n",
      "Epoch: 50/200 Iteration: 15200 Train loss: 0.541426 Train acc: 0.725000\n",
      "Epoch: 50/200 Iteration: 15250 Train loss: 0.430274 Train acc: 0.805000\n",
      "Epoch: 50/200 Iteration: 15250 Validation loss: 0.532163 Validation acc: 0.710950\n",
      "Epoch: 50/200 Iteration: 15300 Train loss: 0.395420 Train acc: 0.825000\n",
      "Epoch: 51/200 Iteration: 15350 Train loss: 0.599646 Train acc: 0.690000\n",
      "Epoch: 51/200 Iteration: 15400 Train loss: 0.609249 Train acc: 0.740000\n",
      "Epoch: 51/200 Iteration: 15450 Train loss: 0.496645 Train acc: 0.760000\n",
      "Epoch: 51/200 Iteration: 15500 Train loss: 0.513258 Train acc: 0.750000\n",
      "Epoch: 51/200 Iteration: 15500 Validation loss: 0.531694 Validation acc: 0.711350\n",
      "Epoch: 51/200 Iteration: 15550 Train loss: 0.415722 Train acc: 0.830000\n",
      "Epoch: 51/200 Iteration: 15600 Train loss: 0.393418 Train acc: 0.840000\n",
      "Epoch: 52/200 Iteration: 15650 Train loss: 0.599871 Train acc: 0.710000\n",
      "Epoch: 52/200 Iteration: 15700 Train loss: 0.596929 Train acc: 0.745000\n",
      "Epoch: 52/200 Iteration: 15750 Train loss: 0.496605 Train acc: 0.735000\n",
      "Epoch: 52/200 Iteration: 15750 Validation loss: 0.530819 Validation acc: 0.711050\n",
      "Epoch: 52/200 Iteration: 15800 Train loss: 0.509351 Train acc: 0.745000\n",
      "Epoch: 52/200 Iteration: 15850 Train loss: 0.422794 Train acc: 0.815000\n",
      "Epoch: 52/200 Iteration: 15900 Train loss: 0.400088 Train acc: 0.820000\n",
      "Epoch: 53/200 Iteration: 15950 Train loss: 0.605197 Train acc: 0.700000\n",
      "Epoch: 53/200 Iteration: 16000 Train loss: 0.599650 Train acc: 0.735000\n",
      "Epoch: 53/200 Iteration: 16000 Validation loss: 0.530283 Validation acc: 0.712750\n",
      "Epoch: 53/200 Iteration: 16050 Train loss: 0.513204 Train acc: 0.740000\n",
      "Epoch: 53/200 Iteration: 16100 Train loss: 0.503379 Train acc: 0.760000\n",
      "Epoch: 53/200 Iteration: 16150 Train loss: 0.455326 Train acc: 0.780000\n",
      "Epoch: 53/200 Iteration: 16200 Train loss: 0.400283 Train acc: 0.810000\n",
      "Epoch: 54/200 Iteration: 16250 Train loss: 0.591526 Train acc: 0.700000\n",
      "Epoch: 54/200 Iteration: 16250 Validation loss: 0.530441 Validation acc: 0.711750\n",
      "Epoch: 54/200 Iteration: 16300 Train loss: 0.619006 Train acc: 0.725000\n",
      "Epoch: 54/200 Iteration: 16350 Train loss: 0.497856 Train acc: 0.750000\n",
      "Epoch: 54/200 Iteration: 16400 Train loss: 0.514335 Train acc: 0.730000\n",
      "Epoch: 54/200 Iteration: 16450 Train loss: 0.428493 Train acc: 0.800000\n",
      "Epoch: 54/200 Iteration: 16500 Train loss: 0.397722 Train acc: 0.805000\n",
      "Epoch: 54/200 Iteration: 16500 Validation loss: 0.531334 Validation acc: 0.710250\n",
      "Epoch: 55/200 Iteration: 16550 Train loss: 0.612476 Train acc: 0.675000\n",
      "Epoch: 55/200 Iteration: 16600 Train loss: 0.592619 Train acc: 0.755000\n",
      "Epoch: 55/200 Iteration: 16650 Train loss: 0.484285 Train acc: 0.735000\n",
      "Epoch: 55/200 Iteration: 16700 Train loss: 0.516641 Train acc: 0.735000\n",
      "Epoch: 55/200 Iteration: 16750 Train loss: 0.416475 Train acc: 0.830000\n",
      "Epoch: 55/200 Iteration: 16750 Validation loss: 0.532181 Validation acc: 0.708050\n",
      "Epoch: 55/200 Iteration: 16800 Train loss: 0.401287 Train acc: 0.785000\n",
      "Epoch: 56/200 Iteration: 16850 Train loss: 0.591055 Train acc: 0.655000\n",
      "Epoch: 56/200 Iteration: 16900 Train loss: 0.624764 Train acc: 0.735000\n",
      "Epoch: 56/200 Iteration: 16950 Train loss: 0.484533 Train acc: 0.755000\n",
      "Epoch: 56/200 Iteration: 17000 Train loss: 0.508633 Train acc: 0.755000\n",
      "Epoch: 56/200 Iteration: 17000 Validation loss: 0.531705 Validation acc: 0.710400\n",
      "Epoch: 56/200 Iteration: 17050 Train loss: 0.430312 Train acc: 0.805000\n",
      "Epoch: 56/200 Iteration: 17100 Train loss: 0.393470 Train acc: 0.805000\n",
      "Epoch: 57/200 Iteration: 17150 Train loss: 0.588932 Train acc: 0.700000\n",
      "Epoch: 57/200 Iteration: 17200 Train loss: 0.615106 Train acc: 0.770000\n",
      "Epoch: 57/200 Iteration: 17250 Train loss: 0.486592 Train acc: 0.745000\n",
      "Epoch: 57/200 Iteration: 17250 Validation loss: 0.530787 Validation acc: 0.711850\n",
      "Epoch: 57/200 Iteration: 17300 Train loss: 0.531858 Train acc: 0.765000\n",
      "Epoch: 57/200 Iteration: 17350 Train loss: 0.437642 Train acc: 0.820000\n",
      "Epoch: 57/200 Iteration: 17400 Train loss: 0.403719 Train acc: 0.815000\n",
      "Epoch: 58/200 Iteration: 17450 Train loss: 0.575637 Train acc: 0.705000\n",
      "Epoch: 58/200 Iteration: 17500 Train loss: 0.573316 Train acc: 0.755000\n",
      "Epoch: 58/200 Iteration: 17500 Validation loss: 0.530114 Validation acc: 0.711700\n",
      "Epoch: 58/200 Iteration: 17550 Train loss: 0.476866 Train acc: 0.765000\n",
      "Epoch: 58/200 Iteration: 17600 Train loss: 0.508895 Train acc: 0.735000\n",
      "Epoch: 58/200 Iteration: 17650 Train loss: 0.426140 Train acc: 0.780000\n",
      "Epoch: 58/200 Iteration: 17700 Train loss: 0.393792 Train acc: 0.795000\n",
      "Epoch: 59/200 Iteration: 17750 Train loss: 0.588142 Train acc: 0.720000\n",
      "Epoch: 59/200 Iteration: 17750 Validation loss: 0.530693 Validation acc: 0.710700\n",
      "Epoch: 59/200 Iteration: 17800 Train loss: 0.584370 Train acc: 0.755000\n",
      "Epoch: 59/200 Iteration: 17850 Train loss: 0.514217 Train acc: 0.735000\n",
      "Epoch: 59/200 Iteration: 17900 Train loss: 0.520088 Train acc: 0.760000\n",
      "Epoch: 59/200 Iteration: 17950 Train loss: 0.418273 Train acc: 0.825000\n",
      "Epoch: 59/200 Iteration: 18000 Train loss: 0.382510 Train acc: 0.815000\n",
      "Epoch: 59/200 Iteration: 18000 Validation loss: 0.531586 Validation acc: 0.710350\n",
      "Epoch: 60/200 Iteration: 18050 Train loss: 0.591124 Train acc: 0.725000\n",
      "Epoch: 60/200 Iteration: 18100 Train loss: 0.577563 Train acc: 0.745000\n",
      "Epoch: 60/200 Iteration: 18150 Train loss: 0.486528 Train acc: 0.745000\n",
      "Epoch: 60/200 Iteration: 18200 Train loss: 0.524433 Train acc: 0.740000\n",
      "Epoch: 60/200 Iteration: 18250 Train loss: 0.422724 Train acc: 0.815000\n",
      "Epoch: 60/200 Iteration: 18250 Validation loss: 0.532614 Validation acc: 0.705350\n",
      "Epoch: 60/200 Iteration: 18300 Train loss: 0.393831 Train acc: 0.795000\n",
      "Epoch: 61/200 Iteration: 18350 Train loss: 0.605055 Train acc: 0.700000\n",
      "Epoch: 61/200 Iteration: 18400 Train loss: 0.575834 Train acc: 0.730000\n",
      "Epoch: 61/200 Iteration: 18450 Train loss: 0.491658 Train acc: 0.760000\n",
      "Epoch: 61/200 Iteration: 18500 Train loss: 0.507518 Train acc: 0.740000\n",
      "Epoch: 61/200 Iteration: 18500 Validation loss: 0.532495 Validation acc: 0.710650\n",
      "Epoch: 61/200 Iteration: 18550 Train loss: 0.407054 Train acc: 0.805000\n",
      "Epoch: 61/200 Iteration: 18600 Train loss: 0.400245 Train acc: 0.820000\n",
      "Epoch: 62/200 Iteration: 18650 Train loss: 0.595957 Train acc: 0.695000\n",
      "Epoch: 62/200 Iteration: 18700 Train loss: 0.608904 Train acc: 0.755000\n",
      "Epoch: 62/200 Iteration: 18750 Train loss: 0.499311 Train acc: 0.750000\n",
      "Epoch: 62/200 Iteration: 18750 Validation loss: 0.531574 Validation acc: 0.711350\n",
      "Epoch: 62/200 Iteration: 18800 Train loss: 0.507248 Train acc: 0.755000\n",
      "Epoch: 62/200 Iteration: 18850 Train loss: 0.419028 Train acc: 0.795000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/200 Iteration: 18900 Train loss: 0.398477 Train acc: 0.825000\n",
      "Epoch: 63/200 Iteration: 18950 Train loss: 0.592728 Train acc: 0.685000\n",
      "Epoch: 63/200 Iteration: 19000 Train loss: 0.606269 Train acc: 0.765000\n",
      "Epoch: 63/200 Iteration: 19000 Validation loss: 0.530811 Validation acc: 0.711500\n",
      "Epoch: 63/200 Iteration: 19050 Train loss: 0.508200 Train acc: 0.750000\n",
      "Epoch: 63/200 Iteration: 19100 Train loss: 0.515488 Train acc: 0.750000\n",
      "Epoch: 63/200 Iteration: 19150 Train loss: 0.422431 Train acc: 0.800000\n",
      "Epoch: 63/200 Iteration: 19200 Train loss: 0.386647 Train acc: 0.810000\n",
      "Epoch: 64/200 Iteration: 19250 Train loss: 0.596222 Train acc: 0.695000\n",
      "Epoch: 64/200 Iteration: 19250 Validation loss: 0.531118 Validation acc: 0.709700\n",
      "Epoch: 64/200 Iteration: 19300 Train loss: 0.588653 Train acc: 0.720000\n",
      "Epoch: 64/200 Iteration: 19350 Train loss: 0.477411 Train acc: 0.770000\n",
      "Epoch: 64/200 Iteration: 19400 Train loss: 0.512361 Train acc: 0.740000\n",
      "Epoch: 64/200 Iteration: 19450 Train loss: 0.406991 Train acc: 0.825000\n",
      "Epoch: 64/200 Iteration: 19500 Train loss: 0.417333 Train acc: 0.795000\n",
      "Epoch: 64/200 Iteration: 19500 Validation loss: 0.531998 Validation acc: 0.707350\n",
      "Epoch: 65/200 Iteration: 19550 Train loss: 0.613316 Train acc: 0.710000\n",
      "Epoch: 65/200 Iteration: 19600 Train loss: 0.571224 Train acc: 0.765000\n",
      "Epoch: 65/200 Iteration: 19650 Train loss: 0.505454 Train acc: 0.735000\n",
      "Epoch: 65/200 Iteration: 19700 Train loss: 0.529284 Train acc: 0.750000\n",
      "Epoch: 65/200 Iteration: 19750 Train loss: 0.427383 Train acc: 0.805000\n",
      "Epoch: 65/200 Iteration: 19750 Validation loss: 0.532688 Validation acc: 0.708700\n",
      "Epoch: 65/200 Iteration: 19800 Train loss: 0.401861 Train acc: 0.810000\n",
      "Epoch: 66/200 Iteration: 19850 Train loss: 0.580330 Train acc: 0.700000\n",
      "Epoch: 66/200 Iteration: 19900 Train loss: 0.589795 Train acc: 0.760000\n",
      "Epoch: 66/200 Iteration: 19950 Train loss: 0.489620 Train acc: 0.755000\n",
      "Epoch: 66/200 Iteration: 20000 Train loss: 0.494709 Train acc: 0.755000\n",
      "Epoch: 66/200 Iteration: 20000 Validation loss: 0.532922 Validation acc: 0.707400\n",
      "Epoch: 66/200 Iteration: 20050 Train loss: 0.431088 Train acc: 0.820000\n",
      "Epoch: 66/200 Iteration: 20100 Train loss: 0.409297 Train acc: 0.825000\n",
      "Epoch: 67/200 Iteration: 20150 Train loss: 0.602846 Train acc: 0.715000\n",
      "Epoch: 67/200 Iteration: 20200 Train loss: 0.603633 Train acc: 0.750000\n",
      "Epoch: 67/200 Iteration: 20250 Train loss: 0.489943 Train acc: 0.770000\n",
      "Epoch: 67/200 Iteration: 20250 Validation loss: 0.532258 Validation acc: 0.706700\n",
      "Epoch: 67/200 Iteration: 20300 Train loss: 0.497088 Train acc: 0.750000\n",
      "Epoch: 67/200 Iteration: 20350 Train loss: 0.409097 Train acc: 0.795000\n",
      "Epoch: 67/200 Iteration: 20400 Train loss: 0.387450 Train acc: 0.795000\n",
      "Epoch: 68/200 Iteration: 20450 Train loss: 0.583852 Train acc: 0.720000\n",
      "Epoch: 68/200 Iteration: 20500 Train loss: 0.585349 Train acc: 0.760000\n",
      "Epoch: 68/200 Iteration: 20500 Validation loss: 0.531634 Validation acc: 0.706450\n",
      "Epoch: 68/200 Iteration: 20550 Train loss: 0.477989 Train acc: 0.750000\n",
      "Epoch: 68/200 Iteration: 20600 Train loss: 0.520446 Train acc: 0.740000\n",
      "Epoch: 68/200 Iteration: 20650 Train loss: 0.425289 Train acc: 0.810000\n",
      "Epoch: 68/200 Iteration: 20700 Train loss: 0.382126 Train acc: 0.810000\n",
      "Epoch: 69/200 Iteration: 20750 Train loss: 0.591988 Train acc: 0.710000\n",
      "Epoch: 69/200 Iteration: 20750 Validation loss: 0.531760 Validation acc: 0.706800\n",
      "Epoch: 69/200 Iteration: 20800 Train loss: 0.581509 Train acc: 0.770000\n",
      "Epoch: 69/200 Iteration: 20850 Train loss: 0.508036 Train acc: 0.745000\n",
      "Epoch: 69/200 Iteration: 20900 Train loss: 0.521275 Train acc: 0.745000\n",
      "Epoch: 69/200 Iteration: 20950 Train loss: 0.384219 Train acc: 0.845000\n",
      "Epoch: 69/200 Iteration: 21000 Train loss: 0.388972 Train acc: 0.810000\n",
      "Epoch: 69/200 Iteration: 21000 Validation loss: 0.532427 Validation acc: 0.706850\n",
      "Epoch: 70/200 Iteration: 21050 Train loss: 0.564680 Train acc: 0.735000\n",
      "Epoch: 70/200 Iteration: 21100 Train loss: 0.597380 Train acc: 0.740000\n",
      "Epoch: 70/200 Iteration: 21150 Train loss: 0.485446 Train acc: 0.760000\n",
      "Epoch: 70/200 Iteration: 21200 Train loss: 0.492950 Train acc: 0.745000\n",
      "Epoch: 70/200 Iteration: 21250 Train loss: 0.413057 Train acc: 0.825000\n",
      "Epoch: 70/200 Iteration: 21250 Validation loss: 0.532968 Validation acc: 0.706600\n",
      "Epoch: 70/200 Iteration: 21300 Train loss: 0.405376 Train acc: 0.815000\n",
      "Epoch: 71/200 Iteration: 21350 Train loss: 0.585213 Train acc: 0.680000\n",
      "Epoch: 71/200 Iteration: 21400 Train loss: 0.597621 Train acc: 0.770000\n",
      "Epoch: 71/200 Iteration: 21450 Train loss: 0.500143 Train acc: 0.760000\n",
      "Epoch: 71/200 Iteration: 21500 Train loss: 0.501950 Train acc: 0.765000\n",
      "Epoch: 71/200 Iteration: 21500 Validation loss: 0.533398 Validation acc: 0.706450\n",
      "Epoch: 71/200 Iteration: 21550 Train loss: 0.393943 Train acc: 0.830000\n",
      "Epoch: 71/200 Iteration: 21600 Train loss: 0.404287 Train acc: 0.800000\n",
      "Epoch: 72/200 Iteration: 21650 Train loss: 0.608160 Train acc: 0.695000\n",
      "Epoch: 72/200 Iteration: 21700 Train loss: 0.603518 Train acc: 0.745000\n",
      "Epoch: 72/200 Iteration: 21750 Train loss: 0.502977 Train acc: 0.745000\n",
      "Epoch: 72/200 Iteration: 21750 Validation loss: 0.533136 Validation acc: 0.706400\n",
      "Epoch: 72/200 Iteration: 21800 Train loss: 0.525427 Train acc: 0.750000\n",
      "Epoch: 72/200 Iteration: 21850 Train loss: 0.410534 Train acc: 0.820000\n",
      "Epoch: 72/200 Iteration: 21900 Train loss: 0.391287 Train acc: 0.810000\n",
      "Epoch: 73/200 Iteration: 21950 Train loss: 0.590201 Train acc: 0.690000\n",
      "Epoch: 73/200 Iteration: 22000 Train loss: 0.573352 Train acc: 0.780000\n",
      "Epoch: 73/200 Iteration: 22000 Validation loss: 0.532048 Validation acc: 0.709550\n",
      "Epoch: 73/200 Iteration: 22050 Train loss: 0.484700 Train acc: 0.780000\n",
      "Epoch: 73/200 Iteration: 22100 Train loss: 0.509609 Train acc: 0.740000\n",
      "Epoch: 73/200 Iteration: 22150 Train loss: 0.412063 Train acc: 0.805000\n",
      "Epoch: 73/200 Iteration: 22200 Train loss: 0.388785 Train acc: 0.815000\n",
      "Epoch: 74/200 Iteration: 22250 Train loss: 0.599068 Train acc: 0.715000\n",
      "Epoch: 74/200 Iteration: 22250 Validation loss: 0.532239 Validation acc: 0.708800\n",
      "Epoch: 74/200 Iteration: 22300 Train loss: 0.583167 Train acc: 0.785000\n",
      "Epoch: 74/200 Iteration: 22350 Train loss: 0.493921 Train acc: 0.765000\n",
      "Epoch: 74/200 Iteration: 22400 Train loss: 0.508196 Train acc: 0.740000\n",
      "Epoch: 74/200 Iteration: 22450 Train loss: 0.403411 Train acc: 0.825000\n",
      "Epoch: 74/200 Iteration: 22500 Train loss: 0.395447 Train acc: 0.825000\n",
      "Epoch: 74/200 Iteration: 22500 Validation loss: 0.532788 Validation acc: 0.705850\n",
      "Epoch: 75/200 Iteration: 22550 Train loss: 0.607355 Train acc: 0.690000\n",
      "Epoch: 75/200 Iteration: 22600 Train loss: 0.594114 Train acc: 0.740000\n",
      "Epoch: 75/200 Iteration: 22650 Train loss: 0.489245 Train acc: 0.765000\n",
      "Epoch: 75/200 Iteration: 22700 Train loss: 0.485123 Train acc: 0.760000\n",
      "Epoch: 75/200 Iteration: 22750 Train loss: 0.416920 Train acc: 0.825000\n",
      "Epoch: 75/200 Iteration: 22750 Validation loss: 0.533646 Validation acc: 0.705100\n",
      "Epoch: 75/200 Iteration: 22800 Train loss: 0.382953 Train acc: 0.800000\n",
      "Epoch: 76/200 Iteration: 22850 Train loss: 0.591090 Train acc: 0.695000\n",
      "Epoch: 76/200 Iteration: 22900 Train loss: 0.606833 Train acc: 0.745000\n",
      "Epoch: 76/200 Iteration: 22950 Train loss: 0.479609 Train acc: 0.765000\n",
      "Epoch: 76/200 Iteration: 23000 Train loss: 0.512822 Train acc: 0.755000\n",
      "Epoch: 76/200 Iteration: 23000 Validation loss: 0.534913 Validation acc: 0.704150\n",
      "Epoch: 76/200 Iteration: 23050 Train loss: 0.401860 Train acc: 0.835000\n",
      "Epoch: 76/200 Iteration: 23100 Train loss: 0.403719 Train acc: 0.790000\n",
      "Epoch: 77/200 Iteration: 23150 Train loss: 0.586616 Train acc: 0.700000\n",
      "Epoch: 77/200 Iteration: 23200 Train loss: 0.588932 Train acc: 0.750000\n",
      "Epoch: 77/200 Iteration: 23250 Train loss: 0.499014 Train acc: 0.770000\n",
      "Epoch: 77/200 Iteration: 23250 Validation loss: 0.534276 Validation acc: 0.704750\n",
      "Epoch: 77/200 Iteration: 23300 Train loss: 0.497913 Train acc: 0.770000\n",
      "Epoch: 77/200 Iteration: 23350 Train loss: 0.408232 Train acc: 0.835000\n",
      "Epoch: 77/200 Iteration: 23400 Train loss: 0.364253 Train acc: 0.810000\n",
      "Epoch: 78/200 Iteration: 23450 Train loss: 0.582086 Train acc: 0.705000\n",
      "Epoch: 78/200 Iteration: 23500 Train loss: 0.596912 Train acc: 0.750000\n",
      "Epoch: 78/200 Iteration: 23500 Validation loss: 0.533377 Validation acc: 0.707750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/200 Iteration: 23550 Train loss: 0.459431 Train acc: 0.755000\n",
      "Epoch: 78/200 Iteration: 23600 Train loss: 0.509645 Train acc: 0.740000\n",
      "Epoch: 78/200 Iteration: 23650 Train loss: 0.407180 Train acc: 0.855000\n",
      "Epoch: 78/200 Iteration: 23700 Train loss: 0.408366 Train acc: 0.815000\n",
      "Epoch: 79/200 Iteration: 23750 Train loss: 0.596899 Train acc: 0.700000\n",
      "Epoch: 79/200 Iteration: 23750 Validation loss: 0.533699 Validation acc: 0.710250\n",
      "Epoch: 79/200 Iteration: 23800 Train loss: 0.609784 Train acc: 0.720000\n",
      "Epoch: 79/200 Iteration: 23850 Train loss: 0.484381 Train acc: 0.765000\n",
      "Epoch: 79/200 Iteration: 23900 Train loss: 0.520640 Train acc: 0.735000\n",
      "Epoch: 79/200 Iteration: 23950 Train loss: 0.394549 Train acc: 0.800000\n",
      "Epoch: 79/200 Iteration: 24000 Train loss: 0.390627 Train acc: 0.810000\n",
      "Epoch: 79/200 Iteration: 24000 Validation loss: 0.534847 Validation acc: 0.707500\n",
      "Epoch: 80/200 Iteration: 24050 Train loss: 0.586290 Train acc: 0.695000\n",
      "Epoch: 80/200 Iteration: 24100 Train loss: 0.580584 Train acc: 0.760000\n",
      "Epoch: 80/200 Iteration: 24150 Train loss: 0.482639 Train acc: 0.765000\n",
      "Epoch: 80/200 Iteration: 24200 Train loss: 0.499976 Train acc: 0.740000\n",
      "Epoch: 80/200 Iteration: 24250 Train loss: 0.398498 Train acc: 0.825000\n",
      "Epoch: 80/200 Iteration: 24250 Validation loss: 0.535008 Validation acc: 0.705650\n",
      "Epoch: 80/200 Iteration: 24300 Train loss: 0.390792 Train acc: 0.805000\n",
      "Epoch: 81/200 Iteration: 24350 Train loss: 0.588749 Train acc: 0.710000\n",
      "Epoch: 81/200 Iteration: 24400 Train loss: 0.589574 Train acc: 0.735000\n",
      "Epoch: 81/200 Iteration: 24450 Train loss: 0.486355 Train acc: 0.750000\n",
      "Epoch: 81/200 Iteration: 24500 Train loss: 0.506013 Train acc: 0.745000\n",
      "Epoch: 81/200 Iteration: 24500 Validation loss: 0.536666 Validation acc: 0.704300\n",
      "Epoch: 81/200 Iteration: 24550 Train loss: 0.401009 Train acc: 0.825000\n",
      "Epoch: 81/200 Iteration: 24600 Train loss: 0.388480 Train acc: 0.815000\n",
      "Epoch: 82/200 Iteration: 24650 Train loss: 0.587702 Train acc: 0.685000\n",
      "Epoch: 82/200 Iteration: 24700 Train loss: 0.606853 Train acc: 0.765000\n",
      "Epoch: 82/200 Iteration: 24750 Train loss: 0.479527 Train acc: 0.755000\n",
      "Epoch: 82/200 Iteration: 24750 Validation loss: 0.535784 Validation acc: 0.704600\n",
      "Epoch: 82/200 Iteration: 24800 Train loss: 0.508502 Train acc: 0.750000\n",
      "Epoch: 82/200 Iteration: 24850 Train loss: 0.404338 Train acc: 0.805000\n",
      "Epoch: 82/200 Iteration: 24900 Train loss: 0.391253 Train acc: 0.820000\n",
      "Epoch: 83/200 Iteration: 24950 Train loss: 0.595613 Train acc: 0.680000\n",
      "Epoch: 83/200 Iteration: 25000 Train loss: 0.581236 Train acc: 0.755000\n",
      "Epoch: 83/200 Iteration: 25000 Validation loss: 0.534265 Validation acc: 0.708400\n",
      "Epoch: 83/200 Iteration: 25050 Train loss: 0.479554 Train acc: 0.745000\n",
      "Epoch: 83/200 Iteration: 25100 Train loss: 0.522590 Train acc: 0.765000\n",
      "Epoch: 83/200 Iteration: 25150 Train loss: 0.413047 Train acc: 0.810000\n",
      "Epoch: 83/200 Iteration: 25200 Train loss: 0.397608 Train acc: 0.810000\n",
      "Epoch: 84/200 Iteration: 25250 Train loss: 0.580581 Train acc: 0.690000\n",
      "Epoch: 84/200 Iteration: 25250 Validation loss: 0.534779 Validation acc: 0.707350\n",
      "Epoch: 84/200 Iteration: 25300 Train loss: 0.584406 Train acc: 0.750000\n",
      "Epoch: 84/200 Iteration: 25350 Train loss: 0.467420 Train acc: 0.775000\n",
      "Epoch: 84/200 Iteration: 25400 Train loss: 0.531235 Train acc: 0.740000\n",
      "Epoch: 84/200 Iteration: 25450 Train loss: 0.393926 Train acc: 0.840000\n",
      "Epoch: 84/200 Iteration: 25500 Train loss: 0.382592 Train acc: 0.815000\n",
      "Epoch: 84/200 Iteration: 25500 Validation loss: 0.535441 Validation acc: 0.704750\n",
      "Epoch: 85/200 Iteration: 25550 Train loss: 0.597213 Train acc: 0.670000\n",
      "Epoch: 85/200 Iteration: 25600 Train loss: 0.578691 Train acc: 0.765000\n",
      "Epoch: 85/200 Iteration: 25650 Train loss: 0.479488 Train acc: 0.780000\n",
      "Epoch: 85/200 Iteration: 25700 Train loss: 0.512225 Train acc: 0.750000\n",
      "Epoch: 85/200 Iteration: 25750 Train loss: 0.395721 Train acc: 0.840000\n",
      "Epoch: 85/200 Iteration: 25750 Validation loss: 0.535335 Validation acc: 0.705350\n",
      "Epoch: 85/200 Iteration: 25800 Train loss: 0.398893 Train acc: 0.830000\n",
      "Epoch: 86/200 Iteration: 25850 Train loss: 0.581539 Train acc: 0.695000\n",
      "Epoch: 86/200 Iteration: 25900 Train loss: 0.594650 Train acc: 0.740000\n",
      "Epoch: 86/200 Iteration: 25950 Train loss: 0.479021 Train acc: 0.760000\n",
      "Epoch: 86/200 Iteration: 26000 Train loss: 0.498590 Train acc: 0.745000\n",
      "Epoch: 86/200 Iteration: 26000 Validation loss: 0.537125 Validation acc: 0.703200\n",
      "Epoch: 86/200 Iteration: 26050 Train loss: 0.399617 Train acc: 0.820000\n",
      "Epoch: 86/200 Iteration: 26100 Train loss: 0.395617 Train acc: 0.805000\n",
      "Epoch: 87/200 Iteration: 26150 Train loss: 0.585190 Train acc: 0.720000\n",
      "Epoch: 87/200 Iteration: 26200 Train loss: 0.594284 Train acc: 0.765000\n",
      "Epoch: 87/200 Iteration: 26250 Train loss: 0.471479 Train acc: 0.760000\n",
      "Epoch: 87/200 Iteration: 26250 Validation loss: 0.537292 Validation acc: 0.703500\n",
      "Epoch: 87/200 Iteration: 26300 Train loss: 0.507390 Train acc: 0.735000\n",
      "Epoch: 87/200 Iteration: 26350 Train loss: 0.391673 Train acc: 0.830000\n",
      "Epoch: 87/200 Iteration: 26400 Train loss: 0.408027 Train acc: 0.780000\n",
      "Epoch: 88/200 Iteration: 26450 Train loss: 0.598656 Train acc: 0.665000\n",
      "Epoch: 88/200 Iteration: 26500 Train loss: 0.587430 Train acc: 0.750000\n",
      "Epoch: 88/200 Iteration: 26500 Validation loss: 0.537321 Validation acc: 0.707500\n",
      "Epoch: 88/200 Iteration: 26550 Train loss: 0.494355 Train acc: 0.765000\n",
      "Epoch: 88/200 Iteration: 26600 Train loss: 0.500067 Train acc: 0.760000\n",
      "Epoch: 88/200 Iteration: 26650 Train loss: 0.416087 Train acc: 0.800000\n",
      "Epoch: 88/200 Iteration: 26700 Train loss: 0.403935 Train acc: 0.815000\n",
      "Epoch: 89/200 Iteration: 26750 Train loss: 0.584843 Train acc: 0.690000\n",
      "Epoch: 89/200 Iteration: 26750 Validation loss: 0.537770 Validation acc: 0.704950\n",
      "Epoch: 89/200 Iteration: 26800 Train loss: 0.587337 Train acc: 0.745000\n",
      "Epoch: 89/200 Iteration: 26850 Train loss: 0.481542 Train acc: 0.765000\n",
      "Epoch: 89/200 Iteration: 26900 Train loss: 0.532324 Train acc: 0.735000\n",
      "Epoch: 89/200 Iteration: 26950 Train loss: 0.400070 Train acc: 0.835000\n",
      "Epoch: 89/200 Iteration: 27000 Train loss: 0.393607 Train acc: 0.835000\n",
      "Epoch: 89/200 Iteration: 27000 Validation loss: 0.537876 Validation acc: 0.705150\n",
      "Epoch: 90/200 Iteration: 27050 Train loss: 0.576890 Train acc: 0.715000\n",
      "Epoch: 90/200 Iteration: 27100 Train loss: 0.595056 Train acc: 0.725000\n",
      "Epoch: 90/200 Iteration: 27150 Train loss: 0.478336 Train acc: 0.770000\n",
      "Epoch: 90/200 Iteration: 27200 Train loss: 0.513790 Train acc: 0.730000\n",
      "Epoch: 90/200 Iteration: 27250 Train loss: 0.390858 Train acc: 0.860000\n",
      "Epoch: 90/200 Iteration: 27250 Validation loss: 0.536962 Validation acc: 0.704350\n",
      "Epoch: 90/200 Iteration: 27300 Train loss: 0.387566 Train acc: 0.800000\n",
      "Epoch: 91/200 Iteration: 27350 Train loss: 0.560344 Train acc: 0.700000\n",
      "Epoch: 91/200 Iteration: 27400 Train loss: 0.600966 Train acc: 0.755000\n",
      "Epoch: 91/200 Iteration: 27450 Train loss: 0.481563 Train acc: 0.750000\n",
      "Epoch: 91/200 Iteration: 27500 Train loss: 0.508980 Train acc: 0.740000\n",
      "Epoch: 91/200 Iteration: 27500 Validation loss: 0.539048 Validation acc: 0.699600\n",
      "Epoch: 91/200 Iteration: 27550 Train loss: 0.399245 Train acc: 0.840000\n",
      "Epoch: 91/200 Iteration: 27600 Train loss: 0.396453 Train acc: 0.830000\n",
      "Epoch: 92/200 Iteration: 27650 Train loss: 0.578628 Train acc: 0.720000\n",
      "Epoch: 92/200 Iteration: 27700 Train loss: 0.571078 Train acc: 0.770000\n",
      "Epoch: 92/200 Iteration: 27750 Train loss: 0.489318 Train acc: 0.775000\n",
      "Epoch: 92/200 Iteration: 27750 Validation loss: 0.538896 Validation acc: 0.700950\n",
      "Epoch: 92/200 Iteration: 27800 Train loss: 0.496055 Train acc: 0.750000\n",
      "Epoch: 92/200 Iteration: 27850 Train loss: 0.400220 Train acc: 0.835000\n",
      "Epoch: 92/200 Iteration: 27900 Train loss: 0.399589 Train acc: 0.820000\n",
      "Epoch: 93/200 Iteration: 27950 Train loss: 0.563112 Train acc: 0.695000\n",
      "Epoch: 93/200 Iteration: 28000 Train loss: 0.597817 Train acc: 0.770000\n",
      "Epoch: 93/200 Iteration: 28000 Validation loss: 0.537063 Validation acc: 0.707500\n",
      "Epoch: 93/200 Iteration: 28050 Train loss: 0.492248 Train acc: 0.740000\n",
      "Epoch: 93/200 Iteration: 28100 Train loss: 0.499181 Train acc: 0.745000\n",
      "Epoch: 93/200 Iteration: 28150 Train loss: 0.400691 Train acc: 0.830000\n",
      "Epoch: 93/200 Iteration: 28200 Train loss: 0.397298 Train acc: 0.800000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/200 Iteration: 28250 Train loss: 0.575822 Train acc: 0.695000\n",
      "Epoch: 94/200 Iteration: 28250 Validation loss: 0.538421 Validation acc: 0.706400\n",
      "Epoch: 94/200 Iteration: 28300 Train loss: 0.600639 Train acc: 0.760000\n",
      "Epoch: 94/200 Iteration: 28350 Train loss: 0.481711 Train acc: 0.745000\n",
      "Epoch: 94/200 Iteration: 28400 Train loss: 0.509726 Train acc: 0.745000\n",
      "Epoch: 94/200 Iteration: 28450 Train loss: 0.383116 Train acc: 0.850000\n",
      "Epoch: 94/200 Iteration: 28500 Train loss: 0.369225 Train acc: 0.840000\n",
      "Epoch: 94/200 Iteration: 28500 Validation loss: 0.538009 Validation acc: 0.705650\n",
      "Epoch: 95/200 Iteration: 28550 Train loss: 0.566338 Train acc: 0.700000\n",
      "Epoch: 95/200 Iteration: 28600 Train loss: 0.577950 Train acc: 0.760000\n",
      "Epoch: 95/200 Iteration: 28650 Train loss: 0.472565 Train acc: 0.770000\n",
      "Epoch: 95/200 Iteration: 28700 Train loss: 0.525163 Train acc: 0.740000\n",
      "Epoch: 95/200 Iteration: 28750 Train loss: 0.399708 Train acc: 0.840000\n",
      "Epoch: 95/200 Iteration: 28750 Validation loss: 0.537732 Validation acc: 0.705600\n",
      "Epoch: 95/200 Iteration: 28800 Train loss: 0.389975 Train acc: 0.810000\n",
      "Epoch: 96/200 Iteration: 28850 Train loss: 0.567649 Train acc: 0.695000\n",
      "Epoch: 96/200 Iteration: 28900 Train loss: 0.571759 Train acc: 0.775000\n",
      "Epoch: 96/200 Iteration: 28950 Train loss: 0.479581 Train acc: 0.770000\n",
      "Epoch: 96/200 Iteration: 29000 Train loss: 0.513061 Train acc: 0.755000\n",
      "Epoch: 96/200 Iteration: 29000 Validation loss: 0.540667 Validation acc: 0.703500\n",
      "Epoch: 96/200 Iteration: 29050 Train loss: 0.399768 Train acc: 0.820000\n",
      "Epoch: 96/200 Iteration: 29100 Train loss: 0.372589 Train acc: 0.830000\n",
      "Epoch: 97/200 Iteration: 29150 Train loss: 0.559958 Train acc: 0.720000\n",
      "Epoch: 97/200 Iteration: 29200 Train loss: 0.617281 Train acc: 0.730000\n",
      "Epoch: 97/200 Iteration: 29250 Train loss: 0.485885 Train acc: 0.765000\n",
      "Epoch: 97/200 Iteration: 29250 Validation loss: 0.539980 Validation acc: 0.702950\n",
      "Epoch: 97/200 Iteration: 29300 Train loss: 0.519926 Train acc: 0.730000\n",
      "Epoch: 97/200 Iteration: 29350 Train loss: 0.389474 Train acc: 0.855000\n",
      "Epoch: 97/200 Iteration: 29400 Train loss: 0.375749 Train acc: 0.805000\n",
      "Epoch: 98/200 Iteration: 29450 Train loss: 0.588981 Train acc: 0.705000\n",
      "Epoch: 98/200 Iteration: 29500 Train loss: 0.575901 Train acc: 0.770000\n",
      "Epoch: 98/200 Iteration: 29500 Validation loss: 0.539367 Validation acc: 0.705550\n",
      "Epoch: 98/200 Iteration: 29550 Train loss: 0.470214 Train acc: 0.770000\n",
      "Epoch: 98/200 Iteration: 29600 Train loss: 0.502519 Train acc: 0.745000\n",
      "Epoch: 98/200 Iteration: 29650 Train loss: 0.384714 Train acc: 0.835000\n",
      "Epoch: 98/200 Iteration: 29700 Train loss: 0.389139 Train acc: 0.820000\n",
      "Epoch: 99/200 Iteration: 29750 Train loss: 0.565611 Train acc: 0.695000\n",
      "Epoch: 99/200 Iteration: 29750 Validation loss: 0.540435 Validation acc: 0.703300\n",
      "Epoch: 99/200 Iteration: 29800 Train loss: 0.569217 Train acc: 0.775000\n",
      "Epoch: 99/200 Iteration: 29850 Train loss: 0.478632 Train acc: 0.780000\n",
      "Epoch: 99/200 Iteration: 29900 Train loss: 0.538726 Train acc: 0.720000\n",
      "Epoch: 99/200 Iteration: 29950 Train loss: 0.369236 Train acc: 0.870000\n",
      "Epoch: 99/200 Iteration: 30000 Train loss: 0.388624 Train acc: 0.810000\n",
      "Epoch: 99/200 Iteration: 30000 Validation loss: 0.541354 Validation acc: 0.703900\n",
      "Epoch: 100/200 Iteration: 30050 Train loss: 0.590140 Train acc: 0.715000\n",
      "Epoch: 100/200 Iteration: 30100 Train loss: 0.564420 Train acc: 0.755000\n",
      "Epoch: 100/200 Iteration: 30150 Train loss: 0.470661 Train acc: 0.750000\n",
      "Epoch: 100/200 Iteration: 30200 Train loss: 0.514573 Train acc: 0.730000\n",
      "Epoch: 100/200 Iteration: 30250 Train loss: 0.404751 Train acc: 0.815000\n",
      "Epoch: 100/200 Iteration: 30250 Validation loss: 0.540295 Validation acc: 0.702150\n",
      "Epoch: 100/200 Iteration: 30300 Train loss: 0.400468 Train acc: 0.800000\n",
      "Epoch: 101/200 Iteration: 30350 Train loss: 0.584243 Train acc: 0.685000\n",
      "Epoch: 101/200 Iteration: 30400 Train loss: 0.573759 Train acc: 0.755000\n",
      "Epoch: 101/200 Iteration: 30450 Train loss: 0.467373 Train acc: 0.780000\n",
      "Epoch: 101/200 Iteration: 30500 Train loss: 0.524951 Train acc: 0.735000\n",
      "Epoch: 101/200 Iteration: 30500 Validation loss: 0.542159 Validation acc: 0.702000\n",
      "Epoch: 101/200 Iteration: 30550 Train loss: 0.391083 Train acc: 0.830000\n",
      "Epoch: 101/200 Iteration: 30600 Train loss: 0.410095 Train acc: 0.800000\n",
      "Epoch: 102/200 Iteration: 30650 Train loss: 0.574472 Train acc: 0.675000\n",
      "Epoch: 102/200 Iteration: 30700 Train loss: 0.565037 Train acc: 0.750000\n",
      "Epoch: 102/200 Iteration: 30750 Train loss: 0.472476 Train acc: 0.775000\n",
      "Epoch: 102/200 Iteration: 30750 Validation loss: 0.542685 Validation acc: 0.701750\n",
      "Epoch: 102/200 Iteration: 30800 Train loss: 0.515947 Train acc: 0.730000\n",
      "Epoch: 102/200 Iteration: 30850 Train loss: 0.397189 Train acc: 0.820000\n",
      "Epoch: 102/200 Iteration: 30900 Train loss: 0.392549 Train acc: 0.825000\n",
      "Epoch: 103/200 Iteration: 30950 Train loss: 0.573584 Train acc: 0.715000\n",
      "Epoch: 103/200 Iteration: 31000 Train loss: 0.583790 Train acc: 0.750000\n",
      "Epoch: 103/200 Iteration: 31000 Validation loss: 0.540094 Validation acc: 0.706000\n",
      "Epoch: 103/200 Iteration: 31050 Train loss: 0.462822 Train acc: 0.770000\n",
      "Epoch: 103/200 Iteration: 31100 Train loss: 0.499336 Train acc: 0.740000\n",
      "Epoch: 103/200 Iteration: 31150 Train loss: 0.375445 Train acc: 0.840000\n",
      "Epoch: 103/200 Iteration: 31200 Train loss: 0.384964 Train acc: 0.800000\n",
      "Epoch: 104/200 Iteration: 31250 Train loss: 0.575725 Train acc: 0.695000\n",
      "Epoch: 104/200 Iteration: 31250 Validation loss: 0.540880 Validation acc: 0.703850\n",
      "Epoch: 104/200 Iteration: 31300 Train loss: 0.591955 Train acc: 0.785000\n",
      "Epoch: 104/200 Iteration: 31350 Train loss: 0.461901 Train acc: 0.785000\n",
      "Epoch: 104/200 Iteration: 31400 Train loss: 0.512647 Train acc: 0.750000\n",
      "Epoch: 104/200 Iteration: 31450 Train loss: 0.406535 Train acc: 0.845000\n",
      "Epoch: 104/200 Iteration: 31500 Train loss: 0.390855 Train acc: 0.800000\n",
      "Epoch: 104/200 Iteration: 31500 Validation loss: 0.541931 Validation acc: 0.703550\n",
      "Epoch: 105/200 Iteration: 31550 Train loss: 0.576154 Train acc: 0.725000\n",
      "Epoch: 105/200 Iteration: 31600 Train loss: 0.549512 Train acc: 0.755000\n",
      "Epoch: 105/200 Iteration: 31650 Train loss: 0.473631 Train acc: 0.750000\n",
      "Epoch: 105/200 Iteration: 31700 Train loss: 0.495856 Train acc: 0.760000\n",
      "Epoch: 105/200 Iteration: 31750 Train loss: 0.369233 Train acc: 0.840000\n",
      "Epoch: 105/200 Iteration: 31750 Validation loss: 0.541227 Validation acc: 0.706600\n",
      "Epoch: 105/200 Iteration: 31800 Train loss: 0.385400 Train acc: 0.825000\n",
      "Epoch: 106/200 Iteration: 31850 Train loss: 0.568564 Train acc: 0.700000\n",
      "Epoch: 106/200 Iteration: 31900 Train loss: 0.577105 Train acc: 0.765000\n",
      "Epoch: 106/200 Iteration: 31950 Train loss: 0.471824 Train acc: 0.755000\n",
      "Epoch: 106/200 Iteration: 32000 Train loss: 0.502701 Train acc: 0.740000\n",
      "Epoch: 106/200 Iteration: 32000 Validation loss: 0.544725 Validation acc: 0.702400\n",
      "Epoch: 106/200 Iteration: 32050 Train loss: 0.382532 Train acc: 0.855000\n",
      "Epoch: 106/200 Iteration: 32100 Train loss: 0.398519 Train acc: 0.815000\n",
      "Epoch: 107/200 Iteration: 32150 Train loss: 0.584619 Train acc: 0.720000\n",
      "Epoch: 107/200 Iteration: 32200 Train loss: 0.555945 Train acc: 0.765000\n",
      "Epoch: 107/200 Iteration: 32250 Train loss: 0.458534 Train acc: 0.780000\n",
      "Epoch: 107/200 Iteration: 32250 Validation loss: 0.545048 Validation acc: 0.701200\n",
      "Epoch: 107/200 Iteration: 32300 Train loss: 0.531665 Train acc: 0.740000\n",
      "Epoch: 107/200 Iteration: 32350 Train loss: 0.407279 Train acc: 0.825000\n",
      "Epoch: 107/200 Iteration: 32400 Train loss: 0.408458 Train acc: 0.815000\n",
      "Epoch: 108/200 Iteration: 32450 Train loss: 0.571497 Train acc: 0.700000\n",
      "Epoch: 108/200 Iteration: 32500 Train loss: 0.555179 Train acc: 0.785000\n",
      "Epoch: 108/200 Iteration: 32500 Validation loss: 0.543467 Validation acc: 0.704150\n",
      "Epoch: 108/200 Iteration: 32550 Train loss: 0.466749 Train acc: 0.770000\n",
      "Epoch: 108/200 Iteration: 32600 Train loss: 0.494213 Train acc: 0.750000\n",
      "Epoch: 108/200 Iteration: 32650 Train loss: 0.393186 Train acc: 0.820000\n",
      "Epoch: 108/200 Iteration: 32700 Train loss: 0.399939 Train acc: 0.810000\n",
      "Epoch: 109/200 Iteration: 32750 Train loss: 0.584154 Train acc: 0.735000\n",
      "Epoch: 109/200 Iteration: 32750 Validation loss: 0.544294 Validation acc: 0.702200\n",
      "Epoch: 109/200 Iteration: 32800 Train loss: 0.586674 Train acc: 0.785000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109/200 Iteration: 32850 Train loss: 0.465512 Train acc: 0.760000\n",
      "Epoch: 109/200 Iteration: 32900 Train loss: 0.493008 Train acc: 0.745000\n",
      "Epoch: 109/200 Iteration: 32950 Train loss: 0.391636 Train acc: 0.815000\n",
      "Epoch: 109/200 Iteration: 33000 Train loss: 0.398605 Train acc: 0.810000\n",
      "Epoch: 109/200 Iteration: 33000 Validation loss: 0.545974 Validation acc: 0.702950\n",
      "Epoch: 110/200 Iteration: 33050 Train loss: 0.600470 Train acc: 0.690000\n",
      "Epoch: 110/200 Iteration: 33100 Train loss: 0.533303 Train acc: 0.740000\n",
      "Epoch: 110/200 Iteration: 33150 Train loss: 0.458434 Train acc: 0.760000\n",
      "Epoch: 110/200 Iteration: 33200 Train loss: 0.514963 Train acc: 0.740000\n",
      "Epoch: 110/200 Iteration: 33250 Train loss: 0.382360 Train acc: 0.860000\n",
      "Epoch: 110/200 Iteration: 33250 Validation loss: 0.543809 Validation acc: 0.705200\n",
      "Epoch: 110/200 Iteration: 33300 Train loss: 0.386941 Train acc: 0.830000\n",
      "Epoch: 111/200 Iteration: 33350 Train loss: 0.563028 Train acc: 0.700000\n",
      "Epoch: 111/200 Iteration: 33400 Train loss: 0.561414 Train acc: 0.760000\n",
      "Epoch: 111/200 Iteration: 33450 Train loss: 0.461571 Train acc: 0.770000\n",
      "Epoch: 111/200 Iteration: 33500 Train loss: 0.501971 Train acc: 0.745000\n",
      "Epoch: 111/200 Iteration: 33500 Validation loss: 0.546971 Validation acc: 0.702300\n",
      "Epoch: 111/200 Iteration: 33550 Train loss: 0.400863 Train acc: 0.815000\n",
      "Epoch: 111/200 Iteration: 33600 Train loss: 0.400597 Train acc: 0.810000\n",
      "Epoch: 112/200 Iteration: 33650 Train loss: 0.583510 Train acc: 0.685000\n",
      "Epoch: 112/200 Iteration: 33700 Train loss: 0.575405 Train acc: 0.750000\n",
      "Epoch: 112/200 Iteration: 33750 Train loss: 0.468968 Train acc: 0.770000\n",
      "Epoch: 112/200 Iteration: 33750 Validation loss: 0.549343 Validation acc: 0.701000\n",
      "Epoch: 112/200 Iteration: 33800 Train loss: 0.512286 Train acc: 0.735000\n",
      "Epoch: 112/200 Iteration: 33850 Train loss: 0.397567 Train acc: 0.840000\n",
      "Epoch: 112/200 Iteration: 33900 Train loss: 0.376354 Train acc: 0.830000\n",
      "Epoch: 113/200 Iteration: 33950 Train loss: 0.567742 Train acc: 0.725000\n",
      "Epoch: 113/200 Iteration: 34000 Train loss: 0.586867 Train acc: 0.760000\n",
      "Epoch: 113/200 Iteration: 34000 Validation loss: 0.548183 Validation acc: 0.703500\n",
      "Epoch: 113/200 Iteration: 34050 Train loss: 0.474222 Train acc: 0.780000\n",
      "Epoch: 113/200 Iteration: 34100 Train loss: 0.513429 Train acc: 0.740000\n",
      "Epoch: 113/200 Iteration: 34150 Train loss: 0.379039 Train acc: 0.835000\n",
      "Epoch: 113/200 Iteration: 34200 Train loss: 0.382055 Train acc: 0.815000\n",
      "Epoch: 114/200 Iteration: 34250 Train loss: 0.565958 Train acc: 0.715000\n",
      "Epoch: 114/200 Iteration: 34250 Validation loss: 0.546730 Validation acc: 0.704000\n",
      "Epoch: 114/200 Iteration: 34300 Train loss: 0.564815 Train acc: 0.755000\n",
      "Epoch: 114/200 Iteration: 34350 Train loss: 0.488186 Train acc: 0.765000\n",
      "Epoch: 114/200 Iteration: 34400 Train loss: 0.506035 Train acc: 0.750000\n",
      "Epoch: 114/200 Iteration: 34450 Train loss: 0.384851 Train acc: 0.830000\n",
      "Epoch: 114/200 Iteration: 34500 Train loss: 0.371989 Train acc: 0.845000\n",
      "Epoch: 114/200 Iteration: 34500 Validation loss: 0.547931 Validation acc: 0.702800\n",
      "Epoch: 115/200 Iteration: 34550 Train loss: 0.556798 Train acc: 0.715000\n",
      "Epoch: 115/200 Iteration: 34600 Train loss: 0.596777 Train acc: 0.745000\n",
      "Epoch: 115/200 Iteration: 34650 Train loss: 0.478981 Train acc: 0.775000\n",
      "Epoch: 115/200 Iteration: 34700 Train loss: 0.534292 Train acc: 0.705000\n",
      "Epoch: 115/200 Iteration: 34750 Train loss: 0.377621 Train acc: 0.830000\n",
      "Epoch: 115/200 Iteration: 34750 Validation loss: 0.546030 Validation acc: 0.704100\n",
      "Epoch: 115/200 Iteration: 34800 Train loss: 0.392312 Train acc: 0.835000\n",
      "Epoch: 116/200 Iteration: 34850 Train loss: 0.554933 Train acc: 0.705000\n",
      "Epoch: 116/200 Iteration: 34900 Train loss: 0.571912 Train acc: 0.765000\n",
      "Epoch: 116/200 Iteration: 34950 Train loss: 0.468861 Train acc: 0.780000\n",
      "Epoch: 116/200 Iteration: 35000 Train loss: 0.494205 Train acc: 0.760000\n",
      "Epoch: 116/200 Iteration: 35000 Validation loss: 0.550143 Validation acc: 0.702400\n",
      "Epoch: 116/200 Iteration: 35050 Train loss: 0.389861 Train acc: 0.845000\n",
      "Epoch: 116/200 Iteration: 35100 Train loss: 0.373940 Train acc: 0.830000\n",
      "Epoch: 117/200 Iteration: 35150 Train loss: 0.556290 Train acc: 0.680000\n",
      "Epoch: 117/200 Iteration: 35200 Train loss: 0.567180 Train acc: 0.765000\n",
      "Epoch: 117/200 Iteration: 35250 Train loss: 0.474662 Train acc: 0.750000\n",
      "Epoch: 117/200 Iteration: 35250 Validation loss: 0.551430 Validation acc: 0.704250\n",
      "Epoch: 117/200 Iteration: 35300 Train loss: 0.518020 Train acc: 0.735000\n",
      "Epoch: 117/200 Iteration: 35350 Train loss: 0.386611 Train acc: 0.835000\n",
      "Epoch: 117/200 Iteration: 35400 Train loss: 0.379565 Train acc: 0.825000\n",
      "Epoch: 118/200 Iteration: 35450 Train loss: 0.568631 Train acc: 0.705000\n",
      "Epoch: 118/200 Iteration: 35500 Train loss: 0.552416 Train acc: 0.755000\n",
      "Epoch: 118/200 Iteration: 35500 Validation loss: 0.550883 Validation acc: 0.706450\n",
      "Epoch: 118/200 Iteration: 35550 Train loss: 0.471555 Train acc: 0.770000\n",
      "Epoch: 118/200 Iteration: 35600 Train loss: 0.516346 Train acc: 0.730000\n",
      "Epoch: 118/200 Iteration: 35650 Train loss: 0.371946 Train acc: 0.845000\n",
      "Epoch: 118/200 Iteration: 35700 Train loss: 0.383868 Train acc: 0.835000\n",
      "Epoch: 119/200 Iteration: 35750 Train loss: 0.562197 Train acc: 0.710000\n",
      "Epoch: 119/200 Iteration: 35750 Validation loss: 0.550329 Validation acc: 0.704750\n",
      "Epoch: 119/200 Iteration: 35800 Train loss: 0.561640 Train acc: 0.765000\n",
      "Epoch: 119/200 Iteration: 35850 Train loss: 0.468497 Train acc: 0.760000\n",
      "Epoch: 119/200 Iteration: 35900 Train loss: 0.510243 Train acc: 0.745000\n",
      "Epoch: 119/200 Iteration: 35950 Train loss: 0.386168 Train acc: 0.830000\n",
      "Epoch: 119/200 Iteration: 36000 Train loss: 0.379272 Train acc: 0.815000\n",
      "Epoch: 119/200 Iteration: 36000 Validation loss: 0.550794 Validation acc: 0.705200\n",
      "Epoch: 120/200 Iteration: 36050 Train loss: 0.563096 Train acc: 0.705000\n",
      "Epoch: 120/200 Iteration: 36100 Train loss: 0.571339 Train acc: 0.760000\n",
      "Epoch: 120/200 Iteration: 36150 Train loss: 0.462809 Train acc: 0.770000\n",
      "Epoch: 120/200 Iteration: 36200 Train loss: 0.497264 Train acc: 0.745000\n",
      "Epoch: 120/200 Iteration: 36250 Train loss: 0.376168 Train acc: 0.835000\n",
      "Epoch: 120/200 Iteration: 36250 Validation loss: 0.548872 Validation acc: 0.703550\n",
      "Epoch: 120/200 Iteration: 36300 Train loss: 0.381202 Train acc: 0.820000\n",
      "Epoch: 121/200 Iteration: 36350 Train loss: 0.569375 Train acc: 0.685000\n",
      "Epoch: 121/200 Iteration: 36400 Train loss: 0.590011 Train acc: 0.745000\n",
      "Epoch: 121/200 Iteration: 36450 Train loss: 0.464936 Train acc: 0.765000\n",
      "Epoch: 121/200 Iteration: 36500 Train loss: 0.523940 Train acc: 0.730000\n",
      "Epoch: 121/200 Iteration: 36500 Validation loss: 0.553743 Validation acc: 0.704250\n",
      "Epoch: 121/200 Iteration: 36550 Train loss: 0.383729 Train acc: 0.820000\n",
      "Epoch: 121/200 Iteration: 36600 Train loss: 0.398582 Train acc: 0.810000\n",
      "Epoch: 122/200 Iteration: 36650 Train loss: 0.554529 Train acc: 0.700000\n",
      "Epoch: 122/200 Iteration: 36700 Train loss: 0.556051 Train acc: 0.760000\n",
      "Epoch: 122/200 Iteration: 36750 Train loss: 0.454944 Train acc: 0.770000\n",
      "Epoch: 122/200 Iteration: 36750 Validation loss: 0.554524 Validation acc: 0.703550\n",
      "Epoch: 122/200 Iteration: 36800 Train loss: 0.494511 Train acc: 0.755000\n",
      "Epoch: 122/200 Iteration: 36850 Train loss: 0.380752 Train acc: 0.840000\n",
      "Epoch: 122/200 Iteration: 36900 Train loss: 0.375143 Train acc: 0.810000\n",
      "Epoch: 123/200 Iteration: 36950 Train loss: 0.563268 Train acc: 0.705000\n",
      "Epoch: 123/200 Iteration: 37000 Train loss: 0.563668 Train acc: 0.740000\n",
      "Epoch: 123/200 Iteration: 37000 Validation loss: 0.552625 Validation acc: 0.705850\n",
      "Epoch: 123/200 Iteration: 37050 Train loss: 0.465977 Train acc: 0.755000\n",
      "Epoch: 123/200 Iteration: 37100 Train loss: 0.515319 Train acc: 0.740000\n",
      "Epoch: 123/200 Iteration: 37150 Train loss: 0.371888 Train acc: 0.850000\n",
      "Epoch: 123/200 Iteration: 37200 Train loss: 0.388069 Train acc: 0.820000\n",
      "Epoch: 124/200 Iteration: 37250 Train loss: 0.571596 Train acc: 0.710000\n",
      "Epoch: 124/200 Iteration: 37250 Validation loss: 0.554016 Validation acc: 0.704250\n",
      "Epoch: 124/200 Iteration: 37300 Train loss: 0.543464 Train acc: 0.765000\n",
      "Epoch: 124/200 Iteration: 37350 Train loss: 0.476778 Train acc: 0.770000\n",
      "Epoch: 124/200 Iteration: 37400 Train loss: 0.507353 Train acc: 0.735000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124/200 Iteration: 37450 Train loss: 0.381527 Train acc: 0.845000\n",
      "Epoch: 124/200 Iteration: 37500 Train loss: 0.374479 Train acc: 0.835000\n",
      "Epoch: 124/200 Iteration: 37500 Validation loss: 0.552935 Validation acc: 0.705800\n",
      "Epoch: 125/200 Iteration: 37550 Train loss: 0.552004 Train acc: 0.710000\n",
      "Epoch: 125/200 Iteration: 37600 Train loss: 0.564671 Train acc: 0.785000\n",
      "Epoch: 125/200 Iteration: 37650 Train loss: 0.483076 Train acc: 0.775000\n",
      "Epoch: 125/200 Iteration: 37700 Train loss: 0.512088 Train acc: 0.735000\n",
      "Epoch: 125/200 Iteration: 37750 Train loss: 0.378226 Train acc: 0.845000\n",
      "Epoch: 125/200 Iteration: 37750 Validation loss: 0.549371 Validation acc: 0.705400\n",
      "Epoch: 125/200 Iteration: 37800 Train loss: 0.412914 Train acc: 0.800000\n",
      "Epoch: 126/200 Iteration: 37850 Train loss: 0.559531 Train acc: 0.710000\n",
      "Epoch: 126/200 Iteration: 37900 Train loss: 0.558101 Train acc: 0.750000\n",
      "Epoch: 126/200 Iteration: 37950 Train loss: 0.471189 Train acc: 0.785000\n",
      "Epoch: 126/200 Iteration: 38000 Train loss: 0.511707 Train acc: 0.740000\n",
      "Epoch: 126/200 Iteration: 38000 Validation loss: 0.554016 Validation acc: 0.704500\n",
      "Epoch: 126/200 Iteration: 38050 Train loss: 0.375371 Train acc: 0.850000\n",
      "Epoch: 126/200 Iteration: 38100 Train loss: 0.385670 Train acc: 0.825000\n",
      "Epoch: 127/200 Iteration: 38150 Train loss: 0.570625 Train acc: 0.710000\n",
      "Epoch: 127/200 Iteration: 38200 Train loss: 0.561528 Train acc: 0.765000\n",
      "Epoch: 127/200 Iteration: 38250 Train loss: 0.462838 Train acc: 0.770000\n",
      "Epoch: 127/200 Iteration: 38250 Validation loss: 0.555493 Validation acc: 0.705050\n",
      "Epoch: 127/200 Iteration: 38300 Train loss: 0.521848 Train acc: 0.735000\n",
      "Epoch: 127/200 Iteration: 38350 Train loss: 0.371638 Train acc: 0.855000\n",
      "Epoch: 127/200 Iteration: 38400 Train loss: 0.386891 Train acc: 0.835000\n",
      "Epoch: 128/200 Iteration: 38450 Train loss: 0.550634 Train acc: 0.730000\n",
      "Epoch: 128/200 Iteration: 38500 Train loss: 0.573974 Train acc: 0.770000\n",
      "Epoch: 128/200 Iteration: 38500 Validation loss: 0.553743 Validation acc: 0.704100\n",
      "Epoch: 128/200 Iteration: 38550 Train loss: 0.453013 Train acc: 0.790000\n",
      "Epoch: 128/200 Iteration: 38600 Train loss: 0.501290 Train acc: 0.735000\n",
      "Epoch: 128/200 Iteration: 38650 Train loss: 0.370381 Train acc: 0.835000\n",
      "Epoch: 128/200 Iteration: 38700 Train loss: 0.383521 Train acc: 0.825000\n",
      "Epoch: 129/200 Iteration: 38750 Train loss: 0.535544 Train acc: 0.715000\n",
      "Epoch: 129/200 Iteration: 38750 Validation loss: 0.553458 Validation acc: 0.704650\n",
      "Epoch: 129/200 Iteration: 38800 Train loss: 0.557297 Train acc: 0.750000\n",
      "Epoch: 129/200 Iteration: 38850 Train loss: 0.443554 Train acc: 0.780000\n",
      "Epoch: 129/200 Iteration: 38900 Train loss: 0.516862 Train acc: 0.745000\n",
      "Epoch: 129/200 Iteration: 38950 Train loss: 0.382613 Train acc: 0.825000\n",
      "Epoch: 129/200 Iteration: 39000 Train loss: 0.376363 Train acc: 0.830000\n",
      "Epoch: 129/200 Iteration: 39000 Validation loss: 0.552826 Validation acc: 0.702850\n",
      "Epoch: 130/200 Iteration: 39050 Train loss: 0.578281 Train acc: 0.675000\n",
      "Epoch: 130/200 Iteration: 39100 Train loss: 0.596719 Train acc: 0.775000\n",
      "Epoch: 130/200 Iteration: 39150 Train loss: 0.461195 Train acc: 0.775000\n",
      "Epoch: 130/200 Iteration: 39200 Train loss: 0.486315 Train acc: 0.740000\n",
      "Epoch: 130/200 Iteration: 39250 Train loss: 0.371948 Train acc: 0.835000\n",
      "Epoch: 130/200 Iteration: 39250 Validation loss: 0.550339 Validation acc: 0.704600\n",
      "Epoch: 130/200 Iteration: 39300 Train loss: 0.374853 Train acc: 0.825000\n",
      "Epoch: 131/200 Iteration: 39350 Train loss: 0.572089 Train acc: 0.700000\n",
      "Epoch: 131/200 Iteration: 39400 Train loss: 0.540728 Train acc: 0.770000\n",
      "Epoch: 131/200 Iteration: 39450 Train loss: 0.444643 Train acc: 0.785000\n",
      "Epoch: 131/200 Iteration: 39500 Train loss: 0.516614 Train acc: 0.740000\n",
      "Epoch: 131/200 Iteration: 39500 Validation loss: 0.554216 Validation acc: 0.703900\n",
      "Epoch: 131/200 Iteration: 39550 Train loss: 0.377973 Train acc: 0.860000\n",
      "Epoch: 131/200 Iteration: 39600 Train loss: 0.389630 Train acc: 0.820000\n",
      "Epoch: 132/200 Iteration: 39650 Train loss: 0.555761 Train acc: 0.710000\n",
      "Epoch: 132/200 Iteration: 39700 Train loss: 0.566925 Train acc: 0.745000\n",
      "Epoch: 132/200 Iteration: 39750 Train loss: 0.449753 Train acc: 0.765000\n",
      "Epoch: 132/200 Iteration: 39750 Validation loss: 0.557040 Validation acc: 0.702950\n",
      "Epoch: 132/200 Iteration: 39800 Train loss: 0.515109 Train acc: 0.730000\n",
      "Epoch: 132/200 Iteration: 39850 Train loss: 0.375619 Train acc: 0.830000\n",
      "Epoch: 132/200 Iteration: 39900 Train loss: 0.377940 Train acc: 0.825000\n",
      "Epoch: 133/200 Iteration: 39950 Train loss: 0.560379 Train acc: 0.735000\n",
      "Epoch: 133/200 Iteration: 40000 Train loss: 0.573418 Train acc: 0.775000\n",
      "Epoch: 133/200 Iteration: 40000 Validation loss: 0.554327 Validation acc: 0.704100\n",
      "Epoch: 133/200 Iteration: 40050 Train loss: 0.456389 Train acc: 0.780000\n",
      "Epoch: 133/200 Iteration: 40100 Train loss: 0.520198 Train acc: 0.735000\n",
      "Epoch: 133/200 Iteration: 40150 Train loss: 0.374874 Train acc: 0.850000\n",
      "Epoch: 133/200 Iteration: 40200 Train loss: 0.382923 Train acc: 0.815000\n",
      "Epoch: 134/200 Iteration: 40250 Train loss: 0.568181 Train acc: 0.700000\n",
      "Epoch: 134/200 Iteration: 40250 Validation loss: 0.553884 Validation acc: 0.704400\n",
      "Epoch: 134/200 Iteration: 40300 Train loss: 0.534729 Train acc: 0.750000\n",
      "Epoch: 134/200 Iteration: 40350 Train loss: 0.452611 Train acc: 0.775000\n",
      "Epoch: 134/200 Iteration: 40400 Train loss: 0.509278 Train acc: 0.735000\n",
      "Epoch: 134/200 Iteration: 40450 Train loss: 0.370408 Train acc: 0.845000\n",
      "Epoch: 134/200 Iteration: 40500 Train loss: 0.378968 Train acc: 0.810000\n",
      "Epoch: 134/200 Iteration: 40500 Validation loss: 0.554480 Validation acc: 0.703900\n",
      "Epoch: 135/200 Iteration: 40550 Train loss: 0.562231 Train acc: 0.700000\n",
      "Epoch: 135/200 Iteration: 40600 Train loss: 0.556044 Train acc: 0.790000\n",
      "Epoch: 135/200 Iteration: 40650 Train loss: 0.472260 Train acc: 0.765000\n",
      "Epoch: 135/200 Iteration: 40700 Train loss: 0.500468 Train acc: 0.745000\n",
      "Epoch: 135/200 Iteration: 40750 Train loss: 0.372934 Train acc: 0.820000\n",
      "Epoch: 135/200 Iteration: 40750 Validation loss: 0.550899 Validation acc: 0.705050\n",
      "Epoch: 135/200 Iteration: 40800 Train loss: 0.386602 Train acc: 0.820000\n",
      "Epoch: 136/200 Iteration: 40850 Train loss: 0.560462 Train acc: 0.700000\n",
      "Epoch: 136/200 Iteration: 40900 Train loss: 0.566316 Train acc: 0.755000\n",
      "Epoch: 136/200 Iteration: 40950 Train loss: 0.475836 Train acc: 0.785000\n",
      "Epoch: 136/200 Iteration: 41000 Train loss: 0.505729 Train acc: 0.735000\n",
      "Epoch: 136/200 Iteration: 41000 Validation loss: 0.554211 Validation acc: 0.702600\n",
      "Epoch: 136/200 Iteration: 41050 Train loss: 0.374065 Train acc: 0.845000\n",
      "Epoch: 136/200 Iteration: 41100 Train loss: 0.388317 Train acc: 0.815000\n",
      "Epoch: 137/200 Iteration: 41150 Train loss: 0.555345 Train acc: 0.700000\n",
      "Epoch: 137/200 Iteration: 41200 Train loss: 0.543512 Train acc: 0.765000\n",
      "Epoch: 137/200 Iteration: 41250 Train loss: 0.449109 Train acc: 0.775000\n",
      "Epoch: 137/200 Iteration: 41250 Validation loss: 0.558107 Validation acc: 0.703500\n",
      "Epoch: 137/200 Iteration: 41300 Train loss: 0.485734 Train acc: 0.735000\n",
      "Epoch: 137/200 Iteration: 41350 Train loss: 0.382408 Train acc: 0.845000\n",
      "Epoch: 137/200 Iteration: 41400 Train loss: 0.391554 Train acc: 0.815000\n",
      "Epoch: 138/200 Iteration: 41450 Train loss: 0.543682 Train acc: 0.710000\n",
      "Epoch: 138/200 Iteration: 41500 Train loss: 0.576861 Train acc: 0.755000\n",
      "Epoch: 138/200 Iteration: 41500 Validation loss: 0.555521 Validation acc: 0.704950\n",
      "Epoch: 138/200 Iteration: 41550 Train loss: 0.455185 Train acc: 0.800000\n",
      "Epoch: 138/200 Iteration: 41600 Train loss: 0.496025 Train acc: 0.770000\n",
      "Epoch: 138/200 Iteration: 41650 Train loss: 0.379423 Train acc: 0.825000\n",
      "Epoch: 138/200 Iteration: 41700 Train loss: 0.422214 Train acc: 0.805000\n",
      "Epoch: 139/200 Iteration: 41750 Train loss: 0.569603 Train acc: 0.710000\n",
      "Epoch: 139/200 Iteration: 41750 Validation loss: 0.556065 Validation acc: 0.703250\n",
      "Epoch: 139/200 Iteration: 41800 Train loss: 0.557977 Train acc: 0.770000\n",
      "Epoch: 139/200 Iteration: 41850 Train loss: 0.449575 Train acc: 0.780000\n",
      "Epoch: 139/200 Iteration: 41900 Train loss: 0.495079 Train acc: 0.730000\n",
      "Epoch: 139/200 Iteration: 41950 Train loss: 0.369369 Train acc: 0.845000\n",
      "Epoch: 139/200 Iteration: 42000 Train loss: 0.394051 Train acc: 0.815000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139/200 Iteration: 42000 Validation loss: 0.555301 Validation acc: 0.703800\n",
      "Epoch: 140/200 Iteration: 42050 Train loss: 0.562884 Train acc: 0.715000\n",
      "Epoch: 140/200 Iteration: 42100 Train loss: 0.560253 Train acc: 0.775000\n",
      "Epoch: 140/200 Iteration: 42150 Train loss: 0.455303 Train acc: 0.775000\n",
      "Epoch: 140/200 Iteration: 42200 Train loss: 0.479383 Train acc: 0.755000\n",
      "Epoch: 140/200 Iteration: 42250 Train loss: 0.371464 Train acc: 0.855000\n",
      "Epoch: 140/200 Iteration: 42250 Validation loss: 0.553370 Validation acc: 0.705300\n",
      "Epoch: 140/200 Iteration: 42300 Train loss: 0.380096 Train acc: 0.820000\n",
      "Epoch: 141/200 Iteration: 42350 Train loss: 0.554086 Train acc: 0.720000\n",
      "Epoch: 141/200 Iteration: 42400 Train loss: 0.570890 Train acc: 0.760000\n",
      "Epoch: 141/200 Iteration: 42450 Train loss: 0.455843 Train acc: 0.790000\n",
      "Epoch: 141/200 Iteration: 42500 Train loss: 0.497767 Train acc: 0.730000\n",
      "Epoch: 141/200 Iteration: 42500 Validation loss: 0.557237 Validation acc: 0.702800\n",
      "Epoch: 141/200 Iteration: 42550 Train loss: 0.384777 Train acc: 0.825000\n",
      "Epoch: 141/200 Iteration: 42600 Train loss: 0.378731 Train acc: 0.800000\n",
      "Epoch: 142/200 Iteration: 42650 Train loss: 0.548360 Train acc: 0.715000\n",
      "Epoch: 142/200 Iteration: 42700 Train loss: 0.561682 Train acc: 0.775000\n",
      "Epoch: 142/200 Iteration: 42750 Train loss: 0.466844 Train acc: 0.770000\n",
      "Epoch: 142/200 Iteration: 42750 Validation loss: 0.558358 Validation acc: 0.702100\n",
      "Epoch: 142/200 Iteration: 42800 Train loss: 0.498422 Train acc: 0.755000\n",
      "Epoch: 142/200 Iteration: 42850 Train loss: 0.373316 Train acc: 0.820000\n",
      "Epoch: 142/200 Iteration: 42900 Train loss: 0.368437 Train acc: 0.820000\n",
      "Epoch: 143/200 Iteration: 42950 Train loss: 0.543032 Train acc: 0.720000\n",
      "Epoch: 143/200 Iteration: 43000 Train loss: 0.554924 Train acc: 0.750000\n",
      "Epoch: 143/200 Iteration: 43000 Validation loss: 0.558123 Validation acc: 0.703350\n",
      "Epoch: 143/200 Iteration: 43050 Train loss: 0.456046 Train acc: 0.780000\n",
      "Epoch: 143/200 Iteration: 43100 Train loss: 0.501032 Train acc: 0.750000\n",
      "Epoch: 143/200 Iteration: 43150 Train loss: 0.356602 Train acc: 0.885000\n",
      "Epoch: 143/200 Iteration: 43200 Train loss: 0.385334 Train acc: 0.825000\n",
      "Epoch: 144/200 Iteration: 43250 Train loss: 0.568455 Train acc: 0.715000\n",
      "Epoch: 144/200 Iteration: 43250 Validation loss: 0.557352 Validation acc: 0.702100\n",
      "Epoch: 144/200 Iteration: 43300 Train loss: 0.538805 Train acc: 0.790000\n",
      "Epoch: 144/200 Iteration: 43350 Train loss: 0.452322 Train acc: 0.785000\n",
      "Epoch: 144/200 Iteration: 43400 Train loss: 0.489400 Train acc: 0.745000\n",
      "Epoch: 144/200 Iteration: 43450 Train loss: 0.366093 Train acc: 0.825000\n",
      "Epoch: 144/200 Iteration: 43500 Train loss: 0.398402 Train acc: 0.805000\n",
      "Epoch: 144/200 Iteration: 43500 Validation loss: 0.557427 Validation acc: 0.702700\n",
      "Epoch: 145/200 Iteration: 43550 Train loss: 0.535402 Train acc: 0.735000\n",
      "Epoch: 145/200 Iteration: 43600 Train loss: 0.545943 Train acc: 0.760000\n",
      "Epoch: 145/200 Iteration: 43650 Train loss: 0.442762 Train acc: 0.800000\n",
      "Epoch: 145/200 Iteration: 43700 Train loss: 0.495265 Train acc: 0.745000\n",
      "Epoch: 145/200 Iteration: 43750 Train loss: 0.369264 Train acc: 0.855000\n",
      "Epoch: 145/200 Iteration: 43750 Validation loss: 0.554938 Validation acc: 0.705100\n",
      "Epoch: 145/200 Iteration: 43800 Train loss: 0.383340 Train acc: 0.820000\n",
      "Epoch: 146/200 Iteration: 43850 Train loss: 0.555767 Train acc: 0.735000\n",
      "Epoch: 146/200 Iteration: 43900 Train loss: 0.556746 Train acc: 0.785000\n",
      "Epoch: 146/200 Iteration: 43950 Train loss: 0.455839 Train acc: 0.765000\n",
      "Epoch: 146/200 Iteration: 44000 Train loss: 0.491107 Train acc: 0.750000\n",
      "Epoch: 146/200 Iteration: 44000 Validation loss: 0.560501 Validation acc: 0.702200\n",
      "Epoch: 146/200 Iteration: 44050 Train loss: 0.357605 Train acc: 0.870000\n",
      "Epoch: 146/200 Iteration: 44100 Train loss: 0.381090 Train acc: 0.805000\n",
      "Epoch: 147/200 Iteration: 44150 Train loss: 0.537941 Train acc: 0.705000\n",
      "Epoch: 147/200 Iteration: 44200 Train loss: 0.534469 Train acc: 0.790000\n",
      "Epoch: 147/200 Iteration: 44250 Train loss: 0.460501 Train acc: 0.795000\n",
      "Epoch: 147/200 Iteration: 44250 Validation loss: 0.562703 Validation acc: 0.702800\n",
      "Epoch: 147/200 Iteration: 44300 Train loss: 0.499932 Train acc: 0.750000\n",
      "Epoch: 147/200 Iteration: 44350 Train loss: 0.358231 Train acc: 0.845000\n",
      "Epoch: 147/200 Iteration: 44400 Train loss: 0.403092 Train acc: 0.810000\n",
      "Epoch: 148/200 Iteration: 44450 Train loss: 0.548417 Train acc: 0.695000\n",
      "Epoch: 148/200 Iteration: 44500 Train loss: 0.563847 Train acc: 0.780000\n",
      "Epoch: 148/200 Iteration: 44500 Validation loss: 0.560878 Validation acc: 0.702650\n",
      "Epoch: 148/200 Iteration: 44550 Train loss: 0.455992 Train acc: 0.775000\n",
      "Epoch: 148/200 Iteration: 44600 Train loss: 0.481283 Train acc: 0.745000\n",
      "Epoch: 148/200 Iteration: 44650 Train loss: 0.366335 Train acc: 0.845000\n",
      "Epoch: 148/200 Iteration: 44700 Train loss: 0.371766 Train acc: 0.820000\n",
      "Epoch: 149/200 Iteration: 44750 Train loss: 0.547135 Train acc: 0.710000\n",
      "Epoch: 149/200 Iteration: 44750 Validation loss: 0.560794 Validation acc: 0.702700\n",
      "Epoch: 149/200 Iteration: 44800 Train loss: 0.544393 Train acc: 0.755000\n",
      "Epoch: 149/200 Iteration: 44850 Train loss: 0.449777 Train acc: 0.780000\n",
      "Epoch: 149/200 Iteration: 44900 Train loss: 0.505013 Train acc: 0.755000\n",
      "Epoch: 149/200 Iteration: 44950 Train loss: 0.362492 Train acc: 0.860000\n",
      "Epoch: 149/200 Iteration: 45000 Train loss: 0.359574 Train acc: 0.825000\n",
      "Epoch: 149/200 Iteration: 45000 Validation loss: 0.560092 Validation acc: 0.702300\n",
      "Epoch: 150/200 Iteration: 45050 Train loss: 0.536007 Train acc: 0.710000\n",
      "Epoch: 150/200 Iteration: 45100 Train loss: 0.561800 Train acc: 0.740000\n",
      "Epoch: 150/200 Iteration: 45150 Train loss: 0.461095 Train acc: 0.785000\n",
      "Epoch: 150/200 Iteration: 45200 Train loss: 0.515461 Train acc: 0.745000\n",
      "Epoch: 150/200 Iteration: 45250 Train loss: 0.391076 Train acc: 0.850000\n",
      "Epoch: 150/200 Iteration: 45250 Validation loss: 0.554381 Validation acc: 0.706500\n",
      "Epoch: 150/200 Iteration: 45300 Train loss: 0.374797 Train acc: 0.815000\n",
      "Epoch: 151/200 Iteration: 45350 Train loss: 0.547257 Train acc: 0.705000\n",
      "Epoch: 151/200 Iteration: 45400 Train loss: 0.550257 Train acc: 0.775000\n",
      "Epoch: 151/200 Iteration: 45450 Train loss: 0.430766 Train acc: 0.775000\n",
      "Epoch: 151/200 Iteration: 45500 Train loss: 0.489648 Train acc: 0.745000\n",
      "Epoch: 151/200 Iteration: 45500 Validation loss: 0.560587 Validation acc: 0.701800\n",
      "Epoch: 151/200 Iteration: 45550 Train loss: 0.368981 Train acc: 0.850000\n",
      "Epoch: 151/200 Iteration: 45600 Train loss: 0.372478 Train acc: 0.800000\n",
      "Epoch: 152/200 Iteration: 45650 Train loss: 0.540766 Train acc: 0.705000\n",
      "Epoch: 152/200 Iteration: 45700 Train loss: 0.541652 Train acc: 0.770000\n",
      "Epoch: 152/200 Iteration: 45750 Train loss: 0.443903 Train acc: 0.785000\n",
      "Epoch: 152/200 Iteration: 45750 Validation loss: 0.561059 Validation acc: 0.703400\n",
      "Epoch: 152/200 Iteration: 45800 Train loss: 0.501007 Train acc: 0.735000\n",
      "Epoch: 152/200 Iteration: 45850 Train loss: 0.361960 Train acc: 0.845000\n",
      "Epoch: 152/200 Iteration: 45900 Train loss: 0.394227 Train acc: 0.810000\n",
      "Epoch: 153/200 Iteration: 45950 Train loss: 0.539091 Train acc: 0.715000\n",
      "Epoch: 153/200 Iteration: 46000 Train loss: 0.554388 Train acc: 0.780000\n",
      "Epoch: 153/200 Iteration: 46000 Validation loss: 0.560867 Validation acc: 0.703750\n",
      "Epoch: 153/200 Iteration: 46050 Train loss: 0.468703 Train acc: 0.775000\n",
      "Epoch: 153/200 Iteration: 46100 Train loss: 0.527702 Train acc: 0.750000\n",
      "Epoch: 153/200 Iteration: 46150 Train loss: 0.370639 Train acc: 0.835000\n",
      "Epoch: 153/200 Iteration: 46200 Train loss: 0.388906 Train acc: 0.825000\n",
      "Epoch: 154/200 Iteration: 46250 Train loss: 0.533023 Train acc: 0.720000\n",
      "Epoch: 154/200 Iteration: 46250 Validation loss: 0.560680 Validation acc: 0.703450\n",
      "Epoch: 154/200 Iteration: 46300 Train loss: 0.533803 Train acc: 0.800000\n",
      "Epoch: 154/200 Iteration: 46350 Train loss: 0.454107 Train acc: 0.775000\n",
      "Epoch: 154/200 Iteration: 46400 Train loss: 0.494650 Train acc: 0.750000\n",
      "Epoch: 154/200 Iteration: 46450 Train loss: 0.370041 Train acc: 0.855000\n",
      "Epoch: 154/200 Iteration: 46500 Train loss: 0.386801 Train acc: 0.805000\n",
      "Epoch: 154/200 Iteration: 46500 Validation loss: 0.563121 Validation acc: 0.702050\n",
      "Epoch: 155/200 Iteration: 46550 Train loss: 0.561652 Train acc: 0.710000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/200 Iteration: 46600 Train loss: 0.543125 Train acc: 0.775000\n",
      "Epoch: 155/200 Iteration: 46650 Train loss: 0.459847 Train acc: 0.770000\n",
      "Epoch: 155/200 Iteration: 46700 Train loss: 0.493289 Train acc: 0.730000\n",
      "Epoch: 155/200 Iteration: 46750 Train loss: 0.353279 Train acc: 0.875000\n",
      "Epoch: 155/200 Iteration: 46750 Validation loss: 0.558530 Validation acc: 0.705800\n",
      "Epoch: 155/200 Iteration: 46800 Train loss: 0.403666 Train acc: 0.805000\n",
      "Epoch: 156/200 Iteration: 46850 Train loss: 0.526523 Train acc: 0.715000\n",
      "Epoch: 156/200 Iteration: 46900 Train loss: 0.560057 Train acc: 0.740000\n",
      "Epoch: 156/200 Iteration: 46950 Train loss: 0.462077 Train acc: 0.755000\n",
      "Epoch: 156/200 Iteration: 47000 Train loss: 0.508528 Train acc: 0.725000\n",
      "Epoch: 156/200 Iteration: 47000 Validation loss: 0.564456 Validation acc: 0.701800\n",
      "Epoch: 156/200 Iteration: 47050 Train loss: 0.368583 Train acc: 0.850000\n",
      "Epoch: 156/200 Iteration: 47100 Train loss: 0.356757 Train acc: 0.835000\n",
      "Epoch: 157/200 Iteration: 47150 Train loss: 0.534365 Train acc: 0.725000\n",
      "Epoch: 157/200 Iteration: 47200 Train loss: 0.536168 Train acc: 0.775000\n",
      "Epoch: 157/200 Iteration: 47250 Train loss: 0.451959 Train acc: 0.800000\n",
      "Epoch: 157/200 Iteration: 47250 Validation loss: 0.569098 Validation acc: 0.702850\n",
      "Epoch: 157/200 Iteration: 47300 Train loss: 0.493435 Train acc: 0.750000\n",
      "Epoch: 157/200 Iteration: 47350 Train loss: 0.372028 Train acc: 0.835000\n",
      "Epoch: 157/200 Iteration: 47400 Train loss: 0.383109 Train acc: 0.800000\n",
      "Epoch: 158/200 Iteration: 47450 Train loss: 0.552676 Train acc: 0.705000\n",
      "Epoch: 158/200 Iteration: 47500 Train loss: 0.540715 Train acc: 0.770000\n",
      "Epoch: 158/200 Iteration: 47500 Validation loss: 0.564794 Validation acc: 0.703450\n",
      "Epoch: 158/200 Iteration: 47550 Train loss: 0.442443 Train acc: 0.770000\n",
      "Epoch: 158/200 Iteration: 47600 Train loss: 0.490892 Train acc: 0.760000\n",
      "Epoch: 158/200 Iteration: 47650 Train loss: 0.363156 Train acc: 0.835000\n",
      "Epoch: 158/200 Iteration: 47700 Train loss: 0.379223 Train acc: 0.820000\n",
      "Epoch: 159/200 Iteration: 47750 Train loss: 0.545310 Train acc: 0.715000\n",
      "Epoch: 159/200 Iteration: 47750 Validation loss: 0.567688 Validation acc: 0.705750\n",
      "Epoch: 159/200 Iteration: 47800 Train loss: 0.553124 Train acc: 0.780000\n",
      "Epoch: 159/200 Iteration: 47850 Train loss: 0.455758 Train acc: 0.760000\n",
      "Epoch: 159/200 Iteration: 47900 Train loss: 0.487587 Train acc: 0.740000\n",
      "Epoch: 159/200 Iteration: 47950 Train loss: 0.368600 Train acc: 0.850000\n",
      "Epoch: 159/200 Iteration: 48000 Train loss: 0.383450 Train acc: 0.825000\n",
      "Epoch: 159/200 Iteration: 48000 Validation loss: 0.568034 Validation acc: 0.704450\n",
      "Epoch: 160/200 Iteration: 48050 Train loss: 0.544548 Train acc: 0.705000\n",
      "Epoch: 160/200 Iteration: 48100 Train loss: 0.531280 Train acc: 0.785000\n",
      "Epoch: 160/200 Iteration: 48150 Train loss: 0.437131 Train acc: 0.795000\n",
      "Epoch: 160/200 Iteration: 48200 Train loss: 0.481442 Train acc: 0.770000\n",
      "Epoch: 160/200 Iteration: 48250 Train loss: 0.360670 Train acc: 0.830000\n",
      "Epoch: 160/200 Iteration: 48250 Validation loss: 0.560886 Validation acc: 0.703200\n",
      "Epoch: 160/200 Iteration: 48300 Train loss: 0.364335 Train acc: 0.835000\n",
      "Epoch: 161/200 Iteration: 48350 Train loss: 0.534046 Train acc: 0.705000\n",
      "Epoch: 161/200 Iteration: 48400 Train loss: 0.550977 Train acc: 0.760000\n",
      "Epoch: 161/200 Iteration: 48450 Train loss: 0.466889 Train acc: 0.760000\n",
      "Epoch: 161/200 Iteration: 48500 Train loss: 0.498681 Train acc: 0.735000\n",
      "Epoch: 161/200 Iteration: 48500 Validation loss: 0.566611 Validation acc: 0.703400\n",
      "Epoch: 161/200 Iteration: 48550 Train loss: 0.362672 Train acc: 0.840000\n",
      "Epoch: 161/200 Iteration: 48600 Train loss: 0.395196 Train acc: 0.800000\n",
      "Epoch: 162/200 Iteration: 48650 Train loss: 0.549784 Train acc: 0.710000\n",
      "Epoch: 162/200 Iteration: 48700 Train loss: 0.530854 Train acc: 0.780000\n",
      "Epoch: 162/200 Iteration: 48750 Train loss: 0.427239 Train acc: 0.790000\n",
      "Epoch: 162/200 Iteration: 48750 Validation loss: 0.569540 Validation acc: 0.703300\n",
      "Epoch: 162/200 Iteration: 48800 Train loss: 0.498806 Train acc: 0.765000\n",
      "Epoch: 162/200 Iteration: 48850 Train loss: 0.356364 Train acc: 0.855000\n",
      "Epoch: 162/200 Iteration: 48900 Train loss: 0.382228 Train acc: 0.805000\n",
      "Epoch: 163/200 Iteration: 48950 Train loss: 0.544273 Train acc: 0.740000\n",
      "Epoch: 163/200 Iteration: 49000 Train loss: 0.557503 Train acc: 0.795000\n",
      "Epoch: 163/200 Iteration: 49000 Validation loss: 0.567468 Validation acc: 0.704000\n",
      "Epoch: 163/200 Iteration: 49050 Train loss: 0.448146 Train acc: 0.765000\n",
      "Epoch: 163/200 Iteration: 49100 Train loss: 0.480798 Train acc: 0.780000\n",
      "Epoch: 163/200 Iteration: 49150 Train loss: 0.365905 Train acc: 0.845000\n",
      "Epoch: 163/200 Iteration: 49200 Train loss: 0.384579 Train acc: 0.810000\n",
      "Epoch: 164/200 Iteration: 49250 Train loss: 0.550657 Train acc: 0.705000\n",
      "Epoch: 164/200 Iteration: 49250 Validation loss: 0.570516 Validation acc: 0.705600\n",
      "Epoch: 164/200 Iteration: 49300 Train loss: 0.525449 Train acc: 0.775000\n",
      "Epoch: 164/200 Iteration: 49350 Train loss: 0.463353 Train acc: 0.765000\n",
      "Epoch: 164/200 Iteration: 49400 Train loss: 0.484154 Train acc: 0.750000\n",
      "Epoch: 164/200 Iteration: 49450 Train loss: 0.372644 Train acc: 0.850000\n",
      "Epoch: 164/200 Iteration: 49500 Train loss: 0.388461 Train acc: 0.830000\n",
      "Epoch: 164/200 Iteration: 49500 Validation loss: 0.571710 Validation acc: 0.703550\n",
      "Epoch: 165/200 Iteration: 49550 Train loss: 0.536906 Train acc: 0.730000\n",
      "Epoch: 165/200 Iteration: 49600 Train loss: 0.548548 Train acc: 0.765000\n",
      "Epoch: 165/200 Iteration: 49650 Train loss: 0.441052 Train acc: 0.770000\n",
      "Epoch: 165/200 Iteration: 49700 Train loss: 0.471644 Train acc: 0.745000\n",
      "Epoch: 165/200 Iteration: 49750 Train loss: 0.374052 Train acc: 0.845000\n",
      "Epoch: 165/200 Iteration: 49750 Validation loss: 0.566653 Validation acc: 0.700300\n",
      "Epoch: 165/200 Iteration: 49800 Train loss: 0.361953 Train acc: 0.820000\n",
      "Epoch: 166/200 Iteration: 49850 Train loss: 0.538705 Train acc: 0.730000\n",
      "Epoch: 166/200 Iteration: 49900 Train loss: 0.526522 Train acc: 0.780000\n",
      "Epoch: 166/200 Iteration: 49950 Train loss: 0.433396 Train acc: 0.795000\n",
      "Epoch: 166/200 Iteration: 50000 Train loss: 0.500906 Train acc: 0.735000\n",
      "Epoch: 166/200 Iteration: 50000 Validation loss: 0.572986 Validation acc: 0.700450\n",
      "Epoch: 166/200 Iteration: 50050 Train loss: 0.370359 Train acc: 0.825000\n",
      "Epoch: 166/200 Iteration: 50100 Train loss: 0.381162 Train acc: 0.815000\n",
      "Epoch: 167/200 Iteration: 50150 Train loss: 0.549287 Train acc: 0.670000\n",
      "Epoch: 167/200 Iteration: 50200 Train loss: 0.541022 Train acc: 0.785000\n",
      "Epoch: 167/200 Iteration: 50250 Train loss: 0.447519 Train acc: 0.790000\n",
      "Epoch: 167/200 Iteration: 50250 Validation loss: 0.571923 Validation acc: 0.702950\n",
      "Epoch: 167/200 Iteration: 50300 Train loss: 0.489416 Train acc: 0.745000\n",
      "Epoch: 167/200 Iteration: 50350 Train loss: 0.354991 Train acc: 0.845000\n",
      "Epoch: 167/200 Iteration: 50400 Train loss: 0.367325 Train acc: 0.815000\n",
      "Epoch: 168/200 Iteration: 50450 Train loss: 0.544261 Train acc: 0.710000\n",
      "Epoch: 168/200 Iteration: 50500 Train loss: 0.566922 Train acc: 0.770000\n",
      "Epoch: 168/200 Iteration: 50500 Validation loss: 0.571654 Validation acc: 0.705100\n",
      "Epoch: 168/200 Iteration: 50550 Train loss: 0.464909 Train acc: 0.795000\n",
      "Epoch: 168/200 Iteration: 50600 Train loss: 0.515336 Train acc: 0.760000\n",
      "Epoch: 168/200 Iteration: 50650 Train loss: 0.368887 Train acc: 0.845000\n",
      "Epoch: 168/200 Iteration: 50700 Train loss: 0.408371 Train acc: 0.785000\n",
      "Epoch: 169/200 Iteration: 50750 Train loss: 0.532541 Train acc: 0.720000\n",
      "Epoch: 169/200 Iteration: 50750 Validation loss: 0.571741 Validation acc: 0.703900\n",
      "Epoch: 169/200 Iteration: 50800 Train loss: 0.574366 Train acc: 0.740000\n",
      "Epoch: 169/200 Iteration: 50850 Train loss: 0.444240 Train acc: 0.765000\n",
      "Epoch: 169/200 Iteration: 50900 Train loss: 0.467606 Train acc: 0.755000\n",
      "Epoch: 169/200 Iteration: 50950 Train loss: 0.368958 Train acc: 0.830000\n",
      "Epoch: 169/200 Iteration: 51000 Train loss: 0.397683 Train acc: 0.815000\n",
      "Epoch: 169/200 Iteration: 51000 Validation loss: 0.570618 Validation acc: 0.702500\n",
      "Epoch: 170/200 Iteration: 51050 Train loss: 0.538638 Train acc: 0.715000\n",
      "Epoch: 170/200 Iteration: 51100 Train loss: 0.541638 Train acc: 0.780000\n",
      "Epoch: 170/200 Iteration: 51150 Train loss: 0.430690 Train acc: 0.785000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170/200 Iteration: 51200 Train loss: 0.496258 Train acc: 0.760000\n",
      "Epoch: 170/200 Iteration: 51250 Train loss: 0.348924 Train acc: 0.850000\n",
      "Epoch: 170/200 Iteration: 51250 Validation loss: 0.566644 Validation acc: 0.700950\n",
      "Epoch: 170/200 Iteration: 51300 Train loss: 0.379298 Train acc: 0.815000\n",
      "Epoch: 171/200 Iteration: 51350 Train loss: 0.540625 Train acc: 0.730000\n",
      "Epoch: 171/200 Iteration: 51400 Train loss: 0.559363 Train acc: 0.775000\n",
      "Epoch: 171/200 Iteration: 51450 Train loss: 0.455327 Train acc: 0.790000\n",
      "Epoch: 171/200 Iteration: 51500 Train loss: 0.494182 Train acc: 0.730000\n",
      "Epoch: 171/200 Iteration: 51500 Validation loss: 0.572057 Validation acc: 0.702350\n",
      "Epoch: 171/200 Iteration: 51550 Train loss: 0.381511 Train acc: 0.850000\n",
      "Epoch: 171/200 Iteration: 51600 Train loss: 0.374572 Train acc: 0.815000\n",
      "Epoch: 172/200 Iteration: 51650 Train loss: 0.527854 Train acc: 0.715000\n",
      "Epoch: 172/200 Iteration: 51700 Train loss: 0.530756 Train acc: 0.800000\n",
      "Epoch: 172/200 Iteration: 51750 Train loss: 0.459708 Train acc: 0.775000\n",
      "Epoch: 172/200 Iteration: 51750 Validation loss: 0.574526 Validation acc: 0.703050\n",
      "Epoch: 172/200 Iteration: 51800 Train loss: 0.478903 Train acc: 0.740000\n",
      "Epoch: 172/200 Iteration: 51850 Train loss: 0.373727 Train acc: 0.845000\n",
      "Epoch: 172/200 Iteration: 51900 Train loss: 0.386445 Train acc: 0.805000\n",
      "Epoch: 173/200 Iteration: 51950 Train loss: 0.540187 Train acc: 0.715000\n",
      "Epoch: 173/200 Iteration: 52000 Train loss: 0.546017 Train acc: 0.755000\n",
      "Epoch: 173/200 Iteration: 52000 Validation loss: 0.571979 Validation acc: 0.706250\n",
      "Epoch: 173/200 Iteration: 52050 Train loss: 0.449099 Train acc: 0.795000\n",
      "Epoch: 173/200 Iteration: 52100 Train loss: 0.479916 Train acc: 0.760000\n",
      "Epoch: 173/200 Iteration: 52150 Train loss: 0.368815 Train acc: 0.840000\n",
      "Epoch: 173/200 Iteration: 52200 Train loss: 0.391912 Train acc: 0.795000\n",
      "Epoch: 174/200 Iteration: 52250 Train loss: 0.537373 Train acc: 0.710000\n",
      "Epoch: 174/200 Iteration: 52250 Validation loss: 0.576309 Validation acc: 0.703900\n",
      "Epoch: 174/200 Iteration: 52300 Train loss: 0.518666 Train acc: 0.790000\n",
      "Epoch: 174/200 Iteration: 52350 Train loss: 0.455324 Train acc: 0.785000\n",
      "Epoch: 174/200 Iteration: 52400 Train loss: 0.481664 Train acc: 0.765000\n",
      "Epoch: 174/200 Iteration: 52450 Train loss: 0.349965 Train acc: 0.850000\n",
      "Epoch: 174/200 Iteration: 52500 Train loss: 0.376233 Train acc: 0.800000\n",
      "Epoch: 174/200 Iteration: 52500 Validation loss: 0.569741 Validation acc: 0.703150\n",
      "Epoch: 175/200 Iteration: 52550 Train loss: 0.546866 Train acc: 0.735000\n",
      "Epoch: 175/200 Iteration: 52600 Train loss: 0.560037 Train acc: 0.765000\n",
      "Epoch: 175/200 Iteration: 52650 Train loss: 0.466982 Train acc: 0.770000\n",
      "Epoch: 175/200 Iteration: 52700 Train loss: 0.474709 Train acc: 0.770000\n",
      "Epoch: 175/200 Iteration: 52750 Train loss: 0.340401 Train acc: 0.855000\n",
      "Epoch: 175/200 Iteration: 52750 Validation loss: 0.567543 Validation acc: 0.701250\n",
      "Epoch: 175/200 Iteration: 52800 Train loss: 0.354613 Train acc: 0.815000\n",
      "Epoch: 176/200 Iteration: 52850 Train loss: 0.531413 Train acc: 0.730000\n",
      "Epoch: 176/200 Iteration: 52900 Train loss: 0.529706 Train acc: 0.770000\n",
      "Epoch: 176/200 Iteration: 52950 Train loss: 0.444303 Train acc: 0.770000\n",
      "Epoch: 176/200 Iteration: 53000 Train loss: 0.472778 Train acc: 0.755000\n",
      "Epoch: 176/200 Iteration: 53000 Validation loss: 0.571496 Validation acc: 0.701500\n",
      "Epoch: 176/200 Iteration: 53050 Train loss: 0.359409 Train acc: 0.845000\n",
      "Epoch: 176/200 Iteration: 53100 Train loss: 0.361350 Train acc: 0.805000\n",
      "Epoch: 177/200 Iteration: 53150 Train loss: 0.546251 Train acc: 0.670000\n",
      "Epoch: 177/200 Iteration: 53200 Train loss: 0.526284 Train acc: 0.775000\n",
      "Epoch: 177/200 Iteration: 53250 Train loss: 0.466282 Train acc: 0.775000\n",
      "Epoch: 177/200 Iteration: 53250 Validation loss: 0.575759 Validation acc: 0.701600\n",
      "Epoch: 177/200 Iteration: 53300 Train loss: 0.501356 Train acc: 0.730000\n",
      "Epoch: 177/200 Iteration: 53350 Train loss: 0.349664 Train acc: 0.845000\n",
      "Epoch: 177/200 Iteration: 53400 Train loss: 0.372241 Train acc: 0.835000\n",
      "Epoch: 178/200 Iteration: 53450 Train loss: 0.525719 Train acc: 0.725000\n",
      "Epoch: 178/200 Iteration: 53500 Train loss: 0.529620 Train acc: 0.785000\n",
      "Epoch: 178/200 Iteration: 53500 Validation loss: 0.570680 Validation acc: 0.704850\n",
      "Epoch: 178/200 Iteration: 53550 Train loss: 0.458462 Train acc: 0.795000\n",
      "Epoch: 178/200 Iteration: 53600 Train loss: 0.491881 Train acc: 0.760000\n",
      "Epoch: 178/200 Iteration: 53650 Train loss: 0.367641 Train acc: 0.855000\n",
      "Epoch: 178/200 Iteration: 53700 Train loss: 0.359139 Train acc: 0.830000\n",
      "Epoch: 179/200 Iteration: 53750 Train loss: 0.527113 Train acc: 0.745000\n",
      "Epoch: 179/200 Iteration: 53750 Validation loss: 0.576821 Validation acc: 0.702800\n",
      "Epoch: 179/200 Iteration: 53800 Train loss: 0.552677 Train acc: 0.790000\n",
      "Epoch: 179/200 Iteration: 53850 Train loss: 0.457787 Train acc: 0.775000\n",
      "Epoch: 179/200 Iteration: 53900 Train loss: 0.480207 Train acc: 0.750000\n",
      "Epoch: 179/200 Iteration: 53950 Train loss: 0.359588 Train acc: 0.830000\n",
      "Epoch: 179/200 Iteration: 54000 Train loss: 0.395716 Train acc: 0.800000\n",
      "Epoch: 179/200 Iteration: 54000 Validation loss: 0.578101 Validation acc: 0.703700\n",
      "Epoch: 180/200 Iteration: 54050 Train loss: 0.552459 Train acc: 0.695000\n",
      "Epoch: 180/200 Iteration: 54100 Train loss: 0.526346 Train acc: 0.790000\n",
      "Epoch: 180/200 Iteration: 54150 Train loss: 0.457081 Train acc: 0.770000\n",
      "Epoch: 180/200 Iteration: 54200 Train loss: 0.491592 Train acc: 0.770000\n",
      "Epoch: 180/200 Iteration: 54250 Train loss: 0.396319 Train acc: 0.810000\n",
      "Epoch: 180/200 Iteration: 54250 Validation loss: 0.571729 Validation acc: 0.699600\n",
      "Epoch: 180/200 Iteration: 54300 Train loss: 0.372167 Train acc: 0.840000\n",
      "Epoch: 181/200 Iteration: 54350 Train loss: 0.514903 Train acc: 0.730000\n",
      "Epoch: 181/200 Iteration: 54400 Train loss: 0.542204 Train acc: 0.765000\n",
      "Epoch: 181/200 Iteration: 54450 Train loss: 0.440778 Train acc: 0.795000\n",
      "Epoch: 181/200 Iteration: 54500 Train loss: 0.490651 Train acc: 0.745000\n",
      "Epoch: 181/200 Iteration: 54500 Validation loss: 0.576267 Validation acc: 0.701450\n",
      "Epoch: 181/200 Iteration: 54550 Train loss: 0.362064 Train acc: 0.855000\n",
      "Epoch: 181/200 Iteration: 54600 Train loss: 0.392831 Train acc: 0.815000\n",
      "Epoch: 182/200 Iteration: 54650 Train loss: 0.525230 Train acc: 0.705000\n",
      "Epoch: 182/200 Iteration: 54700 Train loss: 0.540572 Train acc: 0.755000\n",
      "Epoch: 182/200 Iteration: 54750 Train loss: 0.457954 Train acc: 0.790000\n",
      "Epoch: 182/200 Iteration: 54750 Validation loss: 0.581737 Validation acc: 0.703100\n",
      "Epoch: 182/200 Iteration: 54800 Train loss: 0.483998 Train acc: 0.750000\n",
      "Epoch: 182/200 Iteration: 54850 Train loss: 0.372436 Train acc: 0.870000\n",
      "Epoch: 182/200 Iteration: 54900 Train loss: 0.402119 Train acc: 0.805000\n",
      "Epoch: 183/200 Iteration: 54950 Train loss: 0.535985 Train acc: 0.695000\n",
      "Epoch: 183/200 Iteration: 55000 Train loss: 0.539632 Train acc: 0.775000\n",
      "Epoch: 183/200 Iteration: 55000 Validation loss: 0.576925 Validation acc: 0.704650\n",
      "Epoch: 183/200 Iteration: 55050 Train loss: 0.459880 Train acc: 0.795000\n",
      "Epoch: 183/200 Iteration: 55100 Train loss: 0.503325 Train acc: 0.745000\n",
      "Epoch: 183/200 Iteration: 55150 Train loss: 0.376529 Train acc: 0.850000\n",
      "Epoch: 183/200 Iteration: 55200 Train loss: 0.391978 Train acc: 0.825000\n",
      "Epoch: 184/200 Iteration: 55250 Train loss: 0.534346 Train acc: 0.720000\n",
      "Epoch: 184/200 Iteration: 55250 Validation loss: 0.577280 Validation acc: 0.703800\n",
      "Epoch: 184/200 Iteration: 55300 Train loss: 0.551033 Train acc: 0.780000\n",
      "Epoch: 184/200 Iteration: 55350 Train loss: 0.451588 Train acc: 0.785000\n",
      "Epoch: 184/200 Iteration: 55400 Train loss: 0.505895 Train acc: 0.730000\n",
      "Epoch: 184/200 Iteration: 55450 Train loss: 0.356071 Train acc: 0.835000\n",
      "Epoch: 184/200 Iteration: 55500 Train loss: 0.391774 Train acc: 0.805000\n",
      "Epoch: 184/200 Iteration: 55500 Validation loss: 0.580720 Validation acc: 0.703300\n",
      "Epoch: 185/200 Iteration: 55550 Train loss: 0.505289 Train acc: 0.715000\n",
      "Epoch: 185/200 Iteration: 55600 Train loss: 0.539071 Train acc: 0.785000\n",
      "Epoch: 185/200 Iteration: 55650 Train loss: 0.438768 Train acc: 0.785000\n",
      "Epoch: 185/200 Iteration: 55700 Train loss: 0.502961 Train acc: 0.755000\n",
      "Epoch: 185/200 Iteration: 55750 Train loss: 0.353304 Train acc: 0.850000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185/200 Iteration: 55750 Validation loss: 0.574957 Validation acc: 0.700350\n",
      "Epoch: 185/200 Iteration: 55800 Train loss: 0.396770 Train acc: 0.830000\n",
      "Epoch: 186/200 Iteration: 55850 Train loss: 0.531122 Train acc: 0.695000\n",
      "Epoch: 186/200 Iteration: 55900 Train loss: 0.543337 Train acc: 0.775000\n",
      "Epoch: 186/200 Iteration: 55950 Train loss: 0.445113 Train acc: 0.765000\n",
      "Epoch: 186/200 Iteration: 56000 Train loss: 0.497590 Train acc: 0.765000\n",
      "Epoch: 186/200 Iteration: 56000 Validation loss: 0.579684 Validation acc: 0.701600\n",
      "Epoch: 186/200 Iteration: 56050 Train loss: 0.354763 Train acc: 0.865000\n",
      "Epoch: 186/200 Iteration: 56100 Train loss: 0.379071 Train acc: 0.830000\n",
      "Epoch: 187/200 Iteration: 56150 Train loss: 0.528377 Train acc: 0.730000\n",
      "Epoch: 187/200 Iteration: 56200 Train loss: 0.548228 Train acc: 0.790000\n",
      "Epoch: 187/200 Iteration: 56250 Train loss: 0.447041 Train acc: 0.785000\n",
      "Epoch: 187/200 Iteration: 56250 Validation loss: 0.585488 Validation acc: 0.701800\n",
      "Epoch: 187/200 Iteration: 56300 Train loss: 0.496510 Train acc: 0.735000\n",
      "Epoch: 187/200 Iteration: 56350 Train loss: 0.353077 Train acc: 0.860000\n",
      "Epoch: 187/200 Iteration: 56400 Train loss: 0.368383 Train acc: 0.815000\n",
      "Epoch: 188/200 Iteration: 56450 Train loss: 0.538965 Train acc: 0.710000\n",
      "Epoch: 188/200 Iteration: 56500 Train loss: 0.551186 Train acc: 0.775000\n",
      "Epoch: 188/200 Iteration: 56500 Validation loss: 0.580876 Validation acc: 0.702750\n",
      "Epoch: 188/200 Iteration: 56550 Train loss: 0.449921 Train acc: 0.795000\n",
      "Epoch: 188/200 Iteration: 56600 Train loss: 0.507809 Train acc: 0.755000\n",
      "Epoch: 188/200 Iteration: 56650 Train loss: 0.359727 Train acc: 0.835000\n",
      "Epoch: 188/200 Iteration: 56700 Train loss: 0.373945 Train acc: 0.800000\n",
      "Epoch: 189/200 Iteration: 56750 Train loss: 0.527971 Train acc: 0.745000\n",
      "Epoch: 189/200 Iteration: 56750 Validation loss: 0.583079 Validation acc: 0.703450\n",
      "Epoch: 189/200 Iteration: 56800 Train loss: 0.524670 Train acc: 0.795000\n",
      "Epoch: 189/200 Iteration: 56850 Train loss: 0.457613 Train acc: 0.790000\n",
      "Epoch: 189/200 Iteration: 56900 Train loss: 0.477298 Train acc: 0.770000\n",
      "Epoch: 189/200 Iteration: 56950 Train loss: 0.372596 Train acc: 0.825000\n",
      "Epoch: 189/200 Iteration: 57000 Train loss: 0.386677 Train acc: 0.825000\n",
      "Epoch: 189/200 Iteration: 57000 Validation loss: 0.582095 Validation acc: 0.703250\n",
      "Epoch: 190/200 Iteration: 57050 Train loss: 0.531451 Train acc: 0.735000\n",
      "Epoch: 190/200 Iteration: 57100 Train loss: 0.531600 Train acc: 0.755000\n",
      "Epoch: 190/200 Iteration: 57150 Train loss: 0.441033 Train acc: 0.770000\n",
      "Epoch: 190/200 Iteration: 57200 Train loss: 0.500865 Train acc: 0.750000\n",
      "Epoch: 190/200 Iteration: 57250 Train loss: 0.352374 Train acc: 0.865000\n",
      "Epoch: 190/200 Iteration: 57250 Validation loss: 0.577816 Validation acc: 0.703150\n",
      "Epoch: 190/200 Iteration: 57300 Train loss: 0.404750 Train acc: 0.795000\n",
      "Epoch: 191/200 Iteration: 57350 Train loss: 0.537055 Train acc: 0.715000\n",
      "Epoch: 191/200 Iteration: 57400 Train loss: 0.576864 Train acc: 0.755000\n",
      "Epoch: 191/200 Iteration: 57450 Train loss: 0.441813 Train acc: 0.780000\n",
      "Epoch: 191/200 Iteration: 57500 Train loss: 0.481465 Train acc: 0.770000\n",
      "Epoch: 191/200 Iteration: 57500 Validation loss: 0.581649 Validation acc: 0.700650\n",
      "Epoch: 191/200 Iteration: 57550 Train loss: 0.383146 Train acc: 0.820000\n",
      "Epoch: 191/200 Iteration: 57600 Train loss: 0.347358 Train acc: 0.835000\n",
      "Epoch: 192/200 Iteration: 57650 Train loss: 0.537913 Train acc: 0.715000\n",
      "Epoch: 192/200 Iteration: 57700 Train loss: 0.518973 Train acc: 0.775000\n",
      "Epoch: 192/200 Iteration: 57750 Train loss: 0.436818 Train acc: 0.760000\n",
      "Epoch: 192/200 Iteration: 57750 Validation loss: 0.585238 Validation acc: 0.701300\n",
      "Epoch: 192/200 Iteration: 57800 Train loss: 0.475957 Train acc: 0.775000\n",
      "Epoch: 192/200 Iteration: 57850 Train loss: 0.377756 Train acc: 0.820000\n",
      "Epoch: 192/200 Iteration: 57900 Train loss: 0.413885 Train acc: 0.820000\n",
      "Epoch: 193/200 Iteration: 57950 Train loss: 0.512325 Train acc: 0.710000\n",
      "Epoch: 193/200 Iteration: 58000 Train loss: 0.536474 Train acc: 0.790000\n",
      "Epoch: 193/200 Iteration: 58000 Validation loss: 0.583282 Validation acc: 0.701800\n",
      "Epoch: 193/200 Iteration: 58050 Train loss: 0.432870 Train acc: 0.780000\n",
      "Epoch: 193/200 Iteration: 58100 Train loss: 0.498166 Train acc: 0.755000\n",
      "Epoch: 193/200 Iteration: 58150 Train loss: 0.355159 Train acc: 0.845000\n",
      "Epoch: 193/200 Iteration: 58200 Train loss: 0.376732 Train acc: 0.820000\n",
      "Epoch: 194/200 Iteration: 58250 Train loss: 0.542158 Train acc: 0.710000\n",
      "Epoch: 194/200 Iteration: 58250 Validation loss: 0.581586 Validation acc: 0.702050\n",
      "Epoch: 194/200 Iteration: 58300 Train loss: 0.541746 Train acc: 0.770000\n",
      "Epoch: 194/200 Iteration: 58350 Train loss: 0.464126 Train acc: 0.785000\n",
      "Epoch: 194/200 Iteration: 58400 Train loss: 0.461325 Train acc: 0.765000\n",
      "Epoch: 194/200 Iteration: 58450 Train loss: 0.340248 Train acc: 0.875000\n",
      "Epoch: 194/200 Iteration: 58500 Train loss: 0.378732 Train acc: 0.830000\n",
      "Epoch: 194/200 Iteration: 58500 Validation loss: 0.580419 Validation acc: 0.700350\n",
      "Epoch: 195/200 Iteration: 58550 Train loss: 0.519621 Train acc: 0.715000\n",
      "Epoch: 195/200 Iteration: 58600 Train loss: 0.517955 Train acc: 0.765000\n",
      "Epoch: 195/200 Iteration: 58650 Train loss: 0.467661 Train acc: 0.775000\n",
      "Epoch: 195/200 Iteration: 58700 Train loss: 0.485492 Train acc: 0.780000\n",
      "Epoch: 195/200 Iteration: 58750 Train loss: 0.335116 Train acc: 0.860000\n",
      "Epoch: 195/200 Iteration: 58750 Validation loss: 0.578621 Validation acc: 0.701600\n",
      "Epoch: 195/200 Iteration: 58800 Train loss: 0.404546 Train acc: 0.810000\n",
      "Epoch: 196/200 Iteration: 58850 Train loss: 0.513583 Train acc: 0.730000\n",
      "Epoch: 196/200 Iteration: 58900 Train loss: 0.553383 Train acc: 0.765000\n",
      "Epoch: 196/200 Iteration: 58950 Train loss: 0.448568 Train acc: 0.775000\n",
      "Epoch: 196/200 Iteration: 59000 Train loss: 0.481145 Train acc: 0.745000\n",
      "Epoch: 196/200 Iteration: 59000 Validation loss: 0.581367 Validation acc: 0.698000\n",
      "Epoch: 196/200 Iteration: 59050 Train loss: 0.352408 Train acc: 0.855000\n",
      "Epoch: 196/200 Iteration: 59100 Train loss: 0.407515 Train acc: 0.820000\n",
      "Epoch: 197/200 Iteration: 59150 Train loss: 0.552835 Train acc: 0.705000\n",
      "Epoch: 197/200 Iteration: 59200 Train loss: 0.532875 Train acc: 0.775000\n",
      "Epoch: 197/200 Iteration: 59250 Train loss: 0.439299 Train acc: 0.785000\n",
      "Epoch: 197/200 Iteration: 59250 Validation loss: 0.584968 Validation acc: 0.700500\n",
      "Epoch: 197/200 Iteration: 59300 Train loss: 0.481307 Train acc: 0.770000\n",
      "Epoch: 197/200 Iteration: 59350 Train loss: 0.350463 Train acc: 0.850000\n",
      "Epoch: 197/200 Iteration: 59400 Train loss: 0.351788 Train acc: 0.845000\n",
      "Epoch: 198/200 Iteration: 59450 Train loss: 0.531429 Train acc: 0.720000\n",
      "Epoch: 198/200 Iteration: 59500 Train loss: 0.556316 Train acc: 0.760000\n",
      "Epoch: 198/200 Iteration: 59500 Validation loss: 0.582599 Validation acc: 0.702150\n",
      "Epoch: 198/200 Iteration: 59550 Train loss: 0.429274 Train acc: 0.790000\n",
      "Epoch: 198/200 Iteration: 59600 Train loss: 0.494769 Train acc: 0.745000\n",
      "Epoch: 198/200 Iteration: 59650 Train loss: 0.354119 Train acc: 0.870000\n",
      "Epoch: 198/200 Iteration: 59700 Train loss: 0.382715 Train acc: 0.805000\n",
      "Epoch: 199/200 Iteration: 59750 Train loss: 0.507313 Train acc: 0.745000\n",
      "Epoch: 199/200 Iteration: 59750 Validation loss: 0.580056 Validation acc: 0.702350\n",
      "Epoch: 199/200 Iteration: 59800 Train loss: 0.528751 Train acc: 0.755000\n",
      "Epoch: 199/200 Iteration: 59850 Train loss: 0.429697 Train acc: 0.775000\n",
      "Epoch: 199/200 Iteration: 59900 Train loss: 0.473531 Train acc: 0.765000\n",
      "Epoch: 199/200 Iteration: 59950 Train loss: 0.362046 Train acc: 0.860000\n",
      "Epoch: 199/200 Iteration: 60000 Train loss: 0.416325 Train acc: 0.810000\n",
      "Epoch: 199/200 Iteration: 60000 Validation loss: 0.581992 Validation acc: 0.699900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 50 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%250 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "  \n",
    "\n",
    "    \n",
    "    saver.save(sess,\"checkpoints/har-lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1508ff57ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 250 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1501a9a3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 250 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\har-lstm.ckpt\n",
      "Test accuracy: 0.676354\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Regression score is 0.88, and the Logistic Regression accuracy is 68%, AUC of Logistic Regression is 0.57, After we use CNN model, the accuracy of predicing hit songs improves to 71.7%, AUC of CNN model is 0.74, which is much better than Logistic Regreesion, ad it means that by using CNN, the accuracy of predicting hit songs improves a lot. At last, we use the LSTM to predict the hit songs by using their time series data, and the accuracy is 67.6%, which means we can use this LSTM model to predict whether a song can be hit song in a given day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
